{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "VGG_quant(\n",
      "  (features): Sequential(\n",
      "    (0): QuantConv2d(\n",
      "      3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): QuantConv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): QuantConv2d(\n",
      "      64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): QuantConv2d(\n",
      "      128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): QuantConv2d(\n",
      "      128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (16): ReLU(inplace=True)\n",
      "    (17): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (19): ReLU(inplace=True)\n",
      "    (20): QuantConv2d(\n",
      "      256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantConv2d(\n",
      "      256, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (25): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): QuantConv2d(\n",
      "      8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (28): ReLU(inplace=True)\n",
      "    (29): QuantConv2d(\n",
      "      8, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (30): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (33): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (38): ReLU(inplace=True)\n",
      "    (39): QuantConv2d(\n",
      "      512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "      (weight_quant): weight_quantize_fn()\n",
      "    )\n",
      "    (40): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (41): ReLU(inplace=True)\n",
      "    (42): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (43): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
      "  )\n",
      "  (classifier): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import *\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"VGG16_quant\"\n",
    "model = VGG16_quant()\n",
    "\n",
    "model.features[24] = QuantConv2d(256, 8, kernel_size=3, padding=1)  \n",
    "model.features[25] = nn.BatchNorm2d(8)\n",
    "model.features[27] = QuantConv2d(8, 8, kernel_size=3, padding=1)   \n",
    "model.features[30] = QuantConv2d(8, 512, kernel_size=3, padding=1)  \n",
    "\n",
    "\n",
    "features = list(model.features)\n",
    "del features[28]  # Remove the BatchNorm2d layer at index 28\n",
    "model.features = nn.Sequential(*features)\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.2023, 0.1994, 0.2010])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "class QuantAwareLoss(nn.Module):\n",
    "    def __init__(self, base_loss, alpha=0.1):\n",
    "        super(QuantAwareLoss, self).__init__()\n",
    "        self.base_loss = base_loss\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, outputs, targets, model):\n",
    "        base_loss = self.base_loss(outputs, targets)\n",
    "        quant_loss = sum([torch.sum(torch.abs(param)) for param in model.parameters() if param.requires_grad])\n",
    "        return base_loss + self.alpha * quant_loss\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [25, 70, 100, 150]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1  \n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 0.839 (0.839)\tData 0.200 (0.200)\tLoss 2.4246 (2.4246)\tPrec 10.156% (10.156%)\n",
      "Epoch: [0][100/391]\tTime 0.046 (0.058)\tData 0.001 (0.006)\tLoss 2.0024 (2.1562)\tPrec 24.219% (18.796%)\n",
      "Epoch: [0][200/391]\tTime 0.046 (0.052)\tData 0.001 (0.004)\tLoss 2.1164 (2.0965)\tPrec 20.312% (20.196%)\n",
      "Epoch: [0][300/391]\tTime 0.046 (0.050)\tData 0.001 (0.003)\tLoss 2.0287 (2.0633)\tPrec 21.875% (20.764%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.154 (0.154)\tLoss 1.9597 (1.9597)\tPrec 16.406% (16.406%)\n",
      " * Prec 24.260% \n",
      "best acc: 24.260000\n",
      "Epoch: [1][0/391]\tTime 0.238 (0.238)\tData 0.198 (0.198)\tLoss 1.8653 (1.8653)\tPrec 28.906% (28.906%)\n",
      "Epoch: [1][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 1.9420 (1.9258)\tPrec 24.219% (25.286%)\n",
      "Epoch: [1][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 1.8767 (1.9185)\tPrec 25.781% (25.237%)\n",
      "Epoch: [1][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 2.0233 (1.9005)\tPrec 19.531% (25.859%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.136 (0.136)\tLoss 1.9672 (1.9672)\tPrec 32.031% (32.031%)\n",
      " * Prec 30.230% \n",
      "best acc: 30.230000\n",
      "Epoch: [2][0/391]\tTime 0.244 (0.244)\tData 0.204 (0.204)\tLoss 1.7577 (1.7577)\tPrec 30.469% (30.469%)\n",
      "Epoch: [2][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 1.7205 (1.8177)\tPrec 32.031% (29.401%)\n",
      "Epoch: [2][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 1.9159 (1.8140)\tPrec 25.781% (29.404%)\n",
      "Epoch: [2][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 1.7973 (1.8089)\tPrec 31.250% (29.991%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.190 (0.190)\tLoss 1.9606 (1.9606)\tPrec 21.094% (21.094%)\n",
      " * Prec 30.470% \n",
      "best acc: 30.470000\n",
      "Epoch: [3][0/391]\tTime 0.253 (0.253)\tData 0.214 (0.214)\tLoss 1.7506 (1.7506)\tPrec 32.031% (32.031%)\n",
      "Epoch: [3][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 1.9487 (1.7640)\tPrec 30.469% (32.913%)\n",
      "Epoch: [3][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 1.6750 (1.7432)\tPrec 36.719% (33.877%)\n",
      "Epoch: [3][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 1.5907 (1.7283)\tPrec 38.281% (34.684%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.178 (0.178)\tLoss 1.8466 (1.8466)\tPrec 38.281% (38.281%)\n",
      " * Prec 34.510% \n",
      "best acc: 34.510000\n",
      "Epoch: [4][0/391]\tTime 0.224 (0.224)\tData 0.185 (0.185)\tLoss 1.6728 (1.6728)\tPrec 39.062% (39.062%)\n",
      "Epoch: [4][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 1.7684 (1.6652)\tPrec 38.281% (38.250%)\n",
      "Epoch: [4][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 1.5513 (1.6442)\tPrec 39.062% (38.946%)\n",
      "Epoch: [4][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 1.5102 (1.6339)\tPrec 44.531% (39.537%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.217 (0.217)\tLoss 1.7803 (1.7803)\tPrec 40.625% (40.625%)\n",
      " * Prec 41.340% \n",
      "best acc: 41.340000\n",
      "Epoch: [5][0/391]\tTime 0.222 (0.222)\tData 0.176 (0.176)\tLoss 1.4311 (1.4311)\tPrec 50.000% (50.000%)\n",
      "Epoch: [5][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 1.5292 (1.5326)\tPrec 47.656% (43.773%)\n",
      "Epoch: [5][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 1.5935 (1.5268)\tPrec 42.188% (44.314%)\n",
      "Epoch: [5][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 1.4886 (1.5273)\tPrec 45.312% (44.261%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.155 (0.155)\tLoss 1.7014 (1.7014)\tPrec 41.406% (41.406%)\n",
      " * Prec 43.600% \n",
      "best acc: 43.600000\n",
      "Epoch: [6][0/391]\tTime 0.256 (0.256)\tData 0.217 (0.217)\tLoss 1.4327 (1.4327)\tPrec 45.312% (45.312%)\n",
      "Epoch: [6][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 1.3642 (1.4438)\tPrec 51.562% (47.749%)\n",
      "Epoch: [6][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 1.6072 (1.4542)\tPrec 40.625% (47.474%)\n",
      "Epoch: [6][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.002)\tLoss 1.4564 (1.4417)\tPrec 50.000% (48.053%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.176 (0.176)\tLoss 1.4544 (1.4544)\tPrec 47.656% (47.656%)\n",
      " * Prec 50.240% \n",
      "best acc: 50.240000\n",
      "Epoch: [7][0/391]\tTime 0.275 (0.275)\tData 0.234 (0.234)\tLoss 1.5110 (1.5110)\tPrec 50.000% (50.000%)\n",
      "Epoch: [7][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 1.4389 (1.3813)\tPrec 53.125% (50.518%)\n",
      "Epoch: [7][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 1.4537 (1.3701)\tPrec 44.531% (50.735%)\n",
      "Epoch: [7][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 1.3460 (1.3603)\tPrec 52.344% (51.004%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.204 (0.204)\tLoss 1.3547 (1.3547)\tPrec 50.000% (50.000%)\n",
      " * Prec 47.370% \n",
      "best acc: 50.240000\n",
      "Epoch: [8][0/391]\tTime 0.239 (0.239)\tData 0.201 (0.201)\tLoss 1.4711 (1.4711)\tPrec 46.094% (46.094%)\n",
      "Epoch: [8][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 1.1642 (1.3009)\tPrec 56.250% (53.844%)\n",
      "Epoch: [8][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 1.2833 (1.2870)\tPrec 51.562% (54.310%)\n",
      "Epoch: [8][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 1.1093 (1.2718)\tPrec 58.594% (55.017%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 1.4041 (1.4041)\tPrec 52.344% (52.344%)\n",
      " * Prec 50.230% \n",
      "best acc: 50.240000\n",
      "Epoch: [9][0/391]\tTime 0.249 (0.249)\tData 0.211 (0.211)\tLoss 1.1949 (1.1949)\tPrec 53.125% (53.125%)\n",
      "Epoch: [9][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 1.1136 (1.2466)\tPrec 65.625% (55.879%)\n",
      "Epoch: [9][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 1.2373 (1.2358)\tPrec 53.125% (56.437%)\n",
      "Epoch: [9][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 1.3895 (1.2295)\tPrec 53.906% (56.652%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.192 (0.192)\tLoss 1.2196 (1.2196)\tPrec 59.375% (59.375%)\n",
      " * Prec 57.220% \n",
      "best acc: 57.220000\n",
      "Epoch: [10][0/391]\tTime 0.281 (0.281)\tData 0.243 (0.243)\tLoss 1.3581 (1.3581)\tPrec 52.344% (52.344%)\n",
      "Epoch: [10][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 1.2028 (1.1666)\tPrec 53.125% (59.112%)\n",
      "Epoch: [10][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 1.0468 (1.1724)\tPrec 61.719% (58.835%)\n",
      "Epoch: [10][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 1.2109 (1.1666)\tPrec 53.125% (59.009%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.182 (0.182)\tLoss 1.0179 (1.0179)\tPrec 63.281% (63.281%)\n",
      " * Prec 58.830% \n",
      "best acc: 58.830000\n",
      "Epoch: [11][0/391]\tTime 0.248 (0.248)\tData 0.209 (0.209)\tLoss 1.0824 (1.0824)\tPrec 62.500% (62.500%)\n",
      "Epoch: [11][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 1.2152 (1.1092)\tPrec 59.375% (60.736%)\n",
      "Epoch: [11][200/391]\tTime 0.045 (0.047)\tData 0.001 (0.002)\tLoss 1.0682 (1.1110)\tPrec 63.281% (60.813%)\n",
      "Epoch: [11][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 1.1414 (1.1134)\tPrec 56.250% (60.826%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 1.2163 (1.2163)\tPrec 57.031% (57.031%)\n",
      " * Prec 54.600% \n",
      "best acc: 58.830000\n",
      "Epoch: [12][0/391]\tTime 0.254 (0.254)\tData 0.215 (0.215)\tLoss 1.1475 (1.1475)\tPrec 60.156% (60.156%)\n",
      "Epoch: [12][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 1.0207 (1.0840)\tPrec 66.406% (61.866%)\n",
      "Epoch: [12][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 1.1139 (1.0799)\tPrec 57.031% (62.376%)\n",
      "Epoch: [12][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 1.1905 (1.0643)\tPrec 58.594% (63.074%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.166 (0.166)\tLoss 1.0611 (1.0611)\tPrec 60.938% (60.938%)\n",
      " * Prec 62.330% \n",
      "best acc: 62.330000\n",
      "Epoch: [13][0/391]\tTime 0.189 (0.189)\tData 0.149 (0.149)\tLoss 0.8813 (0.8813)\tPrec 71.094% (71.094%)\n",
      "Epoch: [13][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 1.1164 (1.0181)\tPrec 57.812% (64.952%)\n",
      "Epoch: [13][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.9557 (1.0344)\tPrec 66.406% (64.327%)\n",
      "Epoch: [13][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 1.0407 (1.0362)\tPrec 64.062% (64.395%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.170 (0.170)\tLoss 1.1925 (1.1925)\tPrec 63.281% (63.281%)\n",
      " * Prec 59.140% \n",
      "best acc: 62.330000\n",
      "Epoch: [14][0/391]\tTime 0.197 (0.197)\tData 0.160 (0.160)\tLoss 1.0079 (1.0079)\tPrec 67.188% (67.188%)\n",
      "Epoch: [14][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.9867 (0.9912)\tPrec 66.406% (65.981%)\n",
      "Epoch: [14][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.8387 (0.9814)\tPrec 71.094% (66.282%)\n",
      "Epoch: [14][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.9270 (0.9765)\tPrec 71.094% (66.668%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.170 (0.170)\tLoss 0.9102 (0.9102)\tPrec 65.625% (65.625%)\n",
      " * Prec 67.360% \n",
      "best acc: 67.360000\n",
      "Epoch: [15][0/391]\tTime 0.194 (0.194)\tData 0.141 (0.141)\tLoss 0.9261 (0.9261)\tPrec 67.188% (67.188%)\n",
      "Epoch: [15][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.9949 (0.9095)\tPrec 67.188% (68.998%)\n",
      "Epoch: [15][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.8483 (0.9105)\tPrec 75.000% (69.139%)\n",
      "Epoch: [15][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.7199 (0.9034)\tPrec 75.781% (69.399%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.8335 (0.8335)\tPrec 71.094% (71.094%)\n",
      " * Prec 67.370% \n",
      "best acc: 67.370000\n",
      "Epoch: [16][0/391]\tTime 0.279 (0.279)\tData 0.240 (0.240)\tLoss 0.7081 (0.7081)\tPrec 79.688% (79.688%)\n",
      "Epoch: [16][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 0.9241 (0.8727)\tPrec 64.062% (70.877%)\n",
      "Epoch: [16][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.7326 (0.8690)\tPrec 73.438% (71.039%)\n",
      "Epoch: [16][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.8806 (0.8596)\tPrec 69.531% (71.226%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.7888 (0.7888)\tPrec 75.000% (75.000%)\n",
      " * Prec 70.800% \n",
      "best acc: 70.800000\n",
      "Epoch: [17][0/391]\tTime 0.240 (0.240)\tData 0.183 (0.183)\tLoss 0.7411 (0.7411)\tPrec 74.219% (74.219%)\n",
      "Epoch: [17][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.8153 (0.8247)\tPrec 74.219% (72.339%)\n",
      "Epoch: [17][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.7050 (0.8322)\tPrec 76.562% (72.027%)\n",
      "Epoch: [17][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.8727 (0.8384)\tPrec 69.531% (71.984%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.172 (0.172)\tLoss 0.7423 (0.7423)\tPrec 73.438% (73.438%)\n",
      " * Prec 71.930% \n",
      "best acc: 71.930000\n",
      "Epoch: [18][0/391]\tTime 0.236 (0.236)\tData 0.189 (0.189)\tLoss 0.6945 (0.6945)\tPrec 75.000% (75.000%)\n",
      "Epoch: [18][100/391]\tTime 0.047 (0.048)\tData 0.001 (0.003)\tLoss 0.6844 (0.7899)\tPrec 75.000% (73.724%)\n",
      "Epoch: [18][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.7588 (0.7957)\tPrec 74.219% (73.305%)\n",
      "Epoch: [18][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.7214 (0.7941)\tPrec 74.219% (73.305%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.150 (0.150)\tLoss 0.9013 (0.9013)\tPrec 69.531% (69.531%)\n",
      " * Prec 68.010% \n",
      "best acc: 71.930000\n",
      "Epoch: [19][0/391]\tTime 0.183 (0.183)\tData 0.138 (0.138)\tLoss 0.8265 (0.8265)\tPrec 73.438% (73.438%)\n",
      "Epoch: [19][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.7130 (0.7875)\tPrec 74.219% (74.041%)\n",
      "Epoch: [19][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6181 (0.7740)\tPrec 78.906% (74.347%)\n",
      "Epoch: [19][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.7769 (0.7700)\tPrec 76.562% (74.471%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.197 (0.197)\tLoss 0.8598 (0.8598)\tPrec 71.094% (71.094%)\n",
      " * Prec 72.010% \n",
      "best acc: 72.010000\n",
      "Epoch: [20][0/391]\tTime 0.241 (0.241)\tData 0.202 (0.202)\tLoss 0.8904 (0.8904)\tPrec 71.094% (71.094%)\n",
      "Epoch: [20][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.7463 (0.7507)\tPrec 77.344% (75.054%)\n",
      "Epoch: [20][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6576 (0.7523)\tPrec 78.906% (75.140%)\n",
      "Epoch: [20][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.7552 (0.7580)\tPrec 78.906% (74.847%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.177 (0.177)\tLoss 0.7681 (0.7681)\tPrec 75.000% (75.000%)\n",
      " * Prec 73.880% \n",
      "best acc: 73.880000\n",
      "Epoch: [21][0/391]\tTime 0.229 (0.229)\tData 0.176 (0.176)\tLoss 0.7078 (0.7078)\tPrec 75.781% (75.781%)\n",
      "Epoch: [21][100/391]\tTime 0.047 (0.048)\tData 0.002 (0.003)\tLoss 0.7469 (0.7461)\tPrec 71.094% (75.062%)\n",
      "Epoch: [21][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.7025 (0.7357)\tPrec 76.562% (75.513%)\n",
      "Epoch: [21][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.7735 (0.7370)\tPrec 74.219% (75.535%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.192 (0.192)\tLoss 0.8007 (0.8007)\tPrec 74.219% (74.219%)\n",
      " * Prec 70.780% \n",
      "best acc: 73.880000\n",
      "Epoch: [22][0/391]\tTime 0.264 (0.264)\tData 0.216 (0.216)\tLoss 0.7973 (0.7973)\tPrec 75.000% (75.000%)\n",
      "Epoch: [22][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.7263 (0.7147)\tPrec 71.875% (75.843%)\n",
      "Epoch: [22][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6479 (0.7194)\tPrec 76.562% (75.882%)\n",
      "Epoch: [22][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.8653 (0.7148)\tPrec 75.781% (76.075%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.192 (0.192)\tLoss 0.7556 (0.7556)\tPrec 77.344% (77.344%)\n",
      " * Prec 74.080% \n",
      "best acc: 74.080000\n",
      "Epoch: [23][0/391]\tTime 0.251 (0.251)\tData 0.199 (0.199)\tLoss 0.5792 (0.5792)\tPrec 82.812% (82.812%)\n",
      "Epoch: [23][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.7716 (0.6774)\tPrec 71.094% (77.498%)\n",
      "Epoch: [23][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.5218 (0.6862)\tPrec 80.469% (77.254%)\n",
      "Epoch: [23][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6152 (0.6998)\tPrec 76.562% (76.739%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.194 (0.194)\tLoss 0.8117 (0.8117)\tPrec 70.312% (70.312%)\n",
      " * Prec 73.070% \n",
      "best acc: 74.080000\n",
      "Epoch: [24][0/391]\tTime 0.238 (0.238)\tData 0.198 (0.198)\tLoss 0.7157 (0.7157)\tPrec 75.781% (75.781%)\n",
      "Epoch: [24][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.7034 (0.6729)\tPrec 78.906% (77.584%)\n",
      "Epoch: [24][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.6849 (0.6890)\tPrec 82.031% (77.324%)\n",
      "Epoch: [24][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.5126 (0.6861)\tPrec 83.594% (77.388%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.176 (0.176)\tLoss 0.8105 (0.8105)\tPrec 72.656% (72.656%)\n",
      " * Prec 75.580% \n",
      "best acc: 75.580000\n",
      "Epoch: [25][0/391]\tTime 0.255 (0.255)\tData 0.215 (0.215)\tLoss 0.6508 (0.6508)\tPrec 80.469% (80.469%)\n",
      "Epoch: [25][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.5843 (0.6739)\tPrec 81.250% (77.955%)\n",
      "Epoch: [25][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6728 (0.6761)\tPrec 79.688% (77.690%)\n",
      "Epoch: [25][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6824 (0.6844)\tPrec 77.344% (77.284%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.185 (0.185)\tLoss 0.8290 (0.8290)\tPrec 70.312% (70.312%)\n",
      " * Prec 73.450% \n",
      "best acc: 75.580000\n",
      "Epoch: [26][0/391]\tTime 0.185 (0.185)\tData 0.145 (0.145)\tLoss 0.6420 (0.6420)\tPrec 79.688% (79.688%)\n",
      "Epoch: [26][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.4909 (0.6709)\tPrec 82.031% (78.079%)\n",
      "Epoch: [26][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.8355 (0.6765)\tPrec 76.562% (77.911%)\n",
      "Epoch: [26][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.5963 (0.6709)\tPrec 78.125% (78.052%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.197 (0.197)\tLoss 0.7183 (0.7183)\tPrec 76.562% (76.562%)\n",
      " * Prec 71.390% \n",
      "best acc: 75.580000\n",
      "Epoch: [27][0/391]\tTime 0.210 (0.210)\tData 0.160 (0.160)\tLoss 0.6399 (0.6399)\tPrec 79.688% (79.688%)\n",
      "Epoch: [27][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.5976 (0.6607)\tPrec 78.906% (78.086%)\n",
      "Epoch: [27][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6553 (0.6601)\tPrec 79.688% (78.071%)\n",
      "Epoch: [27][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.7101 (0.6632)\tPrec 75.000% (77.982%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.184 (0.184)\tLoss 0.6170 (0.6170)\tPrec 76.562% (76.562%)\n",
      " * Prec 74.510% \n",
      "best acc: 75.580000\n",
      "Epoch: [28][0/391]\tTime 0.257 (0.257)\tData 0.219 (0.219)\tLoss 0.6847 (0.6847)\tPrec 78.125% (78.125%)\n",
      "Epoch: [28][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.6781 (0.6709)\tPrec 76.562% (78.218%)\n",
      "Epoch: [28][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6805 (0.6633)\tPrec 72.656% (78.568%)\n",
      "Epoch: [28][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.8097 (0.6575)\tPrec 76.562% (78.763%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.194 (0.194)\tLoss 0.7427 (0.7427)\tPrec 76.562% (76.562%)\n",
      " * Prec 74.690% \n",
      "best acc: 75.580000\n",
      "Epoch: [29][0/391]\tTime 0.256 (0.256)\tData 0.216 (0.216)\tLoss 0.6339 (0.6339)\tPrec 77.344% (77.344%)\n",
      "Epoch: [29][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 0.7368 (0.6350)\tPrec 75.781% (79.332%)\n",
      "Epoch: [29][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6302 (0.6494)\tPrec 80.469% (78.805%)\n",
      "Epoch: [29][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.7420 (0.6496)\tPrec 73.438% (78.831%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.188 (0.188)\tLoss 0.8302 (0.8302)\tPrec 76.562% (76.562%)\n",
      " * Prec 75.020% \n",
      "best acc: 75.580000\n",
      "Epoch: [30][0/391]\tTime 0.215 (0.215)\tData 0.168 (0.168)\tLoss 0.8108 (0.8108)\tPrec 73.438% (73.438%)\n",
      "Epoch: [30][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.6665 (0.6212)\tPrec 78.906% (79.425%)\n",
      "Epoch: [30][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6687 (0.6236)\tPrec 79.688% (79.377%)\n",
      "Epoch: [30][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6652 (0.6278)\tPrec 79.688% (79.311%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.148 (0.148)\tLoss 0.6820 (0.6820)\tPrec 75.000% (75.000%)\n",
      " * Prec 77.090% \n",
      "best acc: 77.090000\n",
      "Epoch: [31][0/391]\tTime 0.230 (0.230)\tData 0.191 (0.191)\tLoss 0.7778 (0.7778)\tPrec 76.562% (76.562%)\n",
      "Epoch: [31][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.6865 (0.6199)\tPrec 78.125% (79.579%)\n",
      "Epoch: [31][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.6167 (0.6266)\tPrec 78.906% (79.423%)\n",
      "Epoch: [31][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.5855 (0.6324)\tPrec 81.250% (79.259%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.196 (0.196)\tLoss 0.7262 (0.7262)\tPrec 75.781% (75.781%)\n",
      " * Prec 74.320% \n",
      "best acc: 77.090000\n",
      "Epoch: [32][0/391]\tTime 0.261 (0.261)\tData 0.222 (0.222)\tLoss 0.5963 (0.5963)\tPrec 78.125% (78.125%)\n",
      "Epoch: [32][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.5412 (0.6043)\tPrec 77.344% (80.020%)\n",
      "Epoch: [32][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.7426 (0.6098)\tPrec 74.219% (80.193%)\n",
      "Epoch: [32][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.7067 (0.6228)\tPrec 76.562% (79.708%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.215 (0.215)\tLoss 0.6437 (0.6437)\tPrec 78.125% (78.125%)\n",
      " * Prec 76.770% \n",
      "best acc: 77.090000\n",
      "Epoch: [33][0/391]\tTime 0.202 (0.202)\tData 0.162 (0.162)\tLoss 0.5400 (0.5400)\tPrec 78.906% (78.906%)\n",
      "Epoch: [33][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.6236 (0.6089)\tPrec 80.469% (79.680%)\n",
      "Epoch: [33][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.5755 (0.6148)\tPrec 79.688% (79.761%)\n",
      "Epoch: [33][300/391]\tTime 0.046 (0.046)\tData 0.002 (0.002)\tLoss 0.5499 (0.6185)\tPrec 81.250% (79.617%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.189 (0.189)\tLoss 0.5892 (0.5892)\tPrec 82.031% (82.031%)\n",
      " * Prec 77.260% \n",
      "best acc: 77.260000\n",
      "Epoch: [34][0/391]\tTime 0.256 (0.256)\tData 0.210 (0.210)\tLoss 0.7497 (0.7497)\tPrec 76.562% (76.562%)\n",
      "Epoch: [34][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 0.6285 (0.5981)\tPrec 77.344% (80.213%)\n",
      "Epoch: [34][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6197 (0.6045)\tPrec 79.688% (80.076%)\n",
      "Epoch: [34][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.6211 (0.6078)\tPrec 78.906% (80.160%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.191 (0.191)\tLoss 0.7664 (0.7664)\tPrec 73.438% (73.438%)\n",
      " * Prec 75.700% \n",
      "best acc: 77.260000\n",
      "Epoch: [35][0/391]\tTime 0.262 (0.262)\tData 0.224 (0.224)\tLoss 0.4498 (0.4498)\tPrec 85.938% (85.938%)\n",
      "Epoch: [35][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.5733 (0.5936)\tPrec 78.906% (80.306%)\n",
      "Epoch: [35][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6594 (0.5972)\tPrec 77.344% (80.154%)\n",
      "Epoch: [35][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.5949 (0.5999)\tPrec 79.688% (80.160%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.7042 (0.7042)\tPrec 77.344% (77.344%)\n",
      " * Prec 78.420% \n",
      "best acc: 78.420000\n",
      "Epoch: [36][0/391]\tTime 0.242 (0.242)\tData 0.202 (0.202)\tLoss 0.6504 (0.6504)\tPrec 80.469% (80.469%)\n",
      "Epoch: [36][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.5918 (0.6106)\tPrec 85.156% (79.850%)\n",
      "Epoch: [36][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6983 (0.6072)\tPrec 72.656% (79.952%)\n",
      "Epoch: [36][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.5833 (0.6035)\tPrec 82.812% (80.196%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.181 (0.181)\tLoss 0.5614 (0.5614)\tPrec 84.375% (84.375%)\n",
      " * Prec 78.230% \n",
      "best acc: 78.420000\n",
      "Epoch: [37][0/391]\tTime 0.232 (0.232)\tData 0.194 (0.194)\tLoss 0.6398 (0.6398)\tPrec 80.469% (80.469%)\n",
      "Epoch: [37][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.4823 (0.6034)\tPrec 81.250% (80.484%)\n",
      "Epoch: [37][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6737 (0.5965)\tPrec 81.250% (80.546%)\n",
      "Epoch: [37][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.7886 (0.6042)\tPrec 71.875% (80.277%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.195 (0.195)\tLoss 0.6659 (0.6659)\tPrec 78.906% (78.906%)\n",
      " * Prec 78.020% \n",
      "best acc: 78.420000\n",
      "Epoch: [38][0/391]\tTime 0.247 (0.247)\tData 0.209 (0.209)\tLoss 0.9098 (0.9098)\tPrec 71.094% (71.094%)\n",
      "Epoch: [38][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.7555 (0.6103)\tPrec 75.000% (79.765%)\n",
      "Epoch: [38][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.5742 (0.6106)\tPrec 78.906% (79.874%)\n",
      "Epoch: [38][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6067 (0.6043)\tPrec 78.906% (80.079%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.163 (0.163)\tLoss 0.6748 (0.6748)\tPrec 81.250% (81.250%)\n",
      " * Prec 74.800% \n",
      "best acc: 78.420000\n",
      "Epoch: [39][0/391]\tTime 0.209 (0.209)\tData 0.172 (0.172)\tLoss 0.6140 (0.6140)\tPrec 80.469% (80.469%)\n",
      "Epoch: [39][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.003)\tLoss 0.4277 (0.5956)\tPrec 85.156% (80.631%)\n",
      "Epoch: [39][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6317 (0.5905)\tPrec 78.906% (80.768%)\n",
      "Epoch: [39][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6455 (0.5900)\tPrec 75.781% (80.889%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.197 (0.197)\tLoss 0.8549 (0.8549)\tPrec 77.344% (77.344%)\n",
      " * Prec 74.960% \n",
      "best acc: 78.420000\n",
      "Epoch: [40][0/391]\tTime 0.247 (0.247)\tData 0.208 (0.208)\tLoss 0.8507 (0.8507)\tPrec 71.875% (71.875%)\n",
      "Epoch: [40][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.6294 (0.5995)\tPrec 80.469% (80.198%)\n",
      "Epoch: [40][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.6879 (0.5962)\tPrec 75.781% (80.142%)\n",
      "Epoch: [40][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.5882 (0.5910)\tPrec 78.906% (80.300%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.204 (0.204)\tLoss 0.7015 (0.7015)\tPrec 76.562% (76.562%)\n",
      " * Prec 76.580% \n",
      "best acc: 78.420000\n",
      "Epoch: [41][0/391]\tTime 0.207 (0.207)\tData 0.164 (0.164)\tLoss 0.6721 (0.6721)\tPrec 74.219% (74.219%)\n",
      "Epoch: [41][100/391]\tTime 0.048 (0.048)\tData 0.001 (0.003)\tLoss 0.5783 (0.5933)\tPrec 80.469% (80.144%)\n",
      "Epoch: [41][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4619 (0.5865)\tPrec 86.719% (80.539%)\n",
      "Epoch: [41][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.5012 (0.5825)\tPrec 85.156% (80.739%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.179 (0.179)\tLoss 0.6152 (0.6152)\tPrec 78.906% (78.906%)\n",
      " * Prec 77.270% \n",
      "best acc: 78.420000\n",
      "Epoch: [42][0/391]\tTime 0.239 (0.239)\tData 0.191 (0.191)\tLoss 0.5516 (0.5516)\tPrec 83.594% (83.594%)\n",
      "Epoch: [42][100/391]\tTime 0.045 (0.048)\tData 0.003 (0.003)\tLoss 0.5452 (0.5145)\tPrec 82.812% (82.936%)\n",
      "Epoch: [42][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3901 (0.4973)\tPrec 84.375% (83.656%)\n",
      "Epoch: [42][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6122 (0.4902)\tPrec 82.031% (83.840%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.183 (0.183)\tLoss 0.4536 (0.4536)\tPrec 83.594% (83.594%)\n",
      " * Prec 82.850% \n",
      "best acc: 82.850000\n",
      "Epoch: [43][0/391]\tTime 0.230 (0.230)\tData 0.182 (0.182)\tLoss 0.4163 (0.4163)\tPrec 87.500% (87.500%)\n",
      "Epoch: [43][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.5096 (0.4387)\tPrec 85.156% (85.373%)\n",
      "Epoch: [43][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4228 (0.4475)\tPrec 85.938% (85.113%)\n",
      "Epoch: [43][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.3812 (0.4504)\tPrec 88.281% (85.081%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.157 (0.157)\tLoss 0.3736 (0.3736)\tPrec 89.062% (89.062%)\n",
      " * Prec 83.190% \n",
      "best acc: 83.190000\n",
      "Epoch: [44][0/391]\tTime 0.189 (0.189)\tData 0.146 (0.146)\tLoss 0.5066 (0.5066)\tPrec 82.812% (82.812%)\n",
      "Epoch: [44][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.4294 (0.4394)\tPrec 86.719% (85.589%)\n",
      "Epoch: [44][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4541 (0.4424)\tPrec 86.719% (85.405%)\n",
      "Epoch: [44][300/391]\tTime 0.046 (0.046)\tData 0.002 (0.002)\tLoss 0.4290 (0.4437)\tPrec 83.594% (85.346%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.198 (0.198)\tLoss 0.3882 (0.3882)\tPrec 87.500% (87.500%)\n",
      " * Prec 83.250% \n",
      "best acc: 83.250000\n",
      "Epoch: [45][0/391]\tTime 0.256 (0.256)\tData 0.217 (0.217)\tLoss 0.4288 (0.4288)\tPrec 86.719% (86.719%)\n",
      "Epoch: [45][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.3442 (0.4407)\tPrec 89.844% (85.473%)\n",
      "Epoch: [45][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.4316 (0.4393)\tPrec 87.500% (85.479%)\n",
      "Epoch: [45][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.5969 (0.4338)\tPrec 79.688% (85.543%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.196 (0.196)\tLoss 0.4488 (0.4488)\tPrec 84.375% (84.375%)\n",
      " * Prec 82.950% \n",
      "best acc: 83.250000\n",
      "Epoch: [46][0/391]\tTime 0.232 (0.232)\tData 0.192 (0.192)\tLoss 0.4415 (0.4415)\tPrec 88.281% (88.281%)\n",
      "Epoch: [46][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.5260 (0.4215)\tPrec 83.594% (85.961%)\n",
      "Epoch: [46][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4428 (0.4240)\tPrec 85.156% (85.821%)\n",
      "Epoch: [46][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.4386 (0.4225)\tPrec 86.719% (85.816%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.165 (0.165)\tLoss 0.4203 (0.4203)\tPrec 85.156% (85.156%)\n",
      " * Prec 83.420% \n",
      "best acc: 83.420000\n",
      "Epoch: [47][0/391]\tTime 0.254 (0.254)\tData 0.214 (0.214)\tLoss 0.5449 (0.5449)\tPrec 84.375% (84.375%)\n",
      "Epoch: [47][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.4752 (0.4155)\tPrec 89.844% (86.680%)\n",
      "Epoch: [47][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3038 (0.4082)\tPrec 86.719% (86.727%)\n",
      "Epoch: [47][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4062 (0.4148)\tPrec 85.938% (86.340%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.203 (0.203)\tLoss 0.3678 (0.3678)\tPrec 87.500% (87.500%)\n",
      " * Prec 82.830% \n",
      "best acc: 83.420000\n",
      "Epoch: [48][0/391]\tTime 0.191 (0.191)\tData 0.152 (0.152)\tLoss 0.3033 (0.3033)\tPrec 91.406% (91.406%)\n",
      "Epoch: [48][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.4418 (0.3966)\tPrec 82.812% (86.711%)\n",
      "Epoch: [48][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4350 (0.4075)\tPrec 85.156% (86.645%)\n",
      "Epoch: [48][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.5920 (0.4096)\tPrec 81.250% (86.529%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.4001 (0.4001)\tPrec 88.281% (88.281%)\n",
      " * Prec 83.610% \n",
      "best acc: 83.610000\n",
      "Epoch: [49][0/391]\tTime 0.194 (0.194)\tData 0.157 (0.157)\tLoss 0.3691 (0.3691)\tPrec 86.719% (86.719%)\n",
      "Epoch: [49][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.3643 (0.3986)\tPrec 87.500% (86.835%)\n",
      "Epoch: [49][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4106 (0.3967)\tPrec 85.156% (86.746%)\n",
      "Epoch: [49][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.4175 (0.4048)\tPrec 86.719% (86.566%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.181 (0.181)\tLoss 0.4615 (0.4615)\tPrec 84.375% (84.375%)\n",
      " * Prec 83.170% \n",
      "best acc: 83.610000\n",
      "Epoch: [50][0/391]\tTime 0.205 (0.205)\tData 0.149 (0.149)\tLoss 0.4131 (0.4131)\tPrec 82.031% (82.031%)\n",
      "Epoch: [50][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.003)\tLoss 0.3650 (0.3963)\tPrec 89.062% (86.959%)\n",
      "Epoch: [50][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4545 (0.4015)\tPrec 88.281% (86.769%)\n",
      "Epoch: [50][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.3581 (0.4039)\tPrec 88.281% (86.589%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.4478 (0.4478)\tPrec 82.031% (82.031%)\n",
      " * Prec 83.590% \n",
      "best acc: 83.610000\n",
      "Epoch: [51][0/391]\tTime 0.212 (0.212)\tData 0.164 (0.164)\tLoss 0.4659 (0.4659)\tPrec 85.156% (85.156%)\n",
      "Epoch: [51][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.3777 (0.3987)\tPrec 82.031% (86.897%)\n",
      "Epoch: [51][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.3537 (0.4003)\tPrec 89.062% (86.622%)\n",
      "Epoch: [51][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4434 (0.4045)\tPrec 86.719% (86.410%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.166 (0.166)\tLoss 0.4727 (0.4727)\tPrec 82.812% (82.812%)\n",
      " * Prec 83.290% \n",
      "best acc: 83.610000\n",
      "Epoch: [52][0/391]\tTime 0.260 (0.260)\tData 0.221 (0.221)\tLoss 0.3802 (0.3802)\tPrec 85.938% (85.938%)\n",
      "Epoch: [52][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.3271 (0.3959)\tPrec 90.625% (86.850%)\n",
      "Epoch: [52][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.3141 (0.4015)\tPrec 90.625% (86.563%)\n",
      "Epoch: [52][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4335 (0.4032)\tPrec 83.594% (86.659%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.180 (0.180)\tLoss 0.3422 (0.3422)\tPrec 89.062% (89.062%)\n",
      " * Prec 83.360% \n",
      "best acc: 83.610000\n",
      "Epoch: [53][0/391]\tTime 0.214 (0.214)\tData 0.171 (0.171)\tLoss 0.4508 (0.4508)\tPrec 83.594% (83.594%)\n",
      "Epoch: [53][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3657 (0.3947)\tPrec 89.062% (86.881%)\n",
      "Epoch: [53][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3776 (0.3971)\tPrec 84.375% (86.987%)\n",
      "Epoch: [53][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.5648 (0.3964)\tPrec 82.812% (87.009%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.146 (0.146)\tLoss 0.4410 (0.4410)\tPrec 85.156% (85.156%)\n",
      " * Prec 83.640% \n",
      "best acc: 83.640000\n",
      "Epoch: [54][0/391]\tTime 0.252 (0.252)\tData 0.212 (0.212)\tLoss 0.4082 (0.4082)\tPrec 87.500% (87.500%)\n",
      "Epoch: [54][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.3722 (0.3957)\tPrec 90.625% (86.672%)\n",
      "Epoch: [54][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.3758 (0.3922)\tPrec 86.719% (87.224%)\n",
      "Epoch: [54][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4798 (0.3921)\tPrec 83.594% (87.152%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.4846 (0.4846)\tPrec 84.375% (84.375%)\n",
      " * Prec 82.900% \n",
      "best acc: 83.640000\n",
      "Epoch: [55][0/391]\tTime 0.266 (0.266)\tData 0.226 (0.226)\tLoss 0.5388 (0.5388)\tPrec 80.469% (80.469%)\n",
      "Epoch: [55][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.3719 (0.3900)\tPrec 87.500% (87.044%)\n",
      "Epoch: [55][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.4111 (0.3956)\tPrec 84.375% (86.762%)\n",
      "Epoch: [55][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2968 (0.3959)\tPrec 90.625% (86.693%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.188 (0.188)\tLoss 0.4069 (0.4069)\tPrec 85.938% (85.938%)\n",
      " * Prec 83.640% \n",
      "best acc: 83.640000\n",
      "Epoch: [56][0/391]\tTime 0.231 (0.231)\tData 0.192 (0.192)\tLoss 0.3203 (0.3203)\tPrec 88.281% (88.281%)\n",
      "Epoch: [56][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2704 (0.3903)\tPrec 89.844% (87.090%)\n",
      "Epoch: [56][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.2805 (0.3907)\tPrec 88.281% (86.979%)\n",
      "Epoch: [56][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3547 (0.3871)\tPrec 86.719% (87.082%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.184 (0.184)\tLoss 0.3738 (0.3738)\tPrec 83.594% (83.594%)\n",
      " * Prec 83.650% \n",
      "best acc: 83.650000\n",
      "Epoch: [57][0/391]\tTime 0.230 (0.230)\tData 0.192 (0.192)\tLoss 0.3393 (0.3393)\tPrec 87.500% (87.500%)\n",
      "Epoch: [57][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3001 (0.3804)\tPrec 90.625% (87.477%)\n",
      "Epoch: [57][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.5426 (0.3845)\tPrec 81.250% (87.275%)\n",
      "Epoch: [57][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2852 (0.3868)\tPrec 91.406% (87.230%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.190 (0.190)\tLoss 0.4492 (0.4492)\tPrec 83.594% (83.594%)\n",
      " * Prec 82.820% \n",
      "best acc: 83.650000\n",
      "Epoch: [58][0/391]\tTime 0.217 (0.217)\tData 0.176 (0.176)\tLoss 0.4168 (0.4168)\tPrec 88.281% (88.281%)\n",
      "Epoch: [58][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3601 (0.3902)\tPrec 91.406% (87.206%)\n",
      "Epoch: [58][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2144 (0.3891)\tPrec 93.750% (87.158%)\n",
      "Epoch: [58][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.002)\tLoss 0.4439 (0.3912)\tPrec 84.375% (87.007%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.4306 (0.4306)\tPrec 87.500% (87.500%)\n",
      " * Prec 82.960% \n",
      "best acc: 83.650000\n",
      "Epoch: [59][0/391]\tTime 0.232 (0.232)\tData 0.185 (0.185)\tLoss 0.3574 (0.3574)\tPrec 89.062% (89.062%)\n",
      "Epoch: [59][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.4675 (0.3675)\tPrec 85.156% (87.778%)\n",
      "Epoch: [59][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3948 (0.3785)\tPrec 86.719% (87.306%)\n",
      "Epoch: [59][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2951 (0.3832)\tPrec 90.625% (87.225%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.202 (0.202)\tLoss 0.4336 (0.4336)\tPrec 85.156% (85.156%)\n",
      " * Prec 82.580% \n",
      "best acc: 83.650000\n",
      "Epoch: [60][0/391]\tTime 0.216 (0.216)\tData 0.168 (0.168)\tLoss 0.4095 (0.4095)\tPrec 85.156% (85.156%)\n",
      "Epoch: [60][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.2890 (0.3662)\tPrec 89.844% (87.980%)\n",
      "Epoch: [60][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4777 (0.3761)\tPrec 81.250% (87.624%)\n",
      "Epoch: [60][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.5541 (0.3815)\tPrec 82.031% (87.407%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.183 (0.183)\tLoss 0.4145 (0.4145)\tPrec 85.156% (85.156%)\n",
      " * Prec 82.950% \n",
      "best acc: 83.650000\n",
      "Epoch: [61][0/391]\tTime 0.230 (0.230)\tData 0.190 (0.190)\tLoss 0.2714 (0.2714)\tPrec 91.406% (91.406%)\n",
      "Epoch: [61][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3623 (0.3610)\tPrec 86.719% (88.049%)\n",
      "Epoch: [61][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.6773 (0.3701)\tPrec 80.469% (87.757%)\n",
      "Epoch: [61][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2451 (0.3774)\tPrec 93.750% (87.544%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.205 (0.205)\tLoss 0.3943 (0.3943)\tPrec 85.938% (85.938%)\n",
      " * Prec 83.800% \n",
      "best acc: 83.800000\n",
      "Epoch: [62][0/391]\tTime 0.264 (0.264)\tData 0.205 (0.205)\tLoss 0.3877 (0.3877)\tPrec 87.500% (87.500%)\n",
      "Epoch: [62][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3458 (0.3719)\tPrec 87.500% (87.399%)\n",
      "Epoch: [62][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4369 (0.3773)\tPrec 80.469% (87.208%)\n",
      "Epoch: [62][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2852 (0.3778)\tPrec 87.500% (87.264%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.187 (0.187)\tLoss 0.4476 (0.4476)\tPrec 87.500% (87.500%)\n",
      " * Prec 83.600% \n",
      "best acc: 83.800000\n",
      "Epoch: [63][0/391]\tTime 0.227 (0.227)\tData 0.176 (0.176)\tLoss 0.5325 (0.5325)\tPrec 83.594% (83.594%)\n",
      "Epoch: [63][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.4059 (0.3782)\tPrec 87.500% (87.454%)\n",
      "Epoch: [63][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.4462 (0.3784)\tPrec 82.812% (87.449%)\n",
      "Epoch: [63][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4276 (0.3768)\tPrec 86.719% (87.396%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.170 (0.170)\tLoss 0.3589 (0.3589)\tPrec 87.500% (87.500%)\n",
      " * Prec 83.910% \n",
      "best acc: 83.910000\n",
      "Epoch: [64][0/391]\tTime 0.235 (0.235)\tData 0.197 (0.197)\tLoss 0.3556 (0.3556)\tPrec 90.625% (90.625%)\n",
      "Epoch: [64][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3280 (0.3592)\tPrec 92.969% (87.902%)\n",
      "Epoch: [64][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2955 (0.3650)\tPrec 89.062% (87.858%)\n",
      "Epoch: [64][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2829 (0.3707)\tPrec 91.406% (87.638%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.165 (0.165)\tLoss 0.3349 (0.3349)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.050% \n",
      "best acc: 84.050000\n",
      "Epoch: [65][0/391]\tTime 0.246 (0.246)\tData 0.206 (0.206)\tLoss 0.3741 (0.3741)\tPrec 87.500% (87.500%)\n",
      "Epoch: [65][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.3877 (0.3610)\tPrec 84.375% (87.987%)\n",
      "Epoch: [65][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4136 (0.3700)\tPrec 84.375% (87.710%)\n",
      "Epoch: [65][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3412 (0.3686)\tPrec 89.844% (87.708%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.185 (0.185)\tLoss 0.3917 (0.3917)\tPrec 85.156% (85.156%)\n",
      " * Prec 84.300% \n",
      "best acc: 84.300000\n",
      "Epoch: [66][0/391]\tTime 0.272 (0.272)\tData 0.230 (0.230)\tLoss 0.3676 (0.3676)\tPrec 86.719% (86.719%)\n",
      "Epoch: [66][100/391]\tTime 0.045 (0.048)\tData 0.002 (0.004)\tLoss 0.3870 (0.3622)\tPrec 88.281% (88.011%)\n",
      "Epoch: [66][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.3435 (0.3636)\tPrec 90.625% (88.021%)\n",
      "Epoch: [66][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2856 (0.3622)\tPrec 91.406% (87.998%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.176 (0.176)\tLoss 0.4143 (0.4143)\tPrec 88.281% (88.281%)\n",
      " * Prec 83.770% \n",
      "best acc: 84.300000\n",
      "Epoch: [67][0/391]\tTime 0.185 (0.185)\tData 0.143 (0.143)\tLoss 0.3400 (0.3400)\tPrec 89.062% (89.062%)\n",
      "Epoch: [67][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2876 (0.3701)\tPrec 89.062% (87.825%)\n",
      "Epoch: [67][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3000 (0.3629)\tPrec 89.062% (88.180%)\n",
      "Epoch: [67][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2746 (0.3658)\tPrec 87.500% (87.897%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.183 (0.183)\tLoss 0.3732 (0.3732)\tPrec 89.062% (89.062%)\n",
      " * Prec 83.810% \n",
      "best acc: 84.300000\n",
      "Epoch: [68][0/391]\tTime 0.234 (0.234)\tData 0.194 (0.194)\tLoss 0.3562 (0.3562)\tPrec 88.281% (88.281%)\n",
      "Epoch: [68][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3523 (0.3653)\tPrec 89.062% (87.655%)\n",
      "Epoch: [68][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3811 (0.3686)\tPrec 85.156% (87.655%)\n",
      "Epoch: [68][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3728 (0.3683)\tPrec 88.281% (87.765%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.176 (0.176)\tLoss 0.4296 (0.4296)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.390% \n",
      "best acc: 84.390000\n",
      "Epoch: [69][0/391]\tTime 0.253 (0.253)\tData 0.200 (0.200)\tLoss 0.4469 (0.4469)\tPrec 83.594% (83.594%)\n",
      "Epoch: [69][100/391]\tTime 0.049 (0.048)\tData 0.001 (0.003)\tLoss 0.3829 (0.3681)\tPrec 88.281% (87.717%)\n",
      "Epoch: [69][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.4837 (0.3663)\tPrec 85.156% (87.994%)\n",
      "Epoch: [69][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.002)\tLoss 0.3248 (0.3688)\tPrec 87.500% (87.824%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.179 (0.179)\tLoss 0.4445 (0.4445)\tPrec 87.500% (87.500%)\n",
      " * Prec 83.760% \n",
      "best acc: 84.390000\n",
      "Epoch: [70][0/391]\tTime 0.207 (0.207)\tData 0.169 (0.169)\tLoss 0.3220 (0.3220)\tPrec 89.844% (89.844%)\n",
      "Epoch: [70][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.5059 (0.3557)\tPrec 82.031% (88.274%)\n",
      "Epoch: [70][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.3530 (0.3579)\tPrec 89.062% (88.184%)\n",
      "Epoch: [70][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4005 (0.3648)\tPrec 84.375% (87.988%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.3389 (0.3389)\tPrec 89.062% (89.062%)\n",
      " * Prec 83.600% \n",
      "best acc: 84.390000\n",
      "Epoch: [71][0/391]\tTime 0.229 (0.229)\tData 0.187 (0.187)\tLoss 0.3541 (0.3541)\tPrec 88.281% (88.281%)\n",
      "Epoch: [71][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3740 (0.3590)\tPrec 88.281% (87.987%)\n",
      "Epoch: [71][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2964 (0.3605)\tPrec 89.062% (87.916%)\n",
      "Epoch: [71][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2958 (0.3621)\tPrec 90.625% (87.895%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.190 (0.190)\tLoss 0.3587 (0.3587)\tPrec 87.500% (87.500%)\n",
      " * Prec 83.780% \n",
      "best acc: 84.390000\n",
      "Epoch: [72][0/391]\tTime 0.221 (0.221)\tData 0.183 (0.183)\tLoss 0.5995 (0.5995)\tPrec 81.250% (81.250%)\n",
      "Epoch: [72][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.4459 (0.3595)\tPrec 85.938% (87.771%)\n",
      "Epoch: [72][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3958 (0.3618)\tPrec 82.031% (87.531%)\n",
      "Epoch: [72][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4338 (0.3608)\tPrec 86.719% (87.692%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.170 (0.170)\tLoss 0.4007 (0.4007)\tPrec 86.719% (86.719%)\n",
      " * Prec 83.430% \n",
      "best acc: 84.390000\n",
      "Epoch: [73][0/391]\tTime 0.223 (0.223)\tData 0.180 (0.180)\tLoss 0.4272 (0.4272)\tPrec 86.719% (86.719%)\n",
      "Epoch: [73][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2565 (0.3547)\tPrec 93.750% (87.995%)\n",
      "Epoch: [73][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3319 (0.3591)\tPrec 87.500% (87.889%)\n",
      "Epoch: [73][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4291 (0.3620)\tPrec 85.938% (87.863%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.3385 (0.3385)\tPrec 89.062% (89.062%)\n",
      " * Prec 83.880% \n",
      "best acc: 84.390000\n",
      "Epoch: [74][0/391]\tTime 0.257 (0.257)\tData 0.219 (0.219)\tLoss 0.3848 (0.3848)\tPrec 85.156% (85.156%)\n",
      "Epoch: [74][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 0.2688 (0.3601)\tPrec 89.844% (87.964%)\n",
      "Epoch: [74][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.3101 (0.3591)\tPrec 88.281% (87.900%)\n",
      "Epoch: [74][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4389 (0.3615)\tPrec 88.281% (87.775%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.174 (0.174)\tLoss 0.4423 (0.4423)\tPrec 83.594% (83.594%)\n",
      " * Prec 83.510% \n",
      "best acc: 84.390000\n",
      "Epoch: [75][0/391]\tTime 0.262 (0.262)\tData 0.220 (0.220)\tLoss 0.5253 (0.5253)\tPrec 84.375% (84.375%)\n",
      "Epoch: [75][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.2739 (0.3440)\tPrec 92.969% (88.629%)\n",
      "Epoch: [75][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.3858 (0.3335)\tPrec 84.375% (88.989%)\n",
      "Epoch: [75][300/391]\tTime 0.045 (0.047)\tData 0.002 (0.002)\tLoss 0.3042 (0.3321)\tPrec 91.406% (88.985%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.185 (0.185)\tLoss 0.3790 (0.3790)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.760% \n",
      "best acc: 84.760000\n",
      "Epoch: [76][0/391]\tTime 0.218 (0.218)\tData 0.176 (0.176)\tLoss 0.3761 (0.3761)\tPrec 89.844% (89.844%)\n",
      "Epoch: [76][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2592 (0.3323)\tPrec 88.281% (89.148%)\n",
      "Epoch: [76][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3229 (0.3290)\tPrec 88.281% (89.167%)\n",
      "Epoch: [76][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2456 (0.3272)\tPrec 92.969% (89.125%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.3501 (0.3501)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.830% \n",
      "best acc: 84.830000\n",
      "Epoch: [77][0/391]\tTime 0.215 (0.215)\tData 0.174 (0.174)\tLoss 0.2622 (0.2622)\tPrec 92.188% (92.188%)\n",
      "Epoch: [77][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2149 (0.3245)\tPrec 93.750% (89.124%)\n",
      "Epoch: [77][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3397 (0.3200)\tPrec 87.500% (89.354%)\n",
      "Epoch: [77][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2560 (0.3174)\tPrec 92.188% (89.473%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.176 (0.176)\tLoss 0.3050 (0.3050)\tPrec 90.625% (90.625%)\n",
      " * Prec 84.870% \n",
      "best acc: 84.870000\n",
      "Epoch: [78][0/391]\tTime 0.236 (0.236)\tData 0.197 (0.197)\tLoss 0.4247 (0.4247)\tPrec 85.938% (85.938%)\n",
      "Epoch: [78][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.5412 (0.3294)\tPrec 83.594% (89.001%)\n",
      "Epoch: [78][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2817 (0.3153)\tPrec 87.500% (89.490%)\n",
      "Epoch: [78][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3349 (0.3204)\tPrec 85.938% (89.234%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.187 (0.187)\tLoss 0.4162 (0.4162)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.870% \n",
      "best acc: 84.870000\n",
      "Epoch: [79][0/391]\tTime 0.243 (0.243)\tData 0.203 (0.203)\tLoss 0.2095 (0.2095)\tPrec 91.406% (91.406%)\n",
      "Epoch: [79][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3145 (0.3221)\tPrec 86.719% (89.279%)\n",
      "Epoch: [79][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3075 (0.3262)\tPrec 88.281% (89.094%)\n",
      "Epoch: [79][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4190 (0.3233)\tPrec 89.844% (89.268%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.195 (0.195)\tLoss 0.3651 (0.3651)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.250% \n",
      "best acc: 85.250000\n",
      "Epoch: [80][0/391]\tTime 0.208 (0.208)\tData 0.169 (0.169)\tLoss 0.2555 (0.2555)\tPrec 92.188% (92.188%)\n",
      "Epoch: [80][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.2374 (0.3136)\tPrec 92.969% (89.233%)\n",
      "Epoch: [80][200/391]\tTime 0.048 (0.047)\tData 0.001 (0.002)\tLoss 0.4671 (0.3184)\tPrec 83.594% (89.117%)\n",
      "Epoch: [80][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2834 (0.3201)\tPrec 89.844% (89.156%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.184 (0.184)\tLoss 0.3171 (0.3171)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.190% \n",
      "best acc: 85.250000\n",
      "Epoch: [81][0/391]\tTime 0.258 (0.258)\tData 0.218 (0.218)\tLoss 0.3354 (0.3354)\tPrec 92.188% (92.188%)\n",
      "Epoch: [81][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.004)\tLoss 0.2736 (0.3184)\tPrec 91.406% (89.140%)\n",
      "Epoch: [81][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2973 (0.3269)\tPrec 84.375% (88.837%)\n",
      "Epoch: [81][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2286 (0.3203)\tPrec 91.406% (89.000%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.191 (0.191)\tLoss 0.3883 (0.3883)\tPrec 84.375% (84.375%)\n",
      " * Prec 84.780% \n",
      "best acc: 85.250000\n",
      "Epoch: [82][0/391]\tTime 0.240 (0.240)\tData 0.199 (0.199)\tLoss 0.3248 (0.3248)\tPrec 85.938% (85.938%)\n",
      "Epoch: [82][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2604 (0.3135)\tPrec 90.625% (89.271%)\n",
      "Epoch: [82][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.4329 (0.3152)\tPrec 83.594% (89.327%)\n",
      "Epoch: [82][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4486 (0.3179)\tPrec 85.938% (89.229%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.173 (0.173)\tLoss 0.3014 (0.3014)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.500% \n",
      "best acc: 85.500000\n",
      "Epoch: [83][0/391]\tTime 0.237 (0.237)\tData 0.196 (0.196)\tLoss 0.3144 (0.3144)\tPrec 90.625% (90.625%)\n",
      "Epoch: [83][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.4448 (0.3196)\tPrec 87.500% (89.295%)\n",
      "Epoch: [83][200/391]\tTime 0.045 (0.047)\tData 0.001 (0.002)\tLoss 0.4102 (0.3240)\tPrec 85.156% (89.039%)\n",
      "Epoch: [83][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3743 (0.3177)\tPrec 88.281% (89.252%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.171 (0.171)\tLoss 0.3456 (0.3456)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.930% \n",
      "best acc: 85.500000\n",
      "Epoch: [84][0/391]\tTime 0.209 (0.209)\tData 0.170 (0.170)\tLoss 0.3231 (0.3231)\tPrec 89.844% (89.844%)\n",
      "Epoch: [84][100/391]\tTime 0.047 (0.048)\tData 0.001 (0.003)\tLoss 0.2168 (0.3133)\tPrec 94.531% (89.356%)\n",
      "Epoch: [84][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2985 (0.3129)\tPrec 89.062% (89.494%)\n",
      "Epoch: [84][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3418 (0.3171)\tPrec 85.156% (89.314%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.172 (0.172)\tLoss 0.3812 (0.3812)\tPrec 85.156% (85.156%)\n",
      " * Prec 84.710% \n",
      "best acc: 85.500000\n",
      "Epoch: [85][0/391]\tTime 0.202 (0.202)\tData 0.163 (0.163)\tLoss 0.3297 (0.3297)\tPrec 90.625% (90.625%)\n",
      "Epoch: [85][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.4656 (0.3125)\tPrec 82.812% (89.418%)\n",
      "Epoch: [85][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2558 (0.3140)\tPrec 89.062% (89.443%)\n",
      "Epoch: [85][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3499 (0.3145)\tPrec 87.500% (89.374%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.2900 (0.2900)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.710% \n",
      "best acc: 85.500000\n",
      "Epoch: [86][0/391]\tTime 0.211 (0.211)\tData 0.172 (0.172)\tLoss 0.3080 (0.3080)\tPrec 90.625% (90.625%)\n",
      "Epoch: [86][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2546 (0.3196)\tPrec 92.969% (89.650%)\n",
      "Epoch: [86][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2152 (0.3157)\tPrec 92.969% (89.498%)\n",
      "Epoch: [86][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2737 (0.3174)\tPrec 92.188% (89.345%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.185 (0.185)\tLoss 0.3583 (0.3583)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.260% \n",
      "best acc: 85.500000\n",
      "Epoch: [87][0/391]\tTime 0.272 (0.272)\tData 0.233 (0.233)\tLoss 0.4121 (0.4121)\tPrec 86.719% (86.719%)\n",
      "Epoch: [87][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 0.2025 (0.3178)\tPrec 92.188% (89.380%)\n",
      "Epoch: [87][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.4707 (0.3131)\tPrec 84.375% (89.622%)\n",
      "Epoch: [87][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3029 (0.3157)\tPrec 92.188% (89.449%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.171 (0.171)\tLoss 0.3523 (0.3523)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.840% \n",
      "best acc: 85.500000\n",
      "Epoch: [88][0/391]\tTime 0.231 (0.231)\tData 0.188 (0.188)\tLoss 0.2358 (0.2358)\tPrec 92.969% (92.969%)\n",
      "Epoch: [88][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.3000 (0.3171)\tPrec 88.281% (89.341%)\n",
      "Epoch: [88][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3456 (0.3178)\tPrec 90.625% (89.381%)\n",
      "Epoch: [88][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2143 (0.3182)\tPrec 92.969% (89.335%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.168 (0.168)\tLoss 0.4038 (0.4038)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.910% \n",
      "best acc: 85.500000\n",
      "Epoch: [89][0/391]\tTime 0.231 (0.231)\tData 0.180 (0.180)\tLoss 0.3351 (0.3351)\tPrec 88.281% (88.281%)\n",
      "Epoch: [89][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2542 (0.3111)\tPrec 91.406% (89.828%)\n",
      "Epoch: [89][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3014 (0.3102)\tPrec 92.188% (89.739%)\n",
      "Epoch: [89][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2091 (0.3067)\tPrec 92.188% (89.789%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.188 (0.188)\tLoss 0.3068 (0.3068)\tPrec 92.969% (92.969%)\n",
      " * Prec 85.190% \n",
      "best acc: 85.500000\n",
      "Epoch: [90][0/391]\tTime 0.206 (0.206)\tData 0.166 (0.166)\tLoss 0.3117 (0.3117)\tPrec 88.281% (88.281%)\n",
      "Epoch: [90][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.2888 (0.3033)\tPrec 88.281% (89.821%)\n",
      "Epoch: [90][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3372 (0.3042)\tPrec 87.500% (89.848%)\n",
      "Epoch: [90][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2960 (0.3077)\tPrec 92.969% (89.828%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.150 (0.150)\tLoss 0.3194 (0.3194)\tPrec 85.156% (85.156%)\n",
      " * Prec 85.410% \n",
      "best acc: 85.500000\n",
      "Epoch: [91][0/391]\tTime 0.248 (0.248)\tData 0.209 (0.209)\tLoss 0.1702 (0.1702)\tPrec 96.094% (96.094%)\n",
      "Epoch: [91][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2669 (0.3156)\tPrec 93.750% (89.318%)\n",
      "Epoch: [91][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4121 (0.3091)\tPrec 88.281% (89.669%)\n",
      "Epoch: [91][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4301 (0.3080)\tPrec 86.719% (89.685%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.188 (0.188)\tLoss 0.2819 (0.2819)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.060% \n",
      "best acc: 85.500000\n",
      "Epoch: [92][0/391]\tTime 0.241 (0.241)\tData 0.202 (0.202)\tLoss 0.3304 (0.3304)\tPrec 91.406% (91.406%)\n",
      "Epoch: [92][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3237 (0.3019)\tPrec 90.625% (89.828%)\n",
      "Epoch: [92][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3313 (0.3025)\tPrec 89.062% (89.937%)\n",
      "Epoch: [92][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4176 (0.3082)\tPrec 86.719% (89.709%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.192 (0.192)\tLoss 0.2665 (0.2665)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.200% \n",
      "best acc: 85.500000\n",
      "Epoch: [93][0/391]\tTime 0.223 (0.223)\tData 0.182 (0.182)\tLoss 0.1947 (0.1947)\tPrec 94.531% (94.531%)\n",
      "Epoch: [93][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3185 (0.3053)\tPrec 88.281% (89.898%)\n",
      "Epoch: [93][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3407 (0.3019)\tPrec 89.844% (89.910%)\n",
      "Epoch: [93][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3041 (0.2994)\tPrec 89.844% (89.937%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.188 (0.188)\tLoss 0.3611 (0.3611)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.350% \n",
      "best acc: 85.500000\n",
      "Epoch: [94][0/391]\tTime 0.271 (0.271)\tData 0.231 (0.231)\tLoss 0.2544 (0.2544)\tPrec 90.625% (90.625%)\n",
      "Epoch: [94][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.2242 (0.3034)\tPrec 90.625% (89.805%)\n",
      "Epoch: [94][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.3093 (0.3037)\tPrec 89.844% (89.867%)\n",
      "Epoch: [94][300/391]\tTime 0.048 (0.047)\tData 0.001 (0.002)\tLoss 0.2637 (0.3053)\tPrec 92.188% (89.854%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.172 (0.172)\tLoss 0.4122 (0.4122)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.210% \n",
      "best acc: 85.500000\n",
      "Epoch: [95][0/391]\tTime 0.221 (0.221)\tData 0.177 (0.177)\tLoss 0.2968 (0.2968)\tPrec 88.281% (88.281%)\n",
      "Epoch: [95][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3098 (0.2981)\tPrec 88.281% (89.766%)\n",
      "Epoch: [95][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2634 (0.2977)\tPrec 92.188% (89.817%)\n",
      "Epoch: [95][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3212 (0.2977)\tPrec 90.625% (89.924%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.154 (0.154)\tLoss 0.3214 (0.3214)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.150% \n",
      "best acc: 85.500000\n",
      "Epoch: [96][0/391]\tTime 0.244 (0.244)\tData 0.195 (0.195)\tLoss 0.3065 (0.3065)\tPrec 89.844% (89.844%)\n",
      "Epoch: [96][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3462 (0.3060)\tPrec 89.062% (89.867%)\n",
      "Epoch: [96][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.2014 (0.3065)\tPrec 92.969% (89.848%)\n",
      "Epoch: [96][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3201 (0.3068)\tPrec 88.281% (89.818%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.177 (0.177)\tLoss 0.3266 (0.3266)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.290% \n",
      "best acc: 85.500000\n",
      "Epoch: [97][0/391]\tTime 0.282 (0.282)\tData 0.243 (0.243)\tLoss 0.2197 (0.2197)\tPrec 91.406% (91.406%)\n",
      "Epoch: [97][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.2948 (0.2989)\tPrec 90.625% (90.084%)\n",
      "Epoch: [97][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.2238 (0.2977)\tPrec 92.969% (90.116%)\n",
      "Epoch: [97][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3171 (0.2984)\tPrec 89.844% (90.002%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.165 (0.165)\tLoss 0.3210 (0.3210)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.210% \n",
      "best acc: 85.500000\n",
      "Epoch: [98][0/391]\tTime 0.220 (0.220)\tData 0.183 (0.183)\tLoss 0.3027 (0.3027)\tPrec 91.406% (91.406%)\n",
      "Epoch: [98][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3271 (0.3048)\tPrec 88.281% (89.906%)\n",
      "Epoch: [98][200/391]\tTime 0.047 (0.047)\tData 0.001 (0.002)\tLoss 0.2104 (0.3058)\tPrec 90.625% (89.937%)\n",
      "Epoch: [98][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.1972 (0.3039)\tPrec 93.750% (89.942%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.184 (0.184)\tLoss 0.3306 (0.3306)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.330% \n",
      "best acc: 85.500000\n",
      "Epoch: [99][0/391]\tTime 0.234 (0.234)\tData 0.196 (0.196)\tLoss 0.2986 (0.2986)\tPrec 92.188% (92.188%)\n",
      "Epoch: [99][100/391]\tTime 0.045 (0.048)\tData 0.001 (0.003)\tLoss 0.2396 (0.2981)\tPrec 92.969% (90.145%)\n",
      "Epoch: [99][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3707 (0.3041)\tPrec 87.500% (89.875%)\n",
      "Epoch: [99][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2394 (0.3053)\tPrec 91.406% (89.833%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.136 (0.136)\tLoss 0.2620 (0.2620)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.170% \n",
      "best acc: 85.500000\n",
      "Epoch: [100][0/391]\tTime 0.248 (0.248)\tData 0.211 (0.211)\tLoss 0.2711 (0.2711)\tPrec 91.406% (91.406%)\n",
      "Epoch: [100][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.2978 (0.3013)\tPrec 90.625% (89.650%)\n",
      "Epoch: [100][200/391]\tTime 0.047 (0.047)\tData 0.001 (0.002)\tLoss 0.2485 (0.3032)\tPrec 91.406% (89.762%)\n",
      "Epoch: [100][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4501 (0.3034)\tPrec 82.812% (89.776%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.3081 (0.3081)\tPrec 91.406% (91.406%)\n",
      " * Prec 85.180% \n",
      "best acc: 85.500000\n",
      "Epoch: [101][0/391]\tTime 0.231 (0.231)\tData 0.192 (0.192)\tLoss 0.4433 (0.4433)\tPrec 85.156% (85.156%)\n",
      "Epoch: [101][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2440 (0.3040)\tPrec 92.188% (89.882%)\n",
      "Epoch: [101][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2645 (0.2965)\tPrec 89.062% (90.139%)\n",
      "Epoch: [101][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4080 (0.2994)\tPrec 89.844% (90.044%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.194 (0.194)\tLoss 0.3085 (0.3085)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.260% \n",
      "best acc: 85.500000\n",
      "Epoch: [102][0/391]\tTime 0.242 (0.242)\tData 0.205 (0.205)\tLoss 0.2526 (0.2526)\tPrec 89.062% (89.062%)\n",
      "Epoch: [102][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3597 (0.2920)\tPrec 85.938% (89.998%)\n",
      "Epoch: [102][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2003 (0.2992)\tPrec 92.969% (89.750%)\n",
      "Epoch: [102][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3091 (0.2981)\tPrec 90.625% (89.906%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.186 (0.186)\tLoss 0.3300 (0.3300)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.300% \n",
      "best acc: 85.500000\n",
      "Epoch: [103][0/391]\tTime 0.203 (0.203)\tData 0.164 (0.164)\tLoss 0.2859 (0.2859)\tPrec 89.062% (89.062%)\n",
      "Epoch: [103][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3081 (0.3059)\tPrec 87.500% (89.705%)\n",
      "Epoch: [103][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3701 (0.3020)\tPrec 87.500% (89.933%)\n",
      "Epoch: [103][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2341 (0.3028)\tPrec 90.625% (89.761%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.173 (0.173)\tLoss 0.3743 (0.3743)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.510% \n",
      "best acc: 85.510000\n",
      "Epoch: [104][0/391]\tTime 0.203 (0.203)\tData 0.164 (0.164)\tLoss 0.2650 (0.2650)\tPrec 90.625% (90.625%)\n",
      "Epoch: [104][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2652 (0.2942)\tPrec 90.625% (90.114%)\n",
      "Epoch: [104][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.2470 (0.3008)\tPrec 91.406% (89.941%)\n",
      "Epoch: [104][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4499 (0.2963)\tPrec 87.500% (90.132%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.180 (0.180)\tLoss 0.3160 (0.3160)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.360% \n",
      "best acc: 85.510000\n",
      "Epoch: [105][0/391]\tTime 0.222 (0.222)\tData 0.181 (0.181)\tLoss 0.5029 (0.5029)\tPrec 82.812% (82.812%)\n",
      "Epoch: [105][100/391]\tTime 0.048 (0.048)\tData 0.002 (0.003)\tLoss 0.3111 (0.3007)\tPrec 91.406% (89.898%)\n",
      "Epoch: [105][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.3078 (0.2955)\tPrec 89.062% (90.116%)\n",
      "Epoch: [105][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3029 (0.2971)\tPrec 92.188% (90.080%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.178 (0.178)\tLoss 0.3617 (0.3617)\tPrec 83.594% (83.594%)\n",
      " * Prec 85.320% \n",
      "best acc: 85.510000\n",
      "Epoch: [106][0/391]\tTime 0.232 (0.232)\tData 0.174 (0.174)\tLoss 0.2449 (0.2449)\tPrec 92.969% (92.969%)\n",
      "Epoch: [106][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.2181 (0.2968)\tPrec 93.750% (90.045%)\n",
      "Epoch: [106][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2194 (0.2941)\tPrec 92.188% (90.194%)\n",
      "Epoch: [106][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3421 (0.2968)\tPrec 86.719% (90.155%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.198 (0.198)\tLoss 0.2875 (0.2875)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.210% \n",
      "best acc: 85.510000\n",
      "Epoch: [107][0/391]\tTime 0.209 (0.209)\tData 0.171 (0.171)\tLoss 0.2080 (0.2080)\tPrec 93.750% (93.750%)\n",
      "Epoch: [107][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2096 (0.3083)\tPrec 92.188% (89.627%)\n",
      "Epoch: [107][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2197 (0.3065)\tPrec 93.750% (89.715%)\n",
      "Epoch: [107][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2670 (0.3031)\tPrec 89.844% (89.841%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.166 (0.166)\tLoss 0.2741 (0.2741)\tPrec 91.406% (91.406%)\n",
      " * Prec 85.520% \n",
      "best acc: 85.520000\n",
      "Epoch: [108][0/391]\tTime 0.258 (0.258)\tData 0.216 (0.216)\tLoss 0.2006 (0.2006)\tPrec 92.969% (92.969%)\n",
      "Epoch: [108][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.2507 (0.3042)\tPrec 91.406% (89.720%)\n",
      "Epoch: [108][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.2261 (0.3039)\tPrec 92.188% (89.789%)\n",
      "Epoch: [108][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3697 (0.3038)\tPrec 85.938% (89.849%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.191 (0.191)\tLoss 0.2844 (0.2844)\tPrec 91.406% (91.406%)\n",
      " * Prec 85.480% \n",
      "best acc: 85.520000\n",
      "Epoch: [109][0/391]\tTime 0.266 (0.266)\tData 0.226 (0.226)\tLoss 0.3257 (0.3257)\tPrec 89.844% (89.844%)\n",
      "Epoch: [109][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.2505 (0.2903)\tPrec 92.188% (90.377%)\n",
      "Epoch: [109][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.3324 (0.2922)\tPrec 91.406% (90.283%)\n",
      "Epoch: [109][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2785 (0.2958)\tPrec 92.969% (90.142%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.204 (0.204)\tLoss 0.3415 (0.3415)\tPrec 87.500% (87.500%)\n",
      " * Prec 85.090% \n",
      "best acc: 85.520000\n",
      "Epoch: [110][0/391]\tTime 0.256 (0.256)\tData 0.216 (0.216)\tLoss 0.4026 (0.4026)\tPrec 83.594% (83.594%)\n",
      "Epoch: [110][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.3597 (0.2974)\tPrec 88.281% (90.053%)\n",
      "Epoch: [110][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.2712 (0.3002)\tPrec 89.844% (89.980%)\n",
      "Epoch: [110][300/391]\tTime 0.052 (0.047)\tData 0.001 (0.002)\tLoss 0.2689 (0.2973)\tPrec 89.844% (90.134%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.174 (0.174)\tLoss 0.3922 (0.3922)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.390% \n",
      "best acc: 85.520000\n",
      "Epoch: [111][0/391]\tTime 0.233 (0.233)\tData 0.188 (0.188)\tLoss 0.3029 (0.3029)\tPrec 89.844% (89.844%)\n",
      "Epoch: [111][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2796 (0.2985)\tPrec 89.062% (89.921%)\n",
      "Epoch: [111][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3320 (0.3039)\tPrec 89.844% (89.704%)\n",
      "Epoch: [111][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2384 (0.3041)\tPrec 94.531% (89.831%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.186 (0.186)\tLoss 0.3019 (0.3019)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.020% \n",
      "best acc: 85.520000\n",
      "Epoch: [112][0/391]\tTime 0.208 (0.208)\tData 0.169 (0.169)\tLoss 0.2272 (0.2272)\tPrec 92.969% (92.969%)\n",
      "Epoch: [112][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2658 (0.3030)\tPrec 94.531% (90.006%)\n",
      "Epoch: [112][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2749 (0.3013)\tPrec 89.062% (90.089%)\n",
      "Epoch: [112][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3785 (0.3042)\tPrec 86.719% (89.942%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.174 (0.174)\tLoss 0.3354 (0.3354)\tPrec 89.844% (89.844%)\n",
      " * Prec 84.830% \n",
      "best acc: 85.520000\n",
      "Epoch: [113][0/391]\tTime 0.215 (0.215)\tData 0.161 (0.161)\tLoss 0.2534 (0.2534)\tPrec 90.625% (90.625%)\n",
      "Epoch: [113][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2567 (0.3022)\tPrec 90.625% (89.983%)\n",
      "Epoch: [113][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3177 (0.3023)\tPrec 89.062% (90.023%)\n",
      "Epoch: [113][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2034 (0.3035)\tPrec 92.188% (89.877%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.152 (0.152)\tLoss 0.3122 (0.3122)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.990% \n",
      "best acc: 85.520000\n",
      "Epoch: [114][0/391]\tTime 0.238 (0.238)\tData 0.199 (0.199)\tLoss 0.2545 (0.2545)\tPrec 91.406% (91.406%)\n",
      "Epoch: [114][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.4111 (0.3056)\tPrec 88.281% (89.565%)\n",
      "Epoch: [114][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3200 (0.3051)\tPrec 92.188% (89.665%)\n",
      "Epoch: [114][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2510 (0.3051)\tPrec 90.625% (89.743%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.168 (0.168)\tLoss 0.3359 (0.3359)\tPrec 89.062% (89.062%)\n",
      " * Prec 85.060% \n",
      "best acc: 85.520000\n",
      "Epoch: [115][0/391]\tTime 0.207 (0.207)\tData 0.169 (0.169)\tLoss 0.3320 (0.3320)\tPrec 87.500% (87.500%)\n",
      "Epoch: [115][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2094 (0.3051)\tPrec 92.969% (89.797%)\n",
      "Epoch: [115][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3660 (0.3065)\tPrec 85.938% (89.696%)\n",
      "Epoch: [115][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3529 (0.3059)\tPrec 89.844% (89.828%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.183 (0.183)\tLoss 0.3043 (0.3043)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.240% \n",
      "best acc: 85.520000\n",
      "Epoch: [116][0/391]\tTime 0.214 (0.214)\tData 0.156 (0.156)\tLoss 0.3255 (0.3255)\tPrec 84.375% (84.375%)\n",
      "Epoch: [116][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2546 (0.3052)\tPrec 92.969% (89.867%)\n",
      "Epoch: [116][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.3846 (0.3000)\tPrec 84.375% (90.058%)\n",
      "Epoch: [116][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2285 (0.3053)\tPrec 94.531% (89.901%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.190 (0.190)\tLoss 0.3005 (0.3005)\tPrec 91.406% (91.406%)\n",
      " * Prec 84.990% \n",
      "best acc: 85.520000\n",
      "Epoch: [117][0/391]\tTime 0.265 (0.265)\tData 0.216 (0.216)\tLoss 0.4987 (0.4987)\tPrec 87.500% (87.500%)\n",
      "Epoch: [117][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.2339 (0.2962)\tPrec 92.188% (90.153%)\n",
      "Epoch: [117][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.1833 (0.3016)\tPrec 92.969% (89.937%)\n",
      "Epoch: [117][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3037 (0.3068)\tPrec 92.188% (89.748%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.189 (0.189)\tLoss 0.2843 (0.2843)\tPrec 91.406% (91.406%)\n",
      " * Prec 84.770% \n",
      "best acc: 85.520000\n",
      "Epoch: [118][0/391]\tTime 0.257 (0.257)\tData 0.219 (0.219)\tLoss 0.3183 (0.3183)\tPrec 86.719% (86.719%)\n",
      "Epoch: [118][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.3261 (0.3210)\tPrec 89.844% (89.233%)\n",
      "Epoch: [118][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.3410 (0.3151)\tPrec 87.500% (89.568%)\n",
      "Epoch: [118][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3193 (0.3134)\tPrec 90.625% (89.574%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.175 (0.175)\tLoss 0.3705 (0.3705)\tPrec 85.938% (85.938%)\n",
      " * Prec 84.970% \n",
      "best acc: 85.520000\n",
      "Epoch: [119][0/391]\tTime 0.217 (0.217)\tData 0.177 (0.177)\tLoss 0.2333 (0.2333)\tPrec 92.188% (92.188%)\n",
      "Epoch: [119][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2773 (0.3135)\tPrec 86.719% (89.558%)\n",
      "Epoch: [119][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3536 (0.3053)\tPrec 85.938% (89.688%)\n",
      "Epoch: [119][300/391]\tTime 0.047 (0.047)\tData 0.002 (0.002)\tLoss 0.3114 (0.3080)\tPrec 89.062% (89.587%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.183 (0.183)\tLoss 0.3056 (0.3056)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.210% \n",
      "best acc: 85.520000\n",
      "Epoch: [120][0/391]\tTime 0.235 (0.235)\tData 0.187 (0.187)\tLoss 0.2980 (0.2980)\tPrec 91.406% (91.406%)\n",
      "Epoch: [120][100/391]\tTime 0.042 (0.048)\tData 0.002 (0.003)\tLoss 0.3282 (0.3174)\tPrec 90.625% (89.596%)\n",
      "Epoch: [120][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.2559 (0.3097)\tPrec 91.406% (89.824%)\n",
      "Epoch: [120][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2553 (0.3066)\tPrec 90.625% (89.877%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.162 (0.162)\tLoss 0.3227 (0.3227)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.350% \n",
      "best acc: 85.520000\n",
      "Epoch: [121][0/391]\tTime 0.245 (0.245)\tData 0.197 (0.197)\tLoss 0.3017 (0.3017)\tPrec 89.062% (89.062%)\n",
      "Epoch: [121][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.4538 (0.3016)\tPrec 86.719% (89.735%)\n",
      "Epoch: [121][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3311 (0.3054)\tPrec 86.719% (89.700%)\n",
      "Epoch: [121][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.2934 (0.3095)\tPrec 87.500% (89.613%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.149 (0.149)\tLoss 0.3457 (0.3457)\tPrec 89.844% (89.844%)\n",
      " * Prec 84.890% \n",
      "best acc: 85.520000\n",
      "Epoch: [122][0/391]\tTime 0.206 (0.206)\tData 0.168 (0.168)\tLoss 0.3032 (0.3032)\tPrec 89.062% (89.062%)\n",
      "Epoch: [122][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.3619 (0.3170)\tPrec 87.500% (89.233%)\n",
      "Epoch: [122][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3206 (0.3176)\tPrec 89.844% (89.319%)\n",
      "Epoch: [122][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3812 (0.3131)\tPrec 87.500% (89.478%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.185 (0.185)\tLoss 0.3493 (0.3493)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.510% \n",
      "best acc: 85.520000\n",
      "Epoch: [123][0/391]\tTime 0.257 (0.257)\tData 0.218 (0.218)\tLoss 0.3022 (0.3022)\tPrec 89.062% (89.062%)\n",
      "Epoch: [123][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.2887 (0.3086)\tPrec 90.625% (89.619%)\n",
      "Epoch: [123][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.3472 (0.3099)\tPrec 86.719% (89.548%)\n",
      "Epoch: [123][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4686 (0.3119)\tPrec 85.156% (89.436%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.190 (0.190)\tLoss 0.3278 (0.3278)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.890% \n",
      "best acc: 85.520000\n",
      "Epoch: [124][0/391]\tTime 0.254 (0.254)\tData 0.214 (0.214)\tLoss 0.3547 (0.3547)\tPrec 85.938% (85.938%)\n",
      "Epoch: [124][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.2367 (0.3054)\tPrec 94.531% (89.534%)\n",
      "Epoch: [124][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.3644 (0.3179)\tPrec 89.844% (89.226%)\n",
      "Epoch: [124][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3212 (0.3151)\tPrec 90.625% (89.369%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.191 (0.191)\tLoss 0.2777 (0.2777)\tPrec 92.969% (92.969%)\n",
      " * Prec 84.970% \n",
      "best acc: 85.520000\n",
      "Epoch: [125][0/391]\tTime 0.233 (0.233)\tData 0.187 (0.187)\tLoss 0.2274 (0.2274)\tPrec 94.531% (94.531%)\n",
      "Epoch: [125][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3523 (0.3146)\tPrec 88.281% (89.364%)\n",
      "Epoch: [125][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2569 (0.3114)\tPrec 92.969% (89.455%)\n",
      "Epoch: [125][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2809 (0.3101)\tPrec 91.406% (89.631%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.193 (0.193)\tLoss 0.3220 (0.3220)\tPrec 89.844% (89.844%)\n",
      " * Prec 84.850% \n",
      "best acc: 85.520000\n",
      "Epoch: [126][0/391]\tTime 0.261 (0.261)\tData 0.220 (0.220)\tLoss 0.2651 (0.2651)\tPrec 88.281% (88.281%)\n",
      "Epoch: [126][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.3072 (0.3074)\tPrec 90.625% (89.658%)\n",
      "Epoch: [126][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2808 (0.3066)\tPrec 88.281% (89.750%)\n",
      "Epoch: [126][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2517 (0.3056)\tPrec 90.625% (89.701%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.135 (0.135)\tLoss 0.3297 (0.3297)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.930% \n",
      "best acc: 85.520000\n",
      "Epoch: [127][0/391]\tTime 0.230 (0.230)\tData 0.192 (0.192)\tLoss 0.2643 (0.2643)\tPrec 90.625% (90.625%)\n",
      "Epoch: [127][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.2845 (0.3211)\tPrec 93.750% (89.302%)\n",
      "Epoch: [127][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2899 (0.3110)\tPrec 87.500% (89.564%)\n",
      "Epoch: [127][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3092 (0.3109)\tPrec 89.844% (89.608%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.158 (0.158)\tLoss 0.3228 (0.3228)\tPrec 89.844% (89.844%)\n",
      " * Prec 84.740% \n",
      "best acc: 85.520000\n",
      "Epoch: [128][0/391]\tTime 0.241 (0.241)\tData 0.202 (0.202)\tLoss 0.3443 (0.3443)\tPrec 85.938% (85.938%)\n",
      "Epoch: [128][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 0.3736 (0.3083)\tPrec 87.500% (89.689%)\n",
      "Epoch: [128][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.2909 (0.3077)\tPrec 88.281% (89.727%)\n",
      "Epoch: [128][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2814 (0.3106)\tPrec 85.938% (89.602%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.184 (0.184)\tLoss 0.3270 (0.3270)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.960% \n",
      "best acc: 85.520000\n",
      "Epoch: [129][0/391]\tTime 0.209 (0.209)\tData 0.171 (0.171)\tLoss 0.3275 (0.3275)\tPrec 90.625% (90.625%)\n",
      "Epoch: [129][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.2994 (0.3084)\tPrec 90.625% (89.503%)\n",
      "Epoch: [129][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.1826 (0.3109)\tPrec 93.750% (89.537%)\n",
      "Epoch: [129][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3234 (0.3086)\tPrec 89.844% (89.654%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.166 (0.166)\tLoss 0.3231 (0.3231)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.100% \n",
      "best acc: 85.520000\n",
      "Epoch: [130][0/391]\tTime 0.236 (0.236)\tData 0.197 (0.197)\tLoss 0.3525 (0.3525)\tPrec 90.625% (90.625%)\n",
      "Epoch: [130][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3850 (0.3053)\tPrec 86.719% (89.952%)\n",
      "Epoch: [130][200/391]\tTime 0.049 (0.047)\tData 0.001 (0.002)\tLoss 0.4053 (0.3154)\tPrec 87.500% (89.572%)\n",
      "Epoch: [130][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3417 (0.3124)\tPrec 88.281% (89.652%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.174 (0.174)\tLoss 0.3313 (0.3313)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.050% \n",
      "best acc: 85.520000\n",
      "Epoch: [131][0/391]\tTime 0.284 (0.284)\tData 0.244 (0.244)\tLoss 0.3445 (0.3445)\tPrec 87.500% (87.500%)\n",
      "Epoch: [131][100/391]\tTime 0.046 (0.049)\tData 0.001 (0.004)\tLoss 0.4913 (0.3138)\tPrec 87.500% (89.604%)\n",
      "Epoch: [131][200/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.1731 (0.3133)\tPrec 94.531% (89.541%)\n",
      "Epoch: [131][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3254 (0.3129)\tPrec 89.844% (89.473%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.194 (0.194)\tLoss 0.3003 (0.3003)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.130% \n",
      "best acc: 85.520000\n",
      "Epoch: [132][0/391]\tTime 0.243 (0.243)\tData 0.203 (0.203)\tLoss 0.3655 (0.3655)\tPrec 89.062% (89.062%)\n",
      "Epoch: [132][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.3093 (0.3035)\tPrec 89.844% (89.681%)\n",
      "Epoch: [132][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.3107 (0.3034)\tPrec 88.281% (89.657%)\n",
      "Epoch: [132][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2692 (0.3064)\tPrec 89.844% (89.589%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.201 (0.201)\tLoss 0.2724 (0.2724)\tPrec 91.406% (91.406%)\n",
      " * Prec 85.050% \n",
      "best acc: 85.520000\n",
      "Epoch: [133][0/391]\tTime 0.261 (0.261)\tData 0.224 (0.224)\tLoss 0.3285 (0.3285)\tPrec 87.500% (87.500%)\n",
      "Epoch: [133][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.2948 (0.3158)\tPrec 90.625% (89.558%)\n",
      "Epoch: [133][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.003)\tLoss 0.2405 (0.3145)\tPrec 93.750% (89.579%)\n",
      "Epoch: [133][300/391]\tTime 0.047 (0.047)\tData 0.001 (0.002)\tLoss 0.3975 (0.3119)\tPrec 84.375% (89.659%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.174 (0.174)\tLoss 0.3025 (0.3025)\tPrec 87.500% (87.500%)\n",
      " * Prec 84.840% \n",
      "best acc: 85.520000\n",
      "Epoch: [134][0/391]\tTime 0.266 (0.266)\tData 0.219 (0.219)\tLoss 0.3016 (0.3016)\tPrec 89.062% (89.062%)\n",
      "Epoch: [134][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.3225 (0.3027)\tPrec 86.719% (89.875%)\n",
      "Epoch: [134][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.3855 (0.3094)\tPrec 88.281% (89.603%)\n",
      "Epoch: [134][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3138 (0.3116)\tPrec 89.844% (89.561%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.177 (0.177)\tLoss 0.3874 (0.3874)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.710% \n",
      "best acc: 85.520000\n",
      "Epoch: [135][0/391]\tTime 0.252 (0.252)\tData 0.203 (0.203)\tLoss 0.2819 (0.2819)\tPrec 90.625% (90.625%)\n",
      "Epoch: [135][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 0.3591 (0.3119)\tPrec 89.844% (89.333%)\n",
      "Epoch: [135][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.2204 (0.3162)\tPrec 93.750% (89.300%)\n",
      "Epoch: [135][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2975 (0.3145)\tPrec 91.406% (89.428%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.168 (0.168)\tLoss 0.3684 (0.3684)\tPrec 90.625% (90.625%)\n",
      " * Prec 84.810% \n",
      "best acc: 85.520000\n",
      "Epoch: [136][0/391]\tTime 0.242 (0.242)\tData 0.202 (0.202)\tLoss 0.1989 (0.1989)\tPrec 93.750% (93.750%)\n",
      "Epoch: [136][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.3252 (0.3025)\tPrec 89.062% (89.720%)\n",
      "Epoch: [136][200/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.2707 (0.3084)\tPrec 88.281% (89.673%)\n",
      "Epoch: [136][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3222 (0.3051)\tPrec 91.406% (89.815%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.203 (0.203)\tLoss 0.2943 (0.2943)\tPrec 91.406% (91.406%)\n",
      " * Prec 84.630% \n",
      "best acc: 85.520000\n",
      "Epoch: [137][0/391]\tTime 0.200 (0.200)\tData 0.160 (0.160)\tLoss 0.3244 (0.3244)\tPrec 89.844% (89.844%)\n",
      "Epoch: [137][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.3110 (0.3069)\tPrec 89.844% (89.573%)\n",
      "Epoch: [137][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3688 (0.3065)\tPrec 89.844% (89.611%)\n",
      "Epoch: [137][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3686 (0.3067)\tPrec 86.719% (89.636%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.193 (0.193)\tLoss 0.2992 (0.2992)\tPrec 90.625% (90.625%)\n",
      " * Prec 85.020% \n",
      "best acc: 85.520000\n",
      "Epoch: [138][0/391]\tTime 0.246 (0.246)\tData 0.207 (0.207)\tLoss 0.3626 (0.3626)\tPrec 87.500% (87.500%)\n",
      "Epoch: [138][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.004)\tLoss 0.2872 (0.3224)\tPrec 91.406% (89.148%)\n",
      "Epoch: [138][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.3225 (0.3165)\tPrec 89.062% (89.381%)\n",
      "Epoch: [138][300/391]\tTime 0.047 (0.047)\tData 0.001 (0.002)\tLoss 0.3590 (0.3137)\tPrec 87.500% (89.506%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.147 (0.147)\tLoss 0.3100 (0.3100)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.050% \n",
      "best acc: 85.520000\n",
      "Epoch: [139][0/391]\tTime 0.234 (0.234)\tData 0.196 (0.196)\tLoss 0.2176 (0.2176)\tPrec 93.750% (93.750%)\n",
      "Epoch: [139][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3187 (0.3111)\tPrec 89.062% (89.434%)\n",
      "Epoch: [139][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3142 (0.3080)\tPrec 87.500% (89.560%)\n",
      "Epoch: [139][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3484 (0.3053)\tPrec 85.156% (89.600%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.156 (0.156)\tLoss 0.3596 (0.3596)\tPrec 86.719% (86.719%)\n",
      " * Prec 84.600% \n",
      "best acc: 85.520000\n",
      "Epoch: [140][0/391]\tTime 0.246 (0.246)\tData 0.207 (0.207)\tLoss 0.3640 (0.3640)\tPrec 88.281% (88.281%)\n",
      "Epoch: [140][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.2622 (0.3164)\tPrec 89.844% (89.279%)\n",
      "Epoch: [140][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3072 (0.3145)\tPrec 89.062% (89.432%)\n",
      "Epoch: [140][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3766 (0.3138)\tPrec 86.719% (89.452%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.196 (0.196)\tLoss 0.3232 (0.3232)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.810% \n",
      "best acc: 85.520000\n",
      "Epoch: [141][0/391]\tTime 0.228 (0.228)\tData 0.187 (0.187)\tLoss 0.3965 (0.3965)\tPrec 89.062% (89.062%)\n",
      "Epoch: [141][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.3453 (0.3094)\tPrec 89.844% (89.612%)\n",
      "Epoch: [141][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3720 (0.3104)\tPrec 89.062% (89.634%)\n",
      "Epoch: [141][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3016 (0.3086)\tPrec 90.625% (89.701%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.207 (0.207)\tLoss 0.2987 (0.2987)\tPrec 90.625% (90.625%)\n",
      " * Prec 84.920% \n",
      "best acc: 85.520000\n",
      "Epoch: [142][0/391]\tTime 0.226 (0.226)\tData 0.187 (0.187)\tLoss 0.2628 (0.2628)\tPrec 92.969% (92.969%)\n",
      "Epoch: [142][100/391]\tTime 0.046 (0.048)\tData 0.002 (0.003)\tLoss 0.2554 (0.3131)\tPrec 92.969% (89.573%)\n",
      "Epoch: [142][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.002)\tLoss 0.3250 (0.3074)\tPrec 88.281% (89.591%)\n",
      "Epoch: [142][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3304 (0.3115)\tPrec 89.062% (89.397%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.193 (0.193)\tLoss 0.3258 (0.3258)\tPrec 88.281% (88.281%)\n",
      " * Prec 85.240% \n",
      "best acc: 85.520000\n",
      "Epoch: [143][0/391]\tTime 0.191 (0.191)\tData 0.151 (0.151)\tLoss 0.3317 (0.3317)\tPrec 89.844% (89.844%)\n",
      "Epoch: [143][100/391]\tTime 0.046 (0.047)\tData 0.002 (0.003)\tLoss 0.2533 (0.3258)\tPrec 90.625% (89.032%)\n",
      "Epoch: [143][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2807 (0.3160)\tPrec 90.625% (89.420%)\n",
      "Epoch: [143][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.3216 (0.3145)\tPrec 87.500% (89.296%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.211 (0.211)\tLoss 0.3503 (0.3503)\tPrec 85.938% (85.938%)\n",
      " * Prec 85.000% \n",
      "best acc: 85.520000\n",
      "Epoch: [144][0/391]\tTime 0.244 (0.244)\tData 0.205 (0.205)\tLoss 0.3293 (0.3293)\tPrec 89.062% (89.062%)\n",
      "Epoch: [144][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.4391 (0.3218)\tPrec 86.719% (89.186%)\n",
      "Epoch: [144][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4266 (0.3174)\tPrec 85.938% (89.265%)\n",
      "Epoch: [144][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2859 (0.3132)\tPrec 89.844% (89.421%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.4367 (0.4367)\tPrec 83.594% (83.594%)\n",
      " * Prec 84.990% \n",
      "best acc: 85.520000\n",
      "Epoch: [145][0/391]\tTime 0.248 (0.248)\tData 0.193 (0.193)\tLoss 0.3059 (0.3059)\tPrec 91.406% (91.406%)\n",
      "Epoch: [145][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3551 (0.3030)\tPrec 90.625% (89.728%)\n",
      "Epoch: [145][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.4015 (0.3072)\tPrec 86.719% (89.684%)\n",
      "Epoch: [145][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2976 (0.3093)\tPrec 89.062% (89.563%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.167 (0.167)\tLoss 0.3848 (0.3848)\tPrec 88.281% (88.281%)\n",
      " * Prec 84.480% \n",
      "best acc: 85.520000\n",
      "Epoch: [146][0/391]\tTime 0.243 (0.243)\tData 0.203 (0.203)\tLoss 0.2833 (0.2833)\tPrec 89.844% (89.844%)\n",
      "Epoch: [146][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.4563 (0.3242)\tPrec 85.938% (89.124%)\n",
      "Epoch: [146][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.1859 (0.3163)\tPrec 95.312% (89.292%)\n",
      "Epoch: [146][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2841 (0.3095)\tPrec 91.406% (89.519%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.186 (0.186)\tLoss 0.2930 (0.2930)\tPrec 90.625% (90.625%)\n",
      " * Prec 84.580% \n",
      "best acc: 85.520000\n",
      "Epoch: [147][0/391]\tTime 0.247 (0.247)\tData 0.207 (0.207)\tLoss 0.2726 (0.2726)\tPrec 92.969% (92.969%)\n",
      "Epoch: [147][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3245 (0.3072)\tPrec 89.062% (89.790%)\n",
      "Epoch: [147][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3308 (0.3082)\tPrec 88.281% (89.688%)\n",
      "Epoch: [147][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2587 (0.3100)\tPrec 91.406% (89.602%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.184 (0.184)\tLoss 0.2941 (0.2941)\tPrec 89.062% (89.062%)\n",
      " * Prec 84.900% \n",
      "best acc: 85.520000\n",
      "Epoch: [148][0/391]\tTime 0.194 (0.194)\tData 0.156 (0.156)\tLoss 0.3313 (0.3313)\tPrec 90.625% (90.625%)\n",
      "Epoch: [148][100/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.2577 (0.3170)\tPrec 88.281% (89.364%)\n",
      "Epoch: [148][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.3854 (0.3190)\tPrec 89.062% (89.366%)\n",
      "Epoch: [148][300/391]\tTime 0.046 (0.046)\tData 0.001 (0.002)\tLoss 0.2817 (0.3166)\tPrec 90.625% (89.462%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.162 (0.162)\tLoss 0.3023 (0.3023)\tPrec 90.625% (90.625%)\n",
      " * Prec 84.390% \n",
      "best acc: 85.520000\n",
      "Epoch: [149][0/391]\tTime 0.211 (0.211)\tData 0.160 (0.160)\tLoss 0.3439 (0.3439)\tPrec 89.062% (89.062%)\n",
      "Epoch: [149][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.2577 (0.3125)\tPrec 92.969% (89.596%)\n",
      "Epoch: [149][200/391]\tTime 0.045 (0.047)\tData 0.002 (0.002)\tLoss 0.3645 (0.3120)\tPrec 88.281% (89.599%)\n",
      "Epoch: [149][300/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2427 (0.3156)\tPrec 88.281% (89.506%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.183 (0.183)\tLoss 0.3921 (0.3921)\tPrec 89.844% (89.844%)\n",
      " * Prec 85.020% \n",
      "best acc: 85.520000\n",
      "Epoch: [150][0/391]\tTime 0.194 (0.194)\tData 0.154 (0.154)\tLoss 0.2959 (0.2959)\tPrec 89.062% (89.062%)\n",
      "Epoch: [150][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.3613 (0.3165)\tPrec 85.938% (89.387%)\n",
      "Epoch: [150][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.002)\tLoss 0.2455 (0.3184)\tPrec 94.531% (89.323%)\n",
      "Epoch: [150][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.3026 (0.3180)\tPrec 90.625% (89.325%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.164 (0.164)\tLoss 0.3049 (0.3049)\tPrec 92.188% (92.188%)\n",
      " * Prec 84.680% \n",
      "best acc: 85.520000\n",
      "Epoch: [151][0/391]\tTime 0.242 (0.242)\tData 0.204 (0.204)\tLoss 0.2854 (0.2854)\tPrec 88.281% (88.281%)\n",
      "Epoch: [151][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.004)\tLoss 0.1888 (0.3150)\tPrec 94.531% (89.264%)\n",
      "Epoch: [151][200/391]\tTime 0.046 (0.047)\tData 0.001 (0.003)\tLoss 0.3767 (0.3166)\tPrec 89.062% (89.335%)\n",
      "Epoch: [151][300/391]\tTime 0.046 (0.047)\tData 0.002 (0.002)\tLoss 0.2789 (0.3161)\tPrec 92.188% (89.384%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.166 (0.166)\tLoss 0.3271 (0.3271)\tPrec 90.625% (90.625%)\n",
      " * Prec 84.560% \n",
      "best acc: 85.520000\n",
      "Epoch: [152][0/391]\tTime 0.243 (0.243)\tData 0.204 (0.204)\tLoss 0.3432 (0.3432)\tPrec 90.625% (90.625%)\n",
      "Epoch: [152][100/391]\tTime 0.046 (0.048)\tData 0.001 (0.003)\tLoss 0.1973 (0.3043)\tPrec 93.750% (89.890%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m     26\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(fdir)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, epochs):\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m#adjust_learning_rate(optimizer, epoch)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# evaluate on test set\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation starts\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 88\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(trainloader, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28minput\u001b[39m, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mcuda(), target\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# compute output\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# measure accuracy and record loss\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/hw5/software/models/vgg_quant.py:25\u001b[0m, in \u001b[0;36mVGG_quant.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 25\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     27\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(out)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/hw5/software/models/quant_layer.py:104\u001b[0m, in \u001b[0;36mQuantConv2d.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 104\u001b[0m     weight_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m       \n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m#self.register_parameter('weight_q', Parameter(weight_q))  # Mingu added\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParameter(weight_q)  \u001b[38;5;66;03m# Store weight_q during the training\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/hw5/software/models/quant_layer.py:57\u001b[0m, in \u001b[0;36mweight_quantize_fn.forward\u001b[0;34m(self, weight)\u001b[0m\n\u001b[1;32m     55\u001b[0m std \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mstd()\n\u001b[1;32m     56\u001b[0m weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;241m-\u001b[39mmean)\u001b[38;5;241m.\u001b[39mdiv(std)      \u001b[38;5;66;03m# weights normalization\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m weight_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwgt_alpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weight_q\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/hw5/software/models/quant_layer.py:25\u001b[0m, in \u001b[0;36mweight_quantization.<locals>._pq.forward\u001b[0;34m(ctx, input, alpha)\u001b[0m\n\u001b[1;32m     23\u001b[0m sign \u001b[38;5;241m=\u001b[39m input_c\u001b[38;5;241m.\u001b[39msign()\n\u001b[1;32m     24\u001b[0m input_abs \u001b[38;5;241m=\u001b[39m input_c\u001b[38;5;241m.\u001b[39mabs()\n\u001b[0;32m---> 25\u001b[0m input_q \u001b[38;5;241m=\u001b[39m \u001b[43muniform_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_abs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43msign\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;28minput\u001b[39m, input_q)\n\u001b[1;32m     27\u001b[0m input_q \u001b[38;5;241m=\u001b[39m input_q\u001b[38;5;241m.\u001b[39mmul(alpha)               \u001b[38;5;66;03m# rescale to the original range\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "lr = 0.001\n",
    "weight_decay = 1e-4\n",
    "epochs = 500\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,  # Initial learning rate\n",
    "    betas=(0.9, 0.999),  # Default values for beta1 and beta2\n",
    "    eps=1e-08,  # Small value to prevent division by zero\n",
    "    weight_decay=weight_decay  # L2 regularization\n",
    ")\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    #adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "    scheduler.step(prec)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 8552/10000 (86%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/VGG16_quant/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e5b8a05-2589-4e8c-98bd-508d2418a343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -th layer prehooked\n",
      "7 -th layer prehooked\n",
      "12 -th layer prehooked\n",
      "16 -th layer prehooked\n",
      "21 -th layer prehooked\n",
      "25 -th layer prehooked\n",
      "29 -th layer prehooked\n",
      "34 -th layer prehooked\n",
      "38 -th layer prehooked\n",
      "41 -th layer prehooked\n",
      "46 -th layer prehooked\n",
      "50 -th layer prehooked\n",
      "54 -th layer prehooked\n"
     ]
    }
   ],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "i = 0\n",
    "\n",
    "for layer in model.modules():\n",
    "    i = i+1\n",
    "    if isinstance(layer, QuantConv2d):\n",
    "        print(i,\"-th layer prehooked\")\n",
    "        layer.register_forward_pre_hook(save_output)             \n",
    "####################################################\n",
    "\n",
    "basic_block0_conv1 = model.features[0] \n",
    "basic_block0_conv1.register_forward_pre_hook(save_output)\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 7,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]]],\n",
      "\n",
      "\n",
      "        [[[-7,  0,  7],\n",
      "          [-7,  0,  7],\n",
      "          [-7,  0,  7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0, -7, -7],\n",
      "          [ 7,  0, -7],\n",
      "          [ 7,  0, -7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]]],\n",
      "\n",
      "\n",
      "        [[[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  7],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 7,  0,  0],\n",
      "          [-7,  0,  0],\n",
      "          [ 7,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0, -7,  7]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0, -7,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  7],\n",
      "          [ 7,  0, -7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]]],\n",
      "\n",
      "\n",
      "        [[[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0, -7, -7],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 7,  7,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0, -7,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]]],\n",
      "\n",
      "\n",
      "        [[[-7,  7,  0],\n",
      "          [ 0,  7,  0],\n",
      "          [-7,  7, -7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 7,  0,  0],\n",
      "          [ 7,  0,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 7, -7,  0],\n",
      "          [ 0,  0,  7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]]]], device='cuda:0', dtype=torch.int32)\n",
      "tensor([[[[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 7,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]]],\n",
      "\n",
      "\n",
      "        [[[-7,  0,  7],\n",
      "          [-7,  0,  7],\n",
      "          [-7,  0,  7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0, -7, -7],\n",
      "          [ 7,  0, -7],\n",
      "          [ 7,  0, -7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]]],\n",
      "\n",
      "\n",
      "        [[[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  7],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 7,  0,  0],\n",
      "          [-7,  0,  0],\n",
      "          [ 7,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0, -7,  7]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0, -7,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  7],\n",
      "          [ 7,  0, -7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]]],\n",
      "\n",
      "\n",
      "        [[[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0, -7, -7],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 7,  7,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0, -7,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]]],\n",
      "\n",
      "\n",
      "        [[[-7,  7,  0],\n",
      "          [ 0,  7,  0],\n",
      "          [-7,  7, -7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 7,  0,  0],\n",
      "          [ 7,  0,  0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 7, -7,  0],\n",
      "          [ 0,  0,  7]],\n",
      "\n",
      "         [[ 0,  0,  0],\n",
      "          [ 0,  0,  0],\n",
      "          [ 0,  0,  0]]]], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "quant_conv_layer = QuantConv2d(in_channels=3, out_channels=16, kernel_size=3)\n",
    "w_bit = 4\n",
    "weight_q = model.features[3].weight_q  # Quantized value stored during training\n",
    "w_alpha = model.features[3].weight_quant.wgt_alpha   # alpha is defined in your model already. bring it out here\n",
    "w_delta = w_alpha / (2**(w_bit - 1) - 1)  # delta can be calculated by using alpha and w_bit\n",
    "weight_int = (weight_q / w_delta).int() # w_int can be calculated by weight_q and w_delta\n",
    "print(weight_int) # you should see clean integer numberslated by weight_q and w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interior-oxygen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          ...,\n",
      "          [-1, -1, -1,  ..., -1, -2, -1],\n",
      "          [-1, -1, -1,  ..., -2, -1, -2],\n",
      "          [-1, -1, -1,  ..., -2, -1, -2]],\n",
      "\n",
      "         [[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ...,  0, -1,  0],\n",
      "          [ 0,  0,  0,  ..., -1, -1, -1],\n",
      "          [ 0,  0,  0,  ..., -1,  0, -1]],\n",
      "\n",
      "         [[-1, -1, -1,  ..., -1, -1, -1],\n",
      "          [-1, -1, -1,  ..., -1, -1, -1],\n",
      "          [-1, -1, -1,  ..., -1, -1, -1],\n",
      "          ...,\n",
      "          [ 1,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 1,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 1,  0,  0,  ...,  0,  0,  0]]],\n",
      "\n",
      "\n",
      "        [[[ 2,  2,  2,  ...,  2,  2,  2],\n",
      "          [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "          [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "          ...,\n",
      "          [ 0, -1, -2,  ...,  1,  1,  1],\n",
      "          [-1, -1, -1,  ...,  1,  1,  1],\n",
      "          [ 0, -1, -1,  ...,  1,  1,  1]],\n",
      "\n",
      "         [[ 2,  2,  2,  ...,  2,  2,  2],\n",
      "          [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "          [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "          ...,\n",
      "          [ 0, -1, -2,  ...,  1,  1,  1],\n",
      "          [ 0, -1, -1,  ...,  1,  1,  1],\n",
      "          [ 0, -1, -1,  ...,  1,  1,  1]],\n",
      "\n",
      "         [[ 2,  2,  2,  ...,  2,  2,  2],\n",
      "          [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "          [ 2,  2,  2,  ...,  2,  2,  2],\n",
      "          ...,\n",
      "          [ 0, -1, -2,  ...,  1,  1,  1],\n",
      "          [ 0, -1, -1,  ...,  1,  1,  1],\n",
      "          [ 0, -1, -1,  ...,  1,  1,  1]]],\n",
      "\n",
      "\n",
      "        [[[ 0,  0,  0,  ...,  2,  2,  2],\n",
      "          [ 1,  1,  0,  ...,  2,  2,  2],\n",
      "          [ 1,  1,  0,  ...,  2,  2,  2],\n",
      "          ...,\n",
      "          [-1, -1, -2,  ..., -1, -2, -2],\n",
      "          [-2, -2, -2,  ..., -2, -2, -2],\n",
      "          [-2, -1, -1,  ..., -2, -2, -2]],\n",
      "\n",
      "         [[ 1,  1,  1,  ...,  2,  2,  2],\n",
      "          [ 1,  1,  1,  ...,  2,  2,  2],\n",
      "          [ 1,  1,  1,  ...,  2,  2,  2],\n",
      "          ...,\n",
      "          [-1, -1, -1,  ..., -1, -2, -2],\n",
      "          [-1, -1, -1,  ..., -2, -2, -2],\n",
      "          [-1, -1, -1,  ..., -2, -2, -2]],\n",
      "\n",
      "         [[ 2,  2,  1,  ...,  2,  2,  2],\n",
      "          [ 2,  2,  1,  ...,  2,  2,  2],\n",
      "          [ 2,  2,  1,  ...,  2,  2,  2],\n",
      "          ...,\n",
      "          [-1, -1, -1,  ..., -1, -2, -2],\n",
      "          [-1, -1, -1,  ..., -1, -2, -2],\n",
      "          [-1, -1, -1,  ..., -2, -2, -2]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1,  1,  1,  ...,  1,  0, -1],\n",
      "          [ 1,  1,  1,  ...,  1,  0, -1],\n",
      "          [ 1,  1,  1,  ...,  1,  0, -1],\n",
      "          ...,\n",
      "          [ 1,  2,  2,  ...,  0,  0,  0],\n",
      "          [ 1,  1,  2,  ..., -1,  0,  0],\n",
      "          [ 1,  1,  1,  ..., -1,  0,  0]],\n",
      "\n",
      "         [[ 1,  0,  0,  ...,  0, -1, -1],\n",
      "          [ 1,  0,  0,  ...,  0, -1, -1],\n",
      "          [ 0,  0,  0,  ...,  0, -1, -1],\n",
      "          ...,\n",
      "          [ 1,  1,  1,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  1,  ..., -1,  0,  0],\n",
      "          [ 0,  0,  0,  ..., -1,  0,  0]],\n",
      "\n",
      "         [[ 0,  0,  0,  ..., -1, -1, -1],\n",
      "          [ 0,  0,  0,  ..., -1, -1,  0],\n",
      "          [ 0,  0,  0,  ..., -1, -1, -1],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ..., -1,  0,  0],\n",
      "          [ 0,  0,  0,  ..., -1,  0,  0]]],\n",
      "\n",
      "\n",
      "        [[[ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ..., -1,  0,  0]],\n",
      "\n",
      "         [[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0]],\n",
      "\n",
      "         [[ 1,  1,  1,  ...,  1,  1,  1],\n",
      "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "          [ 1,  1,  1,  ...,  1,  1,  1],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "          [ 0,  0,  0,  ...,  0,  0,  0]]],\n",
      "\n",
      "\n",
      "        [[[-1, -1, -1,  ..., -2, -2, -2],\n",
      "          [-1, -1, -1,  ..., -2, -2, -2],\n",
      "          [-1, -1, -1,  ..., -2, -2, -2],\n",
      "          ...,\n",
      "          [ 1,  1,  1,  ..., -2, -2, -2],\n",
      "          [ 0,  0,  1,  ..., -2, -2, -2],\n",
      "          [ 0,  0,  0,  ..., -2, -2, -2]],\n",
      "\n",
      "         [[-1, -1, -1,  ..., -2, -2, -2],\n",
      "          [-1, -1, -1,  ..., -2, -2, -2],\n",
      "          [-1, -1, -1,  ..., -2, -2, -2],\n",
      "          ...,\n",
      "          [ 0,  0,  0,  ..., -2, -2, -2],\n",
      "          [ 0,  0,  0,  ..., -2, -2, -2],\n",
      "          [ 0,  0,  0,  ..., -2, -2, -2]],\n",
      "\n",
      "         [[-1, -1, -1,  ..., -2, -2, -2],\n",
      "          [-1, -1, -1,  ..., -2, -2, -2],\n",
      "          [-1, -1, -1,  ..., -2, -2, -2],\n",
      "          ...,\n",
      "          [ 1,  0,  0,  ..., -2, -2, -2],\n",
      "          [ 0,  0,  0,  ..., -2, -2, -2],\n",
      "          [ 0,  0,  0,  ..., -2, -2, -2]]]], device='cuda:0',\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "save_output.clear()\n",
    "model.features[3].register_forward_pre_hook(save_output)\n",
    "sample_data, _ = next(iter(testloader))  # Get a batch of test data\n",
    "sample_data = sample_data.cuda()\n",
    "model(sample_data) \n",
    "\n",
    "x_bit = 4    \n",
    "x = save_output.outputs[0][0]  # input of the 2nd conv layer\n",
    "x_alpha  = quant_conv_layer.act_alpha.data.item()\n",
    "x_delta = (2 * x_alpha) / (2**x_bit - 1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "x_int = (x_q / x_delta).int()\n",
    "print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ranging-porter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "tensor([[[[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[  0.0000,   3.1631,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   6.3263,   3.1631,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   9.4894,   6.3263,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,  -6.3263,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,  -6.3263,  -6.3263,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ..., -12.6526,  -3.1631,   0.0000]],\n",
      "\n",
      "         [[ -3.1631,  -3.1631,  -3.1631,  ...,  -3.1631,  -3.1631,  -3.1631],\n",
      "          [ -3.1631,  -3.1631,  -3.1631,  ...,  -3.1631,  -3.1631,  -3.1631],\n",
      "          [ -3.1631,  -3.1631,  -3.1631,  ...,  -3.1631,  -3.1631,  -3.1631],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [ -3.1631,  -3.1631,  -3.1631,  ...,  -6.3263,  -3.1631,  -3.1631],\n",
      "          [ -3.1631,  -3.1631,  -3.1631,  ...,  -3.1631,  -3.1631,  -6.3263],\n",
      "          [ -3.1631,  -3.1631,  -3.1631,  ...,  -3.1631,  -3.1631,  -3.1631]],\n",
      "\n",
      "         [[  6.3263,   6.3263,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  6.3263,   6.3263,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  6.3263,   6.3263,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,  -3.1631,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,  -3.1631,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,  -3.1631,   0.0000,   0.0000]],\n",
      "\n",
      "         [[ -6.3263,  -9.4894,  -3.1631,  ...,  -6.3263,  -6.3263,  -6.3263],\n",
      "          [ -6.3263,  -9.4894,  -3.1631,  ...,  -6.3263,  -6.3263,  -6.3263],\n",
      "          [ -6.3263,  -9.4894,   0.0000,  ...,  -6.3263,  -6.3263,  -6.3263],\n",
      "          ...,\n",
      "          [  6.3263,   0.0000,   0.0000,  ...,   6.3263,   9.4894,  -6.3263],\n",
      "          [  6.3263,   0.0000,   0.0000,  ...,   6.3263,   3.1631,   0.0000],\n",
      "          [  6.3263,   0.0000,   0.0000,  ...,   6.3263,  -3.1631,   3.1631]]],\n",
      "\n",
      "\n",
      "        [[[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [-15.8157,  -9.4894,   0.0000,  ...,   3.1631,   9.4894,   6.3263],\n",
      "          [-12.6526,  -6.3263,  -3.1631,  ...,   6.3263,   9.4894,   3.1631],\n",
      "          [ -9.4894,  -3.1631,  -3.1631,  ...,   9.4894,   9.4894,   0.0000]],\n",
      "\n",
      "         [[  6.3263,   6.3263,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  6.3263,   6.3263,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  6.3263,   6.3263,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          ...,\n",
      "          [ -6.3263,  -6.3263,  -6.3263,  ...,   3.1631,   3.1631,   3.1631],\n",
      "          [ -6.3263,  -6.3263,  -6.3263,  ...,   3.1631,   3.1631,   3.1631],\n",
      "          [ -3.1631,  -6.3263,  -6.3263,  ...,   3.1631,   3.1631,   3.1631]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  6.3263,   6.3263,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  6.3263,   6.3263,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  6.3263,   6.3263,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          ...,\n",
      "          [ -6.3263,  -3.1631,  -6.3263,  ...,   0.0000,   0.0000,   3.1631],\n",
      "          [ -3.1631,  -6.3263,  -3.1631,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  3.1631,  -3.1631,  -6.3263,  ...,   0.0000,   0.0000,   3.1631]],\n",
      "\n",
      "         [[-12.6526, -12.6526, -12.6526,  ..., -12.6526, -12.6526, -12.6526],\n",
      "          [-12.6526, -12.6526, -12.6526,  ..., -12.6526, -12.6526, -12.6526],\n",
      "          [-12.6526, -12.6526, -12.6526,  ..., -12.6526, -12.6526, -12.6526],\n",
      "          ...,\n",
      "          [  9.4894,  12.6526,  12.6526,  ...,  -3.1631,  -6.3263,  -6.3263],\n",
      "          [  9.4894,  12.6526,  12.6526,  ...,  -6.3263,  -6.3263,  -6.3263],\n",
      "          [  6.3263,   9.4894,  12.6526,  ...,  -6.3263,  -6.3263,  -6.3263]],\n",
      "\n",
      "         [[ 12.6526,  12.6526,  12.6526,  ...,  12.6526,  12.6526,  12.6526],\n",
      "          [ 12.6526,  12.6526,  12.6526,  ...,  12.6526,  12.6526,  12.6526],\n",
      "          [ 12.6526,  12.6526,  12.6526,  ...,  12.6526,  12.6526,  12.6526],\n",
      "          ...,\n",
      "          [  0.0000, -12.6526, -12.6526,  ...,  -3.1631,   3.1631,   9.4894],\n",
      "          [ -3.1631, -12.6526,  -9.4894,  ...,  -3.1631,   9.4894,   9.4894],\n",
      "          [ -6.3263,  -9.4894,  -6.3263,  ...,  -3.1631,  12.6526,   6.3263]]],\n",
      "\n",
      "\n",
      "        [[[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[ -6.3263,  -6.3263,   6.3263,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [ -9.4894,  -9.4894,   9.4894,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [ -9.4894,  -9.4894,   9.4894,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [ -9.4894,  -9.4894,   3.1631,  ...,   6.3263,  -3.1631,  -9.4894],\n",
      "          [ -6.3263,  -6.3263,   3.1631,  ...,   3.1631,  -9.4894,  -6.3263],\n",
      "          [  0.0000,  -3.1631,   3.1631,  ...,   0.0000,  -6.3263,  -3.1631]],\n",
      "\n",
      "         [[  3.1631,   3.1631,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  3.1631,   3.1631,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  3.1631,   3.1631,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          ...,\n",
      "          [ -3.1631,  -3.1631,  -3.1631,  ...,  -3.1631,  -3.1631,  -3.1631],\n",
      "          [ -3.1631,  -3.1631,  -3.1631,  ...,  -3.1631,  -6.3263,  -6.3263],\n",
      "          [ -3.1631,  -3.1631,  -3.1631,  ...,  -3.1631,  -6.3263,  -6.3263]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -3.1631,   0.0000,   3.1631,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  0.0000,   3.1631,   3.1631,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  0.0000,   3.1631,   3.1631,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          ...,\n",
      "          [ -3.1631,  -3.1631,  -6.3263,  ...,  -9.4894,  -9.4894,  -3.1631],\n",
      "          [ -6.3263,  -6.3263,  -6.3263,  ...,  -3.1631,  -6.3263,  -6.3263],\n",
      "          [ -3.1631,   0.0000,  -3.1631,  ...,  -6.3263,  -6.3263,  -3.1631]],\n",
      "\n",
      "         [[ -9.4894,  -6.3263,  -9.4894,  ..., -12.6526, -12.6526, -12.6526],\n",
      "          [ -9.4894,  -6.3263,  -9.4894,  ..., -12.6526, -12.6526, -12.6526],\n",
      "          [ -9.4894,  -6.3263,  -9.4894,  ..., -12.6526, -12.6526, -12.6526],\n",
      "          ...,\n",
      "          [  6.3263,   6.3263,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  6.3263,   6.3263,   6.3263,  ...,   6.3263,   9.4894,  12.6526],\n",
      "          [  6.3263,   6.3263,   6.3263,  ...,   6.3263,   9.4894,  12.6526]],\n",
      "\n",
      "         [[ 15.8157,   9.4894,   3.1631,  ...,  12.6526,  12.6526,  12.6526],\n",
      "          [ 15.8157,   6.3263,   3.1631,  ...,  12.6526,  12.6526,  12.6526],\n",
      "          [ 15.8157,   6.3263,   3.1631,  ...,  12.6526,  12.6526,  12.6526],\n",
      "          ...,\n",
      "          [ -3.1631, -12.6526,  -6.3263,  ...,  -3.1631,   0.0000,  -9.4894],\n",
      "          [ -3.1631,  -9.4894,  -9.4894,  ...,   0.0000,  -6.3263,  -9.4894],\n",
      "          [ -6.3263, -12.6526,  -9.4894,  ...,  -3.1631,  -6.3263, -12.6526]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[  0.0000,   0.0000,  -6.3263,  ...,  -9.4894, -15.8157, -18.9789],\n",
      "          [  0.0000,  -3.1631,  -6.3263,  ..., -12.6526, -12.6526, -15.8157],\n",
      "          [ -3.1631,  -6.3263,  -3.1631,  ..., -15.8157, -12.6526, -12.6526],\n",
      "          ...,\n",
      "          [  3.1631,  -6.3263, -12.6526,  ...,  -3.1631,  -3.1631,   0.0000],\n",
      "          [  6.3263,  -6.3263, -15.8157,  ...,   3.1631,   0.0000,   3.1631],\n",
      "          [  6.3263,  -3.1631, -15.8157,  ...,   6.3263,   6.3263,   6.3263]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,  -3.1631,  -3.1631,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,  -3.1631,  -3.1631,  -3.1631],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,  -3.1631,  -3.1631,  -3.1631],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,  -3.1631,   0.0000,   0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  3.1631,   3.1631,   3.1631,  ...,   6.3263,   3.1631,   3.1631],\n",
      "          [  3.1631,   3.1631,   3.1631,  ...,   6.3263,   6.3263,   0.0000],\n",
      "          [  6.3263,   3.1631,   3.1631,  ...,   6.3263,   3.1631,   6.3263],\n",
      "          ...,\n",
      "          [  3.1631,   6.3263,   6.3263,  ...,  -3.1631,  -3.1631,   0.0000],\n",
      "          [  6.3263,   3.1631,   6.3263,  ...,   3.1631,   3.1631,  -3.1631],\n",
      "          [  3.1631,   6.3263,   3.1631,  ...,   0.0000,   3.1631,   0.0000]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   6.3263,   6.3263,   3.1631],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   9.4894,   3.1631,   0.0000]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   3.1631,  ...,   0.0000,  -6.3263,  -9.4894],\n",
      "          [  0.0000,   3.1631,  -3.1631,  ...,   0.0000,  -9.4894,  -6.3263],\n",
      "          [ -3.1631,   3.1631,  -6.3263,  ...,  -3.1631,  -6.3263,  -9.4894],\n",
      "          ...,\n",
      "          [  3.1631,   3.1631,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   6.3263,  -3.1631,  ...,  -3.1631,  -9.4894,   0.0000],\n",
      "          [  3.1631,   3.1631,   0.0000,  ...,  -6.3263, -12.6526,  -3.1631]]],\n",
      "\n",
      "\n",
      "        [[[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   3.1631,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   6.3263,   3.1631,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   6.3263,   6.3263,   3.1631]],\n",
      "\n",
      "         [[  3.1631,   3.1631,   3.1631,  ...,   3.1631,   3.1631,   3.1631],\n",
      "          [  3.1631,   3.1631,   3.1631,  ...,   3.1631,   3.1631,   3.1631],\n",
      "          [  3.1631,   3.1631,   3.1631,  ...,   3.1631,   3.1631,   3.1631],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,  -3.1631,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,  -3.1631,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,  -3.1631,   0.0000,  -3.1631]],\n",
      "\n",
      "         [[ -6.3263,  -6.3263,  -6.3263,  ...,  -6.3263,  -6.3263,  -6.3263],\n",
      "          [ -6.3263,  -6.3263,  -6.3263,  ...,  -6.3263,  -6.3263,  -6.3263],\n",
      "          [ -6.3263,  -6.3263,  -6.3263,  ...,  -6.3263,  -6.3263,  -6.3263],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[  6.3263,   6.3263,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  6.3263,   6.3263,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          [  6.3263,   6.3263,   6.3263,  ...,   6.3263,   6.3263,   6.3263],\n",
      "          ...,\n",
      "          [  3.1631,   0.0000,   0.0000,  ...,   3.1631,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   3.1631,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   3.1631,   0.0000,   3.1631]]],\n",
      "\n",
      "\n",
      "        [[[  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "         [[  0.0000,   0.0000,   0.0000,  ...,  -9.4894,  -3.1631,   0.0000],\n",
      "          [  0.0000,   0.0000,   3.1631,  ..., -12.6526,  -6.3263,   0.0000],\n",
      "          [ -3.1631,   0.0000,   6.3263,  ..., -15.8157,  -9.4894,   0.0000],\n",
      "          ...,\n",
      "          [  0.0000,   0.0000,   0.0000,  ..., -12.6526,  -6.3263,   0.0000],\n",
      "          [  3.1631,   3.1631,   0.0000,  ..., -12.6526,  -3.1631,   0.0000],\n",
      "          [  3.1631,   3.1631,   0.0000,  ...,  -9.4894,   0.0000,   0.0000]],\n",
      "\n",
      "         [[ -3.1631,  -3.1631,  -3.1631,  ...,  -6.3263,  -6.3263,  -6.3263],\n",
      "          [ -3.1631,  -3.1631,  -3.1631,  ...,  -6.3263,  -6.3263,  -6.3263],\n",
      "          [ -3.1631,  -3.1631,  -3.1631,  ...,  -6.3263,  -6.3263,  -6.3263],\n",
      "          ...,\n",
      "          [  3.1631,   3.1631,   3.1631,  ...,  -6.3263,  -6.3263,  -6.3263],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,  -6.3263,  -6.3263,  -6.3263],\n",
      "          [  0.0000,   3.1631,   0.0000,  ...,  -6.3263,  -6.3263,  -6.3263]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ -3.1631,  -3.1631,  -3.1631,  ...,  -6.3263,  -3.1631,  -6.3263],\n",
      "          [ -3.1631,  -3.1631,  -3.1631,  ...,  -3.1631,  -6.3263,  -6.3263],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,  -6.3263,  -3.1631,  -6.3263],\n",
      "          ...,\n",
      "          [  3.1631,   3.1631,   3.1631,  ...,  -9.4894,  -6.3263,  -6.3263],\n",
      "          [  0.0000,   3.1631,   0.0000,  ...,  -3.1631,  -3.1631,  -6.3263],\n",
      "          [  3.1631,   3.1631,   0.0000,  ...,  -3.1631,  -6.3263,  -6.3263]],\n",
      "\n",
      "         [[  6.3263,   6.3263,   6.3263,  ...,  12.6526,  12.6526,  12.6526],\n",
      "          [  6.3263,   6.3263,   6.3263,  ...,   9.4894,  12.6526,  12.6526],\n",
      "          [  6.3263,   6.3263,   6.3263,  ...,   9.4894,  12.6526,  12.6526],\n",
      "          ...,\n",
      "          [ -6.3263,  -6.3263,  -6.3263,  ...,   9.4894,  12.6526,  12.6526],\n",
      "          [  0.0000,   0.0000,   0.0000,  ...,   9.4894,  12.6526,  12.6526],\n",
      "          [  0.0000,  -3.1631,  -3.1631,  ...,   9.4894,  12.6526,  12.6526]],\n",
      "\n",
      "         [[ -6.3263,  -6.3263,  -6.3263,  ...,  -9.4894, -12.6526, -12.6526],\n",
      "          [ -6.3263,  -6.3263,  -9.4894,  ...,  -6.3263,  -9.4894, -12.6526],\n",
      "          [ -6.3263, -12.6526,  -6.3263,  ...,   0.0000, -12.6526, -12.6526],\n",
      "          ...,\n",
      "          [  6.3263,   3.1631,   3.1631,  ...,  -3.1631,  -9.4894, -12.6526],\n",
      "          [  3.1631,   3.1631,   0.0000,  ..., -12.6526,  -9.4894, -12.6526],\n",
      "          [  0.0000,   3.1631,   3.1631,  ..., -12.6526,  -9.4894, -12.6526]]]],\n",
      "       device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, bias=False).cuda()\n",
    "weight_int_adjusted = weight_int[:, :3, :, :]  # Slice to use only the first 3 input channels\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int_adjusted.float())\n",
    "print(x_int.size())\n",
    "print(weight_int.size())\n",
    "output_int =  conv_int(x_int.float())    # output_int can be calculated with conv_int and x_int\n",
    "output_recovered = output_int * x_delta * w_delta  # recover with x_delta and w_delta\n",
    "print(output_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31ab5522-4091-4f09-b44b-2b38cb70f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_output.clear()\n",
    "model.features[7].register_forward_pre_hook(save_output)\n",
    "sample_data, _ = next(iter(testloader))  # Get a batch of test data\n",
    "sample_data = sample_data.cuda()\n",
    "model(sample_data) \n",
    "\n",
    "hooked_output = save_output.outputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "157dffd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hooked_output size: torch.Size([128, 3, 30, 30])\n",
      "output_recovered size: torch.Size([128, 64, 30, 30])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_recovered size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_recovered\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Compute difference\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m difference \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(\u001b[43mhooked_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_recovered\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean Difference:\u001b[39m\u001b[38;5;124m\"\u001b[39m, difference\u001b[38;5;241m.\u001b[39mmean())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "hooked_output = hooked_output[:, :, :30, :30]\n",
    "output_recovered_reduced = output_recovered.mean(dim=1, keepdim=True)\n",
    "print(\"hooked_output size:\", hooked_output.size())\n",
    "print(\"output_recovered size:\", output_recovered.size())\n",
    "# Compute difference\n",
    "difference = torch.abs(hooked_output - output_recovered)\n",
    "print(\"Mean Difference:\", difference.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight quantized version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, bias = False)\n",
    "conv_ref.weight = model.features[3].weight_q \n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "print(output_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight floating number version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 64, out_channels=64, kernel_size = 3, bias = False)\n",
    "weight = model.layer1[0].conv1.weight\n",
    "mean = weight.data.mean()\n",
    "std = weight.data.std()\n",
    "conv_ref.weight = torch.nn.parameter.Parameter(weight.add(-mean).div(std))\n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "print(output_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = abs( output_ref - output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
