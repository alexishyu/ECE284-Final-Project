{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "radical-fifty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Building model...\n",
      "ResNet_Cifar(\n",
      "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): Sequential()\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): Sequential()\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): QuantConv2d(\n",
      "          32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "          (weight_quant): weight_quantize_fn()\n",
      "        )\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (conv2): QuantConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (weight_quant): weight_quantize_fn()\n",
      "      )\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "     \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from models.quant_layer import *\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \" 3x3 convolution with padding \"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "def Quantconv3x3(in_planes, out_planes, stride=1):\n",
    "    \" 3x3 quantized convolution with padding \"\n",
    "    return QuantConv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion=1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, float=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if float:\n",
    "            self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "            self.conv2 = conv3x3(planes, planes)\n",
    "        else:\n",
    "            self.conv1 = Quantconv3x3(inplanes, planes, stride)\n",
    "            self.conv2 = Quantconv3x3(planes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion=4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes*4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes*4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet_Cifar(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=10, float=False):\n",
    "        super(ResNet_Cifar, self).__init__()\n",
    "        self.inplanes = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0], float=float)\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2, float=float)\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2, float=float)\n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        self.fc = nn.Linear(64 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, float=False):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                QuantConv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False)\n",
    "                if float is False else nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1,\n",
    "                                                 stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, float=float))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, float=float))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def show_params(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, QuantConv2d):\n",
    "                m.show_params()\n",
    "\n",
    "                \n",
    "def resnet20_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [3, 3, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    " \n",
    "def resnet32_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [5, 5, 5], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet44_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [7, 7, 7], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet56_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [9, 9, 9], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet110_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [18, 18, 18], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet1202_quant(**kwargs):\n",
    "    model = ResNet_Cifar(BasicBlock, [200, 200, 200], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet164_quant(**kwargs):\n",
    "    model = ResNet_Cifar(Bottleneck, [18, 18, 18], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet1001_quant(**kwargs):\n",
    "    model = ResNet_Cifar(Bottleneck, [111, 111, 111], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "    # net = resnet20_cifar(float=True)\n",
    "    # y = net(torch.randn(1, 3, 64, 64))\n",
    "    # print(net)\n",
    "    # print(y.size())\n",
    "\n",
    "\n",
    "global best_prec\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print('=> Building model...')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 128\n",
    "model_name = \"resnet20_quant4bit\"\n",
    "model = resnet20_quant()\n",
    "# Modify initial conv and batch norm layers\n",
    "model.conv1 = nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.bn1 = nn.BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "# Modify layer1[0] conv layers\n",
    "model.layer1[0].conv1 = QuantConv2d(8, 8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "model.layer1[0].conv2 = QuantConv2d(8, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "# Replace batch norm layers for quantized convolutions\n",
    "model.layer1[0].bn1 = nn.Sequential()  # Replace if not used\n",
    "model.layer1[0].bn2 = nn.Sequential()  # Replace if not used\n",
    "\n",
    "# Add downsample for residual connection\n",
    "model.layer1[0].downsample = nn.Sequential(\n",
    "    nn.Conv2d(8, 16, kernel_size=1, stride=1, bias=False),\n",
    "    nn.BatchNorm2d(16)\n",
    ")\n",
    "\n",
    "print(model)\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.491, 0.482, 0.447], std=[0.247, 0.243, 0.262])\n",
    "\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]))\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "print_freq = 100 # every 100 batches, accuracy printed. Here, each batch includes \"batch_size\" data points\n",
    "# CIFAR10 has 50,000 training data, and 10,000 validation data.\n",
    "\n",
    "def train(trainloader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(trainloader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input, target = input.cuda(), target.cuda()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec = accuracy(output, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   epoch, i, len(trainloader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "            \n",
    "\n",
    "def validate(val_loader, model, criterion ):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "         \n",
    "            input, target = input.cuda(), target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec = accuracy(output, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % print_freq == 0:  # This line shows how frequently print out the status. e.g., i%5 => every 5 batch, prints out\n",
    "                print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                   top1=top1))\n",
    "\n",
    "    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "def save_checkpoint(state, is_best, fdir):\n",
    "    filepath = os.path.join(fdir, 'checkpoint.pth')\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"For resnet, the lr starts from 0.1, and is divided by 10 at 80 and 120 epochs\"\"\"\n",
    "    adjust_list = [ 70, 90]\n",
    "    if epoch in adjust_list:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = param_group['lr'] * 0.1        \n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "#all_params = checkpoint['state_dict']\n",
    "#model.load_state_dict(all_params, strict=False)\n",
    "#criterion = nn.CrossEntropyLoss().cuda()\n",
    "#validate(testloader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "junior-reminder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/391]\tTime 0.670 (0.670)\tData 0.301 (0.301)\tLoss 2.4107 (2.4107)\tPrec 7.031% (7.031%)\n",
      "Epoch: [0][100/391]\tTime 0.066 (0.068)\tData 0.002 (0.005)\tLoss 2.0070 (2.1908)\tPrec 25.781% (17.079%)\n",
      "Epoch: [0][200/391]\tTime 0.063 (0.064)\tData 0.002 (0.004)\tLoss 2.0637 (2.1062)\tPrec 24.219% (20.056%)\n",
      "Epoch: [0][300/391]\tTime 0.049 (0.059)\tData 0.002 (0.003)\tLoss 1.9592 (2.0467)\tPrec 26.562% (22.135%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.310 (0.310)\tLoss 1.8609 (1.8609)\tPrec 29.688% (29.688%)\n",
      " * Prec 29.190% \n",
      "best acc: 29.190000\n",
      "Epoch: [1][0/391]\tTime 0.365 (0.365)\tData 0.303 (0.303)\tLoss 1.7477 (1.7477)\tPrec 33.594% (33.594%)\n",
      "Epoch: [1][100/391]\tTime 0.066 (0.065)\tData 0.003 (0.005)\tLoss 1.7460 (1.9068)\tPrec 33.594% (27.777%)\n",
      "Epoch: [1][200/391]\tTime 0.071 (0.065)\tData 0.003 (0.004)\tLoss 1.8973 (1.9005)\tPrec 21.875% (28.176%)\n",
      "Epoch: [1][300/391]\tTime 0.065 (0.065)\tData 0.002 (0.003)\tLoss 1.7249 (1.8736)\tPrec 41.406% (29.153%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.285 (0.285)\tLoss 1.6002 (1.6002)\tPrec 46.094% (46.094%)\n",
      " * Prec 34.970% \n",
      "best acc: 34.970000\n",
      "Epoch: [2][0/391]\tTime 0.322 (0.322)\tData 0.261 (0.261)\tLoss 1.7466 (1.7466)\tPrec 34.375% (34.375%)\n",
      "Epoch: [2][100/391]\tTime 0.049 (0.059)\tData 0.002 (0.005)\tLoss 1.7779 (1.7763)\tPrec 29.688% (33.385%)\n",
      "Epoch: [2][200/391]\tTime 0.035 (0.053)\tData 0.001 (0.003)\tLoss 1.7441 (1.8181)\tPrec 35.156% (31.911%)\n",
      "Epoch: [2][300/391]\tTime 0.067 (0.053)\tData 0.002 (0.003)\tLoss 1.8301 (1.8155)\tPrec 27.344% (32.057%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.250 (0.250)\tLoss 1.7062 (1.7062)\tPrec 35.156% (35.156%)\n",
      " * Prec 37.980% \n",
      "best acc: 37.980000\n",
      "Epoch: [3][0/391]\tTime 0.301 (0.301)\tData 0.236 (0.236)\tLoss 1.6983 (1.6983)\tPrec 31.250% (31.250%)\n",
      "Epoch: [3][100/391]\tTime 0.062 (0.053)\tData 0.002 (0.004)\tLoss 1.6805 (1.7506)\tPrec 32.812% (34.839%)\n",
      "Epoch: [3][200/391]\tTime 0.062 (0.059)\tData 0.003 (0.003)\tLoss 1.8451 (1.7761)\tPrec 25.781% (33.776%)\n",
      "Epoch: [3][300/391]\tTime 0.074 (0.061)\tData 0.003 (0.003)\tLoss 1.7944 (1.7978)\tPrec 33.594% (33.082%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.281 (0.281)\tLoss 2.6144 (2.6144)\tPrec 26.562% (26.562%)\n",
      " * Prec 21.490% \n",
      "best acc: 37.980000\n",
      "Epoch: [4][0/391]\tTime 0.408 (0.408)\tData 0.334 (0.334)\tLoss 1.9594 (1.9594)\tPrec 29.688% (29.688%)\n",
      "Epoch: [4][100/391]\tTime 0.065 (0.066)\tData 0.002 (0.006)\tLoss 1.8051 (1.8570)\tPrec 32.031% (30.886%)\n",
      "Epoch: [4][200/391]\tTime 0.076 (0.065)\tData 0.003 (0.004)\tLoss 1.8905 (1.8623)\tPrec 30.469% (30.570%)\n",
      "Epoch: [4][300/391]\tTime 0.059 (0.064)\tData 0.002 (0.004)\tLoss 1.7415 (1.8575)\tPrec 39.062% (30.653%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.293 (0.293)\tLoss 4.1273 (4.1273)\tPrec 8.594% (8.594%)\n",
      " * Prec 11.050% \n",
      "best acc: 37.980000\n",
      "Epoch: [5][0/391]\tTime 0.370 (0.370)\tData 0.312 (0.312)\tLoss 1.9155 (1.9155)\tPrec 27.344% (27.344%)\n",
      "Epoch: [5][100/391]\tTime 0.065 (0.071)\tData 0.003 (0.006)\tLoss 1.7176 (1.8660)\tPrec 30.469% (30.198%)\n",
      "Epoch: [5][200/391]\tTime 0.065 (0.067)\tData 0.003 (0.004)\tLoss 1.7128 (1.8202)\tPrec 34.375% (31.922%)\n",
      "Epoch: [5][300/391]\tTime 0.057 (0.066)\tData 0.003 (0.004)\tLoss 1.7920 (1.7954)\tPrec 35.156% (32.755%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.293 (0.293)\tLoss 1.7521 (1.7521)\tPrec 39.844% (39.844%)\n",
      " * Prec 32.440% \n",
      "best acc: 37.980000\n",
      "Epoch: [6][0/391]\tTime 0.400 (0.400)\tData 0.315 (0.315)\tLoss 1.6228 (1.6228)\tPrec 39.062% (39.062%)\n",
      "Epoch: [6][100/391]\tTime 0.075 (0.068)\tData 0.002 (0.006)\tLoss 1.6402 (1.6358)\tPrec 38.281% (39.418%)\n",
      "Epoch: [6][200/391]\tTime 0.079 (0.071)\tData 0.002 (0.004)\tLoss 1.7841 (1.6224)\tPrec 34.375% (40.209%)\n",
      "Epoch: [6][300/391]\tTime 0.080 (0.072)\tData 0.008 (0.004)\tLoss 1.6390 (1.6021)\tPrec 38.281% (41.027%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.367 (0.367)\tLoss 1.6508 (1.6508)\tPrec 41.406% (41.406%)\n",
      " * Prec 40.900% \n",
      "best acc: 40.900000\n",
      "Epoch: [7][0/391]\tTime 0.448 (0.448)\tData 0.372 (0.372)\tLoss 1.4818 (1.4818)\tPrec 39.844% (39.844%)\n",
      "Epoch: [7][100/391]\tTime 0.066 (0.068)\tData 0.002 (0.006)\tLoss 1.5693 (1.4921)\tPrec 41.406% (44.609%)\n",
      "Epoch: [7][200/391]\tTime 0.054 (0.064)\tData 0.002 (0.004)\tLoss 1.3088 (1.4695)\tPrec 57.812% (45.911%)\n",
      "Epoch: [7][300/391]\tTime 0.052 (0.064)\tData 0.002 (0.004)\tLoss 1.4461 (1.4563)\tPrec 48.438% (46.649%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.337 (0.337)\tLoss 2.0537 (2.0537)\tPrec 33.594% (33.594%)\n",
      " * Prec 37.220% \n",
      "best acc: 40.900000\n",
      "Epoch: [8][0/391]\tTime 0.420 (0.420)\tData 0.346 (0.346)\tLoss 1.3912 (1.3912)\tPrec 47.656% (47.656%)\n",
      "Epoch: [8][100/391]\tTime 0.064 (0.053)\tData 0.002 (0.005)\tLoss 1.4376 (1.3624)\tPrec 45.312% (50.456%)\n",
      "Epoch: [8][200/391]\tTime 0.066 (0.055)\tData 0.002 (0.004)\tLoss 1.4782 (1.3549)\tPrec 44.531% (51.166%)\n",
      "Epoch: [8][300/391]\tTime 0.065 (0.057)\tData 0.002 (0.003)\tLoss 1.3679 (1.3404)\tPrec 56.250% (51.625%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.288 (0.288)\tLoss 1.7958 (1.7958)\tPrec 40.625% (40.625%)\n",
      " * Prec 38.280% \n",
      "best acc: 40.900000\n",
      "Epoch: [9][0/391]\tTime 0.393 (0.393)\tData 0.321 (0.321)\tLoss 1.4168 (1.4168)\tPrec 50.781% (50.781%)\n",
      "Epoch: [9][100/391]\tTime 0.044 (0.059)\tData 0.002 (0.005)\tLoss 1.3956 (1.2871)\tPrec 49.219% (53.666%)\n",
      "Epoch: [9][200/391]\tTime 0.062 (0.059)\tData 0.002 (0.004)\tLoss 1.3112 (1.2828)\tPrec 52.344% (53.712%)\n",
      "Epoch: [9][300/391]\tTime 0.064 (0.061)\tData 0.002 (0.003)\tLoss 1.3779 (1.2702)\tPrec 52.344% (54.311%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.352 (0.352)\tLoss 3.5735 (3.5735)\tPrec 18.750% (18.750%)\n",
      " * Prec 23.170% \n",
      "best acc: 40.900000\n",
      "Epoch: [10][0/391]\tTime 0.368 (0.368)\tData 0.290 (0.290)\tLoss 1.0454 (1.0454)\tPrec 60.938% (60.938%)\n",
      "Epoch: [10][100/391]\tTime 0.064 (0.064)\tData 0.002 (0.005)\tLoss 1.1475 (1.2106)\tPrec 59.375% (56.242%)\n",
      "Epoch: [10][200/391]\tTime 0.057 (0.064)\tData 0.002 (0.004)\tLoss 1.2291 (1.2050)\tPrec 53.906% (56.654%)\n",
      "Epoch: [10][300/391]\tTime 0.077 (0.064)\tData 0.002 (0.003)\tLoss 1.2289 (1.2006)\tPrec 55.469% (56.787%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.297 (0.297)\tLoss 1.8656 (1.8656)\tPrec 42.188% (42.188%)\n",
      " * Prec 41.690% \n",
      "best acc: 41.690000\n",
      "Epoch: [11][0/391]\tTime 0.347 (0.347)\tData 0.272 (0.272)\tLoss 1.3581 (1.3581)\tPrec 50.781% (50.781%)\n",
      "Epoch: [11][100/391]\tTime 0.067 (0.061)\tData 0.002 (0.005)\tLoss 1.2190 (1.1638)\tPrec 60.156% (58.130%)\n",
      "Epoch: [11][200/391]\tTime 0.067 (0.063)\tData 0.002 (0.004)\tLoss 1.1127 (1.1546)\tPrec 60.938% (58.839%)\n",
      "Epoch: [11][300/391]\tTime 0.060 (0.062)\tData 0.002 (0.003)\tLoss 0.9878 (1.1477)\tPrec 59.375% (59.061%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.345 (0.345)\tLoss 1.1282 (1.1282)\tPrec 58.594% (58.594%)\n",
      " * Prec 54.950% \n",
      "best acc: 54.950000\n",
      "Epoch: [12][0/391]\tTime 0.345 (0.345)\tData 0.280 (0.280)\tLoss 1.0677 (1.0677)\tPrec 60.938% (60.938%)\n",
      "Epoch: [12][100/391]\tTime 0.062 (0.053)\tData 0.003 (0.005)\tLoss 1.1543 (1.1432)\tPrec 57.812% (59.143%)\n",
      "Epoch: [12][200/391]\tTime 0.050 (0.057)\tData 0.003 (0.004)\tLoss 1.1657 (1.1264)\tPrec 60.156% (59.939%)\n",
      "Epoch: [12][300/391]\tTime 0.065 (0.057)\tData 0.003 (0.003)\tLoss 1.2414 (1.1225)\tPrec 57.031% (60.039%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.282 (0.282)\tLoss 1.2542 (1.2542)\tPrec 55.469% (55.469%)\n",
      " * Prec 52.580% \n",
      "best acc: 54.950000\n",
      "Epoch: [13][0/391]\tTime 0.395 (0.395)\tData 0.328 (0.328)\tLoss 0.9692 (0.9692)\tPrec 66.406% (66.406%)\n",
      "Epoch: [13][100/391]\tTime 0.067 (0.059)\tData 0.002 (0.005)\tLoss 1.1945 (1.0924)\tPrec 53.125% (60.899%)\n",
      "Epoch: [13][200/391]\tTime 0.051 (0.064)\tData 0.002 (0.004)\tLoss 1.1280 (1.0914)\tPrec 59.375% (61.000%)\n",
      "Epoch: [13][300/391]\tTime 0.064 (0.064)\tData 0.002 (0.003)\tLoss 1.2078 (1.0891)\tPrec 54.688% (61.052%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.336 (0.336)\tLoss 1.7692 (1.7692)\tPrec 42.969% (42.969%)\n",
      " * Prec 40.360% \n",
      "best acc: 54.950000\n",
      "Epoch: [14][0/391]\tTime 0.395 (0.395)\tData 0.324 (0.324)\tLoss 1.4247 (1.4247)\tPrec 46.875% (46.875%)\n",
      "Epoch: [14][100/391]\tTime 0.061 (0.069)\tData 0.002 (0.006)\tLoss 1.0483 (1.2849)\tPrec 60.156% (53.976%)\n",
      "Epoch: [14][200/391]\tTime 0.062 (0.064)\tData 0.002 (0.004)\tLoss 1.1612 (1.2657)\tPrec 58.594% (54.629%)\n",
      "Epoch: [14][300/391]\tTime 0.065 (0.065)\tData 0.002 (0.003)\tLoss 1.1098 (1.2511)\tPrec 59.375% (55.105%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.274 (0.274)\tLoss 1.3921 (1.3921)\tPrec 49.219% (49.219%)\n",
      " * Prec 49.320% \n",
      "best acc: 54.950000\n",
      "Epoch: [15][0/391]\tTime 0.402 (0.402)\tData 0.328 (0.328)\tLoss 1.1969 (1.1969)\tPrec 57.812% (57.812%)\n",
      "Epoch: [15][100/391]\tTime 0.065 (0.064)\tData 0.002 (0.005)\tLoss 1.1412 (1.1833)\tPrec 60.938% (58.052%)\n",
      "Epoch: [15][200/391]\tTime 0.042 (0.060)\tData 0.002 (0.004)\tLoss 1.1995 (1.1832)\tPrec 51.562% (58.003%)\n",
      "Epoch: [15][300/391]\tTime 0.067 (0.059)\tData 0.002 (0.003)\tLoss 1.2237 (1.1751)\tPrec 54.688% (58.199%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.256 (0.256)\tLoss 1.1887 (1.1887)\tPrec 59.375% (59.375%)\n",
      " * Prec 55.510% \n",
      "best acc: 55.510000\n",
      "Epoch: [16][0/391]\tTime 0.460 (0.460)\tData 0.388 (0.388)\tLoss 1.2382 (1.2382)\tPrec 52.344% (52.344%)\n",
      "Epoch: [16][100/391]\tTime 0.067 (0.063)\tData 0.003 (0.006)\tLoss 1.1443 (1.1259)\tPrec 59.375% (59.669%)\n",
      "Epoch: [16][200/391]\tTime 0.060 (0.062)\tData 0.002 (0.004)\tLoss 1.1373 (1.1356)\tPrec 54.688% (59.391%)\n",
      "Epoch: [16][300/391]\tTime 0.043 (0.062)\tData 0.002 (0.004)\tLoss 1.1335 (1.1300)\tPrec 60.938% (59.583%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.329 (0.329)\tLoss 2.2593 (2.2593)\tPrec 41.406% (41.406%)\n",
      " * Prec 43.420% \n",
      "best acc: 55.510000\n",
      "Epoch: [17][0/391]\tTime 0.357 (0.357)\tData 0.285 (0.285)\tLoss 1.1321 (1.1321)\tPrec 60.938% (60.938%)\n",
      "Epoch: [17][100/391]\tTime 0.065 (0.069)\tData 0.002 (0.006)\tLoss 1.1876 (1.1093)\tPrec 58.594% (60.628%)\n",
      "Epoch: [17][200/391]\tTime 0.066 (0.067)\tData 0.002 (0.004)\tLoss 1.1200 (1.1156)\tPrec 61.719% (60.191%)\n",
      "Epoch: [17][300/391]\tTime 0.066 (0.065)\tData 0.002 (0.003)\tLoss 1.0431 (1.1052)\tPrec 64.062% (60.704%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.352 (0.352)\tLoss 1.2047 (1.2047)\tPrec 51.562% (51.562%)\n",
      " * Prec 55.250% \n",
      "best acc: 55.510000\n",
      "Epoch: [18][0/391]\tTime 0.365 (0.365)\tData 0.298 (0.298)\tLoss 1.1190 (1.1190)\tPrec 60.156% (60.156%)\n",
      "Epoch: [18][100/391]\tTime 0.054 (0.064)\tData 0.002 (0.005)\tLoss 0.9967 (1.0862)\tPrec 65.625% (61.139%)\n",
      "Epoch: [18][200/391]\tTime 0.058 (0.062)\tData 0.002 (0.004)\tLoss 1.0803 (1.0757)\tPrec 64.062% (61.563%)\n",
      "Epoch: [18][300/391]\tTime 0.060 (0.060)\tData 0.002 (0.003)\tLoss 1.1694 (1.0713)\tPrec 61.719% (61.615%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.296 (0.296)\tLoss 1.2173 (1.2173)\tPrec 58.594% (58.594%)\n",
      " * Prec 54.520% \n",
      "best acc: 55.510000\n",
      "Epoch: [19][0/391]\tTime 0.318 (0.318)\tData 0.249 (0.249)\tLoss 1.0827 (1.0827)\tPrec 61.719% (61.719%)\n",
      "Epoch: [19][100/391]\tTime 0.055 (0.065)\tData 0.002 (0.005)\tLoss 1.1877 (1.0475)\tPrec 57.031% (62.786%)\n",
      "Epoch: [19][200/391]\tTime 0.050 (0.064)\tData 0.002 (0.004)\tLoss 1.1251 (1.0491)\tPrec 60.156% (62.636%)\n",
      "Epoch: [19][300/391]\tTime 0.065 (0.064)\tData 0.002 (0.003)\tLoss 0.9970 (1.0423)\tPrec 65.625% (62.949%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.278 (0.278)\tLoss 1.7361 (1.7361)\tPrec 49.219% (49.219%)\n",
      " * Prec 48.790% \n",
      "best acc: 55.510000\n",
      "Epoch: [20][0/391]\tTime 0.348 (0.348)\tData 0.276 (0.276)\tLoss 1.0346 (1.0346)\tPrec 59.375% (59.375%)\n",
      "Epoch: [20][100/391]\tTime 0.067 (0.067)\tData 0.002 (0.005)\tLoss 1.1080 (1.0514)\tPrec 58.594% (62.446%)\n",
      "Epoch: [20][200/391]\tTime 0.066 (0.065)\tData 0.002 (0.004)\tLoss 0.9948 (1.0305)\tPrec 66.406% (63.266%)\n",
      "Epoch: [20][300/391]\tTime 0.053 (0.064)\tData 0.002 (0.003)\tLoss 1.3432 (1.0362)\tPrec 52.344% (63.097%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.264 (0.264)\tLoss 1.7043 (1.7043)\tPrec 49.219% (49.219%)\n",
      " * Prec 46.040% \n",
      "best acc: 55.510000\n",
      "Epoch: [21][0/391]\tTime 0.303 (0.303)\tData 0.233 (0.233)\tLoss 0.9917 (0.9917)\tPrec 68.750% (68.750%)\n",
      "Epoch: [21][100/391]\tTime 0.071 (0.066)\tData 0.002 (0.005)\tLoss 1.0336 (1.0103)\tPrec 57.812% (64.209%)\n",
      "Epoch: [21][200/391]\tTime 0.067 (0.061)\tData 0.002 (0.003)\tLoss 1.0394 (1.0046)\tPrec 61.719% (64.381%)\n",
      "Epoch: [21][300/391]\tTime 0.053 (0.061)\tData 0.002 (0.003)\tLoss 0.8642 (1.0083)\tPrec 69.531% (64.179%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.289 (0.289)\tLoss 1.2851 (1.2851)\tPrec 60.156% (60.156%)\n",
      " * Prec 58.180% \n",
      "best acc: 58.180000\n",
      "Epoch: [22][0/391]\tTime 0.415 (0.415)\tData 0.338 (0.338)\tLoss 0.9826 (0.9826)\tPrec 67.188% (67.188%)\n",
      "Epoch: [22][100/391]\tTime 0.043 (0.059)\tData 0.002 (0.006)\tLoss 0.9694 (0.9964)\tPrec 61.719% (64.496%)\n",
      "Epoch: [22][200/391]\tTime 0.061 (0.055)\tData 0.002 (0.004)\tLoss 1.0327 (1.0005)\tPrec 62.500% (64.556%)\n",
      "Epoch: [22][300/391]\tTime 0.074 (0.059)\tData 0.002 (0.003)\tLoss 0.9261 (1.0032)\tPrec 65.625% (64.621%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.299 (0.299)\tLoss 1.3744 (1.3744)\tPrec 53.906% (53.906%)\n",
      " * Prec 52.040% \n",
      "best acc: 58.180000\n",
      "Epoch: [23][0/391]\tTime 0.425 (0.425)\tData 0.350 (0.350)\tLoss 0.9002 (0.9002)\tPrec 69.531% (69.531%)\n",
      "Epoch: [23][100/391]\tTime 0.065 (0.068)\tData 0.003 (0.006)\tLoss 1.0239 (1.0190)\tPrec 65.625% (63.954%)\n",
      "Epoch: [23][200/391]\tTime 0.061 (0.063)\tData 0.002 (0.004)\tLoss 0.8685 (1.0029)\tPrec 67.969% (64.315%)\n",
      "Epoch: [23][300/391]\tTime 0.064 (0.061)\tData 0.002 (0.003)\tLoss 0.9753 (0.9962)\tPrec 69.531% (64.460%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.250 (0.250)\tLoss 1.0474 (1.0474)\tPrec 62.500% (62.500%)\n",
      " * Prec 62.470% \n",
      "best acc: 62.470000\n",
      "Epoch: [24][0/391]\tTime 0.380 (0.380)\tData 0.307 (0.307)\tLoss 0.8555 (0.8555)\tPrec 69.531% (69.531%)\n",
      "Epoch: [24][100/391]\tTime 0.067 (0.068)\tData 0.002 (0.005)\tLoss 1.0572 (0.9582)\tPrec 55.469% (66.066%)\n",
      "Epoch: [24][200/391]\tTime 0.066 (0.065)\tData 0.003 (0.004)\tLoss 0.8606 (0.9632)\tPrec 71.875% (65.971%)\n",
      "Epoch: [24][300/391]\tTime 0.043 (0.063)\tData 0.002 (0.003)\tLoss 1.0267 (0.9707)\tPrec 60.938% (65.651%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.308 (0.308)\tLoss 1.0989 (1.0989)\tPrec 56.250% (56.250%)\n",
      " * Prec 58.980% \n",
      "best acc: 62.470000\n",
      "Epoch: [25][0/391]\tTime 0.413 (0.413)\tData 0.342 (0.342)\tLoss 1.0774 (1.0774)\tPrec 62.500% (62.500%)\n",
      "Epoch: [25][100/391]\tTime 0.061 (0.067)\tData 0.002 (0.006)\tLoss 0.9208 (0.9440)\tPrec 67.969% (66.863%)\n",
      "Epoch: [25][200/391]\tTime 0.064 (0.065)\tData 0.003 (0.004)\tLoss 1.1886 (0.9543)\tPrec 62.500% (66.527%)\n",
      "Epoch: [25][300/391]\tTime 0.066 (0.065)\tData 0.002 (0.004)\tLoss 1.0260 (0.9526)\tPrec 63.281% (66.562%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.277 (0.277)\tLoss 0.9070 (0.9070)\tPrec 66.406% (66.406%)\n",
      " * Prec 60.650% \n",
      "best acc: 62.470000\n",
      "Epoch: [26][0/391]\tTime 0.429 (0.429)\tData 0.356 (0.356)\tLoss 1.0694 (1.0694)\tPrec 57.812% (57.812%)\n",
      "Epoch: [26][100/391]\tTime 0.066 (0.061)\tData 0.003 (0.006)\tLoss 0.9989 (0.9630)\tPrec 64.062% (65.795%)\n",
      "Epoch: [26][200/391]\tTime 0.066 (0.063)\tData 0.002 (0.004)\tLoss 0.8963 (0.9518)\tPrec 68.750% (66.297%)\n",
      "Epoch: [26][300/391]\tTime 0.066 (0.064)\tData 0.002 (0.004)\tLoss 0.9808 (0.9539)\tPrec 68.750% (66.238%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.269 (0.269)\tLoss 1.4684 (1.4684)\tPrec 53.906% (53.906%)\n",
      " * Prec 53.090% \n",
      "best acc: 62.470000\n",
      "Epoch: [27][0/391]\tTime 0.329 (0.329)\tData 0.257 (0.257)\tLoss 0.9444 (0.9444)\tPrec 68.750% (68.750%)\n",
      "Epoch: [27][100/391]\tTime 0.047 (0.062)\tData 0.002 (0.005)\tLoss 0.9177 (0.9422)\tPrec 69.531% (66.592%)\n",
      "Epoch: [27][200/391]\tTime 0.068 (0.061)\tData 0.003 (0.004)\tLoss 0.9506 (0.9380)\tPrec 67.188% (66.744%)\n",
      "Epoch: [27][300/391]\tTime 0.069 (0.062)\tData 0.002 (0.003)\tLoss 0.8067 (0.9391)\tPrec 71.875% (66.798%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.339 (0.339)\tLoss 0.8655 (0.8655)\tPrec 66.406% (66.406%)\n",
      " * Prec 65.340% \n",
      "best acc: 65.340000\n",
      "Epoch: [28][0/391]\tTime 0.378 (0.378)\tData 0.300 (0.300)\tLoss 0.9108 (0.9108)\tPrec 67.969% (67.969%)\n",
      "Epoch: [28][100/391]\tTime 0.067 (0.072)\tData 0.002 (0.005)\tLoss 1.0241 (0.9174)\tPrec 67.969% (67.845%)\n",
      "Epoch: [28][200/391]\tTime 0.072 (0.067)\tData 0.003 (0.004)\tLoss 0.9734 (0.9161)\tPrec 63.281% (67.965%)\n",
      "Epoch: [28][300/391]\tTime 0.054 (0.067)\tData 0.002 (0.003)\tLoss 0.7889 (0.9144)\tPrec 69.531% (67.844%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.348 (0.348)\tLoss 0.9845 (0.9845)\tPrec 64.062% (64.062%)\n",
      " * Prec 64.330% \n",
      "best acc: 65.340000\n",
      "Epoch: [29][0/391]\tTime 0.371 (0.371)\tData 0.289 (0.289)\tLoss 0.7735 (0.7735)\tPrec 73.438% (73.438%)\n",
      "Epoch: [29][100/391]\tTime 0.073 (0.067)\tData 0.003 (0.005)\tLoss 0.8888 (0.9182)\tPrec 72.656% (67.876%)\n",
      "Epoch: [29][200/391]\tTime 0.067 (0.065)\tData 0.002 (0.004)\tLoss 0.7891 (0.9109)\tPrec 66.406% (67.942%)\n",
      "Epoch: [29][300/391]\tTime 0.067 (0.064)\tData 0.003 (0.003)\tLoss 0.9701 (0.9121)\tPrec 65.625% (67.857%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.293 (0.293)\tLoss 0.7958 (0.7958)\tPrec 75.781% (75.781%)\n",
      " * Prec 67.970% \n",
      "best acc: 67.970000\n",
      "Epoch: [30][0/391]\tTime 0.346 (0.346)\tData 0.273 (0.273)\tLoss 0.8432 (0.8432)\tPrec 76.562% (76.562%)\n",
      "Epoch: [30][100/391]\tTime 0.066 (0.065)\tData 0.003 (0.005)\tLoss 0.9560 (0.8898)\tPrec 63.281% (69.005%)\n",
      "Epoch: [30][200/391]\tTime 0.042 (0.062)\tData 0.002 (0.004)\tLoss 0.8619 (0.9013)\tPrec 67.969% (68.583%)\n",
      "Epoch: [30][300/391]\tTime 0.041 (0.056)\tData 0.002 (0.003)\tLoss 0.9208 (0.9033)\tPrec 67.188% (68.278%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.267 (0.267)\tLoss 0.8010 (0.8010)\tPrec 67.188% (67.188%)\n",
      " * Prec 66.060% \n",
      "best acc: 67.970000\n",
      "Epoch: [31][0/391]\tTime 0.396 (0.396)\tData 0.329 (0.329)\tLoss 0.8601 (0.8601)\tPrec 69.531% (69.531%)\n",
      "Epoch: [31][100/391]\tTime 0.060 (0.053)\tData 0.002 (0.005)\tLoss 0.9811 (0.8868)\tPrec 66.406% (68.804%)\n",
      "Epoch: [31][200/391]\tTime 0.060 (0.053)\tData 0.003 (0.004)\tLoss 0.8888 (0.8863)\tPrec 69.531% (68.808%)\n",
      "Epoch: [31][300/391]\tTime 0.038 (0.052)\tData 0.001 (0.003)\tLoss 0.8600 (0.8869)\tPrec 72.656% (68.843%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.276 (0.276)\tLoss 0.9363 (0.9363)\tPrec 67.188% (67.188%)\n",
      " * Prec 65.670% \n",
      "best acc: 67.970000\n",
      "Epoch: [32][0/391]\tTime 0.334 (0.334)\tData 0.260 (0.260)\tLoss 0.9360 (0.9360)\tPrec 64.844% (64.844%)\n",
      "Epoch: [32][100/391]\tTime 0.040 (0.050)\tData 0.002 (0.004)\tLoss 0.7949 (0.8740)\tPrec 78.125% (69.036%)\n",
      "Epoch: [32][200/391]\tTime 0.065 (0.051)\tData 0.002 (0.003)\tLoss 0.8850 (0.8749)\tPrec 67.188% (69.057%)\n",
      "Epoch: [32][300/391]\tTime 0.066 (0.056)\tData 0.002 (0.003)\tLoss 0.8271 (0.8778)\tPrec 71.875% (68.971%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.288 (0.288)\tLoss 0.9096 (0.9096)\tPrec 71.094% (71.094%)\n",
      " * Prec 66.140% \n",
      "best acc: 67.970000\n",
      "Epoch: [33][0/391]\tTime 0.361 (0.361)\tData 0.293 (0.293)\tLoss 0.8813 (0.8813)\tPrec 66.406% (66.406%)\n",
      "Epoch: [33][100/391]\tTime 0.064 (0.067)\tData 0.002 (0.005)\tLoss 0.8764 (0.8796)\tPrec 75.781% (68.526%)\n",
      "Epoch: [33][200/391]\tTime 0.059 (0.061)\tData 0.002 (0.004)\tLoss 0.9239 (0.8844)\tPrec 70.312% (68.540%)\n",
      "Epoch: [33][300/391]\tTime 0.075 (0.063)\tData 0.002 (0.003)\tLoss 0.7395 (0.8802)\tPrec 76.562% (68.877%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.288 (0.288)\tLoss 0.9159 (0.9159)\tPrec 70.312% (70.312%)\n",
      " * Prec 67.460% \n",
      "best acc: 67.970000\n",
      "Epoch: [34][0/391]\tTime 0.333 (0.333)\tData 0.246 (0.246)\tLoss 0.7812 (0.7812)\tPrec 74.219% (74.219%)\n",
      "Epoch: [34][100/391]\tTime 0.074 (0.076)\tData 0.002 (0.005)\tLoss 0.8345 (0.8714)\tPrec 69.531% (69.438%)\n",
      "Epoch: [34][200/391]\tTime 0.046 (0.071)\tData 0.002 (0.003)\tLoss 0.9354 (0.8732)\tPrec 66.406% (69.279%)\n",
      "Epoch: [34][300/391]\tTime 0.040 (0.068)\tData 0.001 (0.003)\tLoss 0.8053 (0.8665)\tPrec 69.531% (69.632%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.301 (0.301)\tLoss 1.2292 (1.2292)\tPrec 57.031% (57.031%)\n",
      " * Prec 60.050% \n",
      "best acc: 67.970000\n",
      "Epoch: [35][0/391]\tTime 0.359 (0.359)\tData 0.289 (0.289)\tLoss 0.7398 (0.7398)\tPrec 75.000% (75.000%)\n",
      "Epoch: [35][100/391]\tTime 0.066 (0.064)\tData 0.002 (0.005)\tLoss 0.8827 (0.8606)\tPrec 70.312% (69.554%)\n",
      "Epoch: [35][200/391]\tTime 0.063 (0.063)\tData 0.002 (0.004)\tLoss 0.9099 (0.8642)\tPrec 69.531% (69.675%)\n",
      "Epoch: [35][300/391]\tTime 0.067 (0.061)\tData 0.002 (0.003)\tLoss 0.7729 (0.8631)\tPrec 71.094% (69.630%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.278 (0.278)\tLoss 1.0706 (1.0706)\tPrec 64.062% (64.062%)\n",
      " * Prec 61.540% \n",
      "best acc: 67.970000\n",
      "Epoch: [36][0/391]\tTime 0.384 (0.384)\tData 0.311 (0.311)\tLoss 0.9287 (0.9287)\tPrec 67.969% (67.969%)\n",
      "Epoch: [36][100/391]\tTime 0.067 (0.067)\tData 0.003 (0.006)\tLoss 0.6657 (0.8797)\tPrec 77.344% (68.719%)\n",
      "Epoch: [36][200/391]\tTime 0.063 (0.065)\tData 0.002 (0.004)\tLoss 1.0613 (0.8681)\tPrec 63.281% (69.181%)\n",
      "Epoch: [36][300/391]\tTime 0.066 (0.065)\tData 0.002 (0.003)\tLoss 0.9819 (0.8676)\tPrec 64.062% (69.337%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.281 (0.281)\tLoss 1.6711 (1.6711)\tPrec 51.562% (51.562%)\n",
      " * Prec 46.500% \n",
      "best acc: 67.970000\n",
      "Epoch: [37][0/391]\tTime 0.372 (0.372)\tData 0.298 (0.298)\tLoss 0.9432 (0.9432)\tPrec 64.844% (64.844%)\n",
      "Epoch: [37][100/391]\tTime 0.067 (0.065)\tData 0.003 (0.005)\tLoss 0.6891 (0.8746)\tPrec 78.125% (68.711%)\n",
      "Epoch: [37][200/391]\tTime 0.066 (0.065)\tData 0.002 (0.004)\tLoss 0.8283 (0.8653)\tPrec 67.969% (69.174%)\n",
      "Epoch: [37][300/391]\tTime 0.042 (0.063)\tData 0.001 (0.003)\tLoss 0.8467 (0.8570)\tPrec 64.844% (69.547%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.272 (0.272)\tLoss 0.9116 (0.9116)\tPrec 66.406% (66.406%)\n",
      " * Prec 65.770% \n",
      "best acc: 67.970000\n",
      "Epoch: [38][0/391]\tTime 0.390 (0.390)\tData 0.312 (0.312)\tLoss 0.6262 (0.6262)\tPrec 79.688% (79.688%)\n",
      "Epoch: [38][100/391]\tTime 0.066 (0.074)\tData 0.003 (0.006)\tLoss 0.8934 (0.8374)\tPrec 72.656% (70.336%)\n",
      "Epoch: [38][200/391]\tTime 0.072 (0.069)\tData 0.002 (0.004)\tLoss 0.7062 (0.8475)\tPrec 73.438% (69.885%)\n",
      "Epoch: [38][300/391]\tTime 0.060 (0.067)\tData 0.002 (0.003)\tLoss 0.7741 (0.8503)\tPrec 77.344% (69.845%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.293 (0.293)\tLoss 0.8045 (0.8045)\tPrec 76.562% (76.562%)\n",
      " * Prec 66.680% \n",
      "best acc: 67.970000\n",
      "Epoch: [39][0/391]\tTime 0.390 (0.390)\tData 0.308 (0.308)\tLoss 0.6919 (0.6919)\tPrec 78.906% (78.906%)\n",
      "Epoch: [39][100/391]\tTime 0.067 (0.056)\tData 0.002 (0.005)\tLoss 0.8682 (0.8300)\tPrec 68.750% (70.869%)\n",
      "Epoch: [39][200/391]\tTime 0.064 (0.059)\tData 0.002 (0.004)\tLoss 0.8975 (0.8511)\tPrec 71.094% (69.935%)\n",
      "Epoch: [39][300/391]\tTime 0.067 (0.063)\tData 0.002 (0.003)\tLoss 0.9245 (0.8462)\tPrec 68.750% (70.175%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.304 (0.304)\tLoss 0.9827 (0.9827)\tPrec 62.500% (62.500%)\n",
      " * Prec 64.770% \n",
      "best acc: 67.970000\n",
      "Epoch: [40][0/391]\tTime 0.374 (0.374)\tData 0.299 (0.299)\tLoss 0.8836 (0.8836)\tPrec 67.969% (67.969%)\n",
      "Epoch: [40][100/391]\tTime 0.066 (0.068)\tData 0.002 (0.005)\tLoss 0.6126 (0.8277)\tPrec 78.125% (71.016%)\n",
      "Epoch: [40][200/391]\tTime 0.065 (0.066)\tData 0.002 (0.004)\tLoss 1.0233 (0.8295)\tPrec 62.500% (70.829%)\n",
      "Epoch: [40][300/391]\tTime 0.066 (0.065)\tData 0.003 (0.003)\tLoss 1.0558 (0.8375)\tPrec 64.844% (70.621%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.305 (0.305)\tLoss 1.2863 (1.2863)\tPrec 57.031% (57.031%)\n",
      " * Prec 56.880% \n",
      "best acc: 67.970000\n",
      "Epoch: [41][0/391]\tTime 0.404 (0.404)\tData 0.333 (0.333)\tLoss 0.7295 (0.7295)\tPrec 73.438% (73.438%)\n",
      "Epoch: [41][100/391]\tTime 0.066 (0.068)\tData 0.002 (0.006)\tLoss 0.7801 (0.8175)\tPrec 72.656% (70.931%)\n",
      "Epoch: [41][200/391]\tTime 0.067 (0.066)\tData 0.003 (0.004)\tLoss 0.9039 (0.8190)\tPrec 70.312% (71.028%)\n",
      "Epoch: [41][300/391]\tTime 0.064 (0.066)\tData 0.002 (0.004)\tLoss 0.8362 (0.8245)\tPrec 69.531% (71.102%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.317 (0.317)\tLoss 0.7544 (0.7544)\tPrec 74.219% (74.219%)\n",
      " * Prec 67.330% \n",
      "best acc: 67.970000\n",
      "Epoch: [42][0/391]\tTime 0.367 (0.367)\tData 0.296 (0.296)\tLoss 0.7293 (0.7293)\tPrec 76.562% (76.562%)\n",
      "Epoch: [42][100/391]\tTime 0.057 (0.064)\tData 0.002 (0.005)\tLoss 0.6907 (0.8298)\tPrec 77.344% (70.854%)\n",
      "Epoch: [42][200/391]\tTime 0.064 (0.065)\tData 0.002 (0.004)\tLoss 0.8174 (0.8337)\tPrec 70.312% (70.752%)\n",
      "Epoch: [42][300/391]\tTime 0.066 (0.065)\tData 0.002 (0.003)\tLoss 0.9884 (0.8411)\tPrec 60.156% (70.486%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.313 (0.313)\tLoss 0.8531 (0.8531)\tPrec 66.406% (66.406%)\n",
      " * Prec 65.140% \n",
      "best acc: 67.970000\n",
      "Epoch: [43][0/391]\tTime 0.349 (0.349)\tData 0.285 (0.285)\tLoss 0.7520 (0.7520)\tPrec 78.125% (78.125%)\n",
      "Epoch: [43][100/391]\tTime 0.059 (0.069)\tData 0.002 (0.005)\tLoss 0.8335 (0.8305)\tPrec 70.312% (70.150%)\n",
      "Epoch: [43][200/391]\tTime 0.072 (0.067)\tData 0.002 (0.004)\tLoss 0.8411 (0.8329)\tPrec 69.531% (70.460%)\n",
      "Epoch: [43][300/391]\tTime 0.066 (0.066)\tData 0.002 (0.003)\tLoss 0.9746 (0.8322)\tPrec 63.281% (70.575%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.267 (0.267)\tLoss 0.7789 (0.7789)\tPrec 70.312% (70.312%)\n",
      " * Prec 66.520% \n",
      "best acc: 67.970000\n",
      "Epoch: [44][0/391]\tTime 0.408 (0.408)\tData 0.334 (0.334)\tLoss 0.6853 (0.6853)\tPrec 77.344% (77.344%)\n",
      "Epoch: [44][100/391]\tTime 0.065 (0.076)\tData 0.002 (0.006)\tLoss 0.7618 (0.8289)\tPrec 73.438% (70.761%)\n",
      "Epoch: [44][200/391]\tTime 0.058 (0.069)\tData 0.002 (0.004)\tLoss 0.7797 (0.8232)\tPrec 74.219% (70.985%)\n",
      "Epoch: [44][300/391]\tTime 0.051 (0.065)\tData 0.002 (0.003)\tLoss 0.8622 (0.8218)\tPrec 71.875% (71.112%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.275 (0.275)\tLoss 0.7744 (0.7744)\tPrec 75.781% (75.781%)\n",
      " * Prec 69.120% \n",
      "best acc: 69.120000\n",
      "Epoch: [45][0/391]\tTime 0.412 (0.412)\tData 0.343 (0.343)\tLoss 0.9124 (0.9124)\tPrec 67.188% (67.188%)\n",
      "Epoch: [45][100/391]\tTime 0.058 (0.067)\tData 0.002 (0.006)\tLoss 0.7987 (0.8172)\tPrec 69.531% (71.527%)\n",
      "Epoch: [45][200/391]\tTime 0.065 (0.061)\tData 0.002 (0.004)\tLoss 0.8407 (0.8217)\tPrec 72.656% (71.171%)\n",
      "Epoch: [45][300/391]\tTime 0.067 (0.062)\tData 0.002 (0.003)\tLoss 0.8332 (0.8213)\tPrec 72.656% (71.247%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.305 (0.305)\tLoss 1.5228 (1.5228)\tPrec 46.875% (46.875%)\n",
      " * Prec 43.040% \n",
      "best acc: 69.120000\n",
      "Epoch: [46][0/391]\tTime 0.389 (0.389)\tData 0.319 (0.319)\tLoss 0.7519 (0.7519)\tPrec 73.438% (73.438%)\n",
      "Epoch: [46][100/391]\tTime 0.052 (0.065)\tData 0.002 (0.005)\tLoss 0.8234 (0.8161)\tPrec 68.750% (71.125%)\n",
      "Epoch: [46][200/391]\tTime 0.067 (0.064)\tData 0.003 (0.004)\tLoss 1.2173 (0.8155)\tPrec 63.281% (71.047%)\n",
      "Epoch: [46][300/391]\tTime 0.060 (0.063)\tData 0.002 (0.003)\tLoss 0.7311 (0.8139)\tPrec 71.094% (71.177%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.311 (0.311)\tLoss 0.8635 (0.8635)\tPrec 68.750% (68.750%)\n",
      " * Prec 63.180% \n",
      "best acc: 69.120000\n",
      "Epoch: [47][0/391]\tTime 0.393 (0.393)\tData 0.316 (0.316)\tLoss 0.9781 (0.9781)\tPrec 65.625% (65.625%)\n",
      "Epoch: [47][100/391]\tTime 0.070 (0.074)\tData 0.003 (0.006)\tLoss 0.9241 (0.8040)\tPrec 66.406% (71.790%)\n",
      "Epoch: [47][200/391]\tTime 0.067 (0.068)\tData 0.002 (0.004)\tLoss 0.9199 (0.8134)\tPrec 67.969% (71.556%)\n",
      "Epoch: [47][300/391]\tTime 0.068 (0.067)\tData 0.003 (0.003)\tLoss 0.8293 (0.8137)\tPrec 66.406% (71.688%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.319 (0.319)\tLoss 0.8869 (0.8869)\tPrec 71.875% (71.875%)\n",
      " * Prec 66.180% \n",
      "best acc: 69.120000\n",
      "Epoch: [48][0/391]\tTime 0.399 (0.399)\tData 0.324 (0.324)\tLoss 0.7629 (0.7629)\tPrec 76.562% (76.562%)\n",
      "Epoch: [48][100/391]\tTime 0.065 (0.068)\tData 0.002 (0.006)\tLoss 0.7856 (0.7967)\tPrec 77.344% (72.502%)\n",
      "Epoch: [48][200/391]\tTime 0.065 (0.065)\tData 0.002 (0.004)\tLoss 0.8241 (0.8014)\tPrec 69.531% (71.751%)\n",
      "Epoch: [48][300/391]\tTime 0.066 (0.065)\tData 0.002 (0.004)\tLoss 0.8553 (0.8069)\tPrec 69.531% (71.628%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.242 (0.242)\tLoss 0.6990 (0.6990)\tPrec 72.656% (72.656%)\n",
      " * Prec 70.110% \n",
      "best acc: 70.110000\n",
      "Epoch: [49][0/391]\tTime 0.438 (0.438)\tData 0.365 (0.365)\tLoss 0.9051 (0.9051)\tPrec 73.438% (73.438%)\n",
      "Epoch: [49][100/391]\tTime 0.072 (0.069)\tData 0.003 (0.006)\tLoss 0.8520 (0.8098)\tPrec 67.969% (71.852%)\n",
      "Epoch: [49][200/391]\tTime 0.071 (0.071)\tData 0.003 (0.004)\tLoss 0.6682 (0.8086)\tPrec 74.219% (71.891%)\n",
      "Epoch: [49][300/391]\tTime 0.063 (0.069)\tData 0.002 (0.004)\tLoss 0.8559 (0.8108)\tPrec 70.312% (71.704%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.270 (0.270)\tLoss 1.2239 (1.2239)\tPrec 58.594% (58.594%)\n",
      " * Prec 58.370% \n",
      "best acc: 70.110000\n",
      "Epoch: [50][0/391]\tTime 0.327 (0.327)\tData 0.254 (0.254)\tLoss 0.6404 (0.6404)\tPrec 78.125% (78.125%)\n",
      "Epoch: [50][100/391]\tTime 0.066 (0.067)\tData 0.003 (0.005)\tLoss 0.8800 (0.8157)\tPrec 70.312% (71.086%)\n",
      "Epoch: [50][200/391]\tTime 0.063 (0.066)\tData 0.002 (0.004)\tLoss 0.7760 (0.8159)\tPrec 73.438% (71.113%)\n",
      "Epoch: [50][300/391]\tTime 0.060 (0.065)\tData 0.002 (0.003)\tLoss 0.8508 (0.8131)\tPrec 70.312% (71.218%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.339 (0.339)\tLoss 1.1615 (1.1615)\tPrec 63.281% (63.281%)\n",
      " * Prec 61.120% \n",
      "best acc: 70.110000\n",
      "Epoch: [51][0/391]\tTime 0.414 (0.414)\tData 0.342 (0.342)\tLoss 0.8430 (0.8430)\tPrec 75.000% (75.000%)\n",
      "Epoch: [51][100/391]\tTime 0.077 (0.067)\tData 0.002 (0.006)\tLoss 0.8818 (0.8108)\tPrec 66.406% (71.705%)\n",
      "Epoch: [51][200/391]\tTime 0.066 (0.065)\tData 0.002 (0.004)\tLoss 0.7388 (0.8093)\tPrec 74.219% (71.665%)\n",
      "Epoch: [51][300/391]\tTime 0.044 (0.064)\tData 0.002 (0.004)\tLoss 0.7305 (0.8048)\tPrec 75.781% (71.769%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.292 (0.292)\tLoss 0.7297 (0.7297)\tPrec 72.656% (72.656%)\n",
      " * Prec 68.850% \n",
      "best acc: 70.110000\n",
      "Epoch: [52][0/391]\tTime 0.386 (0.386)\tData 0.316 (0.316)\tLoss 0.6296 (0.6296)\tPrec 76.562% (76.562%)\n",
      "Epoch: [52][100/391]\tTime 0.056 (0.067)\tData 0.002 (0.006)\tLoss 0.8582 (0.7894)\tPrec 65.625% (72.107%)\n",
      "Epoch: [52][200/391]\tTime 0.066 (0.068)\tData 0.002 (0.004)\tLoss 0.6785 (0.7885)\tPrec 77.344% (72.116%)\n",
      "Epoch: [52][300/391]\tTime 0.066 (0.067)\tData 0.002 (0.004)\tLoss 0.8448 (0.7948)\tPrec 70.312% (71.963%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.269 (0.269)\tLoss 0.7729 (0.7729)\tPrec 71.875% (71.875%)\n",
      " * Prec 70.980% \n",
      "best acc: 70.980000\n",
      "Epoch: [53][0/391]\tTime 0.320 (0.320)\tData 0.258 (0.258)\tLoss 0.8821 (0.8821)\tPrec 67.188% (67.188%)\n",
      "Epoch: [53][100/391]\tTime 0.052 (0.064)\tData 0.002 (0.005)\tLoss 0.8120 (0.7837)\tPrec 73.438% (71.999%)\n",
      "Epoch: [53][200/391]\tTime 0.069 (0.063)\tData 0.003 (0.004)\tLoss 0.7946 (0.7845)\tPrec 71.875% (72.108%)\n",
      "Epoch: [53][300/391]\tTime 0.066 (0.064)\tData 0.003 (0.003)\tLoss 0.7882 (0.7901)\tPrec 72.656% (72.033%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.306 (0.306)\tLoss 0.9873 (0.9873)\tPrec 64.062% (64.062%)\n",
      " * Prec 65.330% \n",
      "best acc: 70.980000\n",
      "Epoch: [54][0/391]\tTime 0.395 (0.395)\tData 0.324 (0.324)\tLoss 0.9029 (0.9029)\tPrec 73.438% (73.438%)\n",
      "Epoch: [54][100/391]\tTime 0.067 (0.069)\tData 0.003 (0.006)\tLoss 0.7138 (0.8104)\tPrec 72.656% (71.744%)\n",
      "Epoch: [54][200/391]\tTime 0.060 (0.067)\tData 0.002 (0.004)\tLoss 0.6215 (0.8025)\tPrec 78.906% (72.069%)\n",
      "Epoch: [54][300/391]\tTime 0.065 (0.068)\tData 0.002 (0.004)\tLoss 0.7926 (0.8034)\tPrec 77.344% (72.051%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.304 (0.304)\tLoss 0.9093 (0.9093)\tPrec 68.750% (68.750%)\n",
      " * Prec 64.810% \n",
      "best acc: 70.980000\n",
      "Epoch: [55][0/391]\tTime 0.373 (0.373)\tData 0.297 (0.297)\tLoss 0.7926 (0.7926)\tPrec 71.094% (71.094%)\n",
      "Epoch: [55][100/391]\tTime 0.066 (0.066)\tData 0.002 (0.005)\tLoss 0.8224 (0.7723)\tPrec 72.656% (72.842%)\n",
      "Epoch: [55][200/391]\tTime 0.067 (0.066)\tData 0.003 (0.004)\tLoss 0.7254 (0.7821)\tPrec 75.000% (72.481%)\n",
      "Epoch: [55][300/391]\tTime 0.074 (0.066)\tData 0.003 (0.003)\tLoss 0.8710 (0.7823)\tPrec 68.750% (72.539%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.324 (0.324)\tLoss 0.9540 (0.9540)\tPrec 67.969% (67.969%)\n",
      " * Prec 64.910% \n",
      "best acc: 70.980000\n",
      "Epoch: [56][0/391]\tTime 0.387 (0.387)\tData 0.314 (0.314)\tLoss 0.9666 (0.9666)\tPrec 65.625% (65.625%)\n",
      "Epoch: [56][100/391]\tTime 0.066 (0.058)\tData 0.002 (0.005)\tLoss 0.6715 (0.7751)\tPrec 77.344% (72.030%)\n",
      "Epoch: [56][200/391]\tTime 0.062 (0.060)\tData 0.002 (0.004)\tLoss 0.8506 (0.7816)\tPrec 71.875% (72.306%)\n",
      "Epoch: [56][300/391]\tTime 0.059 (0.060)\tData 0.002 (0.003)\tLoss 0.7501 (0.7785)\tPrec 70.312% (72.532%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.340 (0.340)\tLoss 1.1803 (1.1803)\tPrec 59.375% (59.375%)\n",
      " * Prec 59.770% \n",
      "best acc: 70.980000\n",
      "Epoch: [57][0/391]\tTime 0.354 (0.354)\tData 0.291 (0.291)\tLoss 0.7248 (0.7248)\tPrec 76.562% (76.562%)\n",
      "Epoch: [57][100/391]\tTime 0.064 (0.060)\tData 0.002 (0.005)\tLoss 0.7047 (0.7427)\tPrec 78.125% (73.933%)\n",
      "Epoch: [57][200/391]\tTime 0.054 (0.058)\tData 0.002 (0.004)\tLoss 0.8056 (0.7433)\tPrec 73.438% (73.846%)\n",
      "Epoch: [57][300/391]\tTime 0.071 (0.060)\tData 0.002 (0.003)\tLoss 0.7328 (0.7477)\tPrec 73.438% (73.720%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.283 (0.283)\tLoss 0.7224 (0.7224)\tPrec 73.438% (73.438%)\n",
      " * Prec 72.070% \n",
      "best acc: 72.070000\n",
      "Epoch: [58][0/391]\tTime 0.346 (0.346)\tData 0.273 (0.273)\tLoss 0.6962 (0.6962)\tPrec 73.438% (73.438%)\n",
      "Epoch: [58][100/391]\tTime 0.066 (0.073)\tData 0.002 (0.005)\tLoss 0.7034 (0.7189)\tPrec 75.000% (74.861%)\n",
      "Epoch: [58][200/391]\tTime 0.066 (0.068)\tData 0.002 (0.004)\tLoss 0.7995 (0.7332)\tPrec 74.219% (74.530%)\n",
      "Epoch: [58][300/391]\tTime 0.064 (0.067)\tData 0.003 (0.003)\tLoss 0.6173 (0.7414)\tPrec 80.469% (74.208%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.278 (0.278)\tLoss 0.7446 (0.7446)\tPrec 76.562% (76.562%)\n",
      " * Prec 67.760% \n",
      "best acc: 72.070000\n",
      "Epoch: [59][0/391]\tTime 0.348 (0.348)\tData 0.276 (0.276)\tLoss 0.7229 (0.7229)\tPrec 76.562% (76.562%)\n",
      "Epoch: [59][100/391]\tTime 0.065 (0.067)\tData 0.002 (0.005)\tLoss 0.7297 (0.7580)\tPrec 71.875% (73.438%)\n",
      "Epoch: [59][200/391]\tTime 0.058 (0.063)\tData 0.002 (0.004)\tLoss 0.7794 (0.7730)\tPrec 72.656% (72.734%)\n",
      "Epoch: [59][300/391]\tTime 0.065 (0.063)\tData 0.003 (0.003)\tLoss 0.6252 (0.7737)\tPrec 78.906% (72.633%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.283 (0.283)\tLoss 0.8746 (0.8746)\tPrec 68.750% (68.750%)\n",
      " * Prec 67.430% \n",
      "best acc: 72.070000\n",
      "Epoch: [60][0/391]\tTime 0.329 (0.329)\tData 0.257 (0.257)\tLoss 0.7905 (0.7905)\tPrec 69.531% (69.531%)\n",
      "Epoch: [60][100/391]\tTime 0.067 (0.066)\tData 0.002 (0.005)\tLoss 0.8048 (0.7652)\tPrec 67.188% (72.935%)\n",
      "Epoch: [60][200/391]\tTime 0.073 (0.067)\tData 0.002 (0.004)\tLoss 0.9260 (0.7552)\tPrec 67.188% (73.434%)\n",
      "Epoch: [60][300/391]\tTime 0.066 (0.069)\tData 0.002 (0.003)\tLoss 0.8755 (0.7538)\tPrec 71.094% (73.528%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.287 (0.287)\tLoss 0.7641 (0.7641)\tPrec 74.219% (74.219%)\n",
      " * Prec 72.270% \n",
      "best acc: 72.270000\n",
      "Epoch: [61][0/391]\tTime 0.350 (0.350)\tData 0.276 (0.276)\tLoss 0.6318 (0.6318)\tPrec 76.562% (76.562%)\n",
      "Epoch: [61][100/391]\tTime 0.068 (0.065)\tData 0.003 (0.005)\tLoss 0.6812 (0.7423)\tPrec 78.906% (73.809%)\n",
      "Epoch: [61][200/391]\tTime 0.059 (0.065)\tData 0.002 (0.004)\tLoss 0.8041 (0.7375)\tPrec 71.094% (73.966%)\n",
      "Epoch: [61][300/391]\tTime 0.066 (0.065)\tData 0.002 (0.003)\tLoss 0.6761 (0.7380)\tPrec 75.000% (74.115%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.279 (0.279)\tLoss 0.9675 (0.9675)\tPrec 64.062% (64.062%)\n",
      " * Prec 64.600% \n",
      "best acc: 72.270000\n",
      "Epoch: [62][0/391]\tTime 0.336 (0.336)\tData 0.262 (0.262)\tLoss 0.7225 (0.7225)\tPrec 73.438% (73.438%)\n",
      "Epoch: [62][100/391]\tTime 0.052 (0.059)\tData 0.002 (0.005)\tLoss 0.7550 (0.7214)\tPrec 76.562% (74.636%)\n",
      "Epoch: [62][200/391]\tTime 0.074 (0.057)\tData 0.002 (0.003)\tLoss 0.7611 (0.7288)\tPrec 71.875% (74.386%)\n",
      "Epoch: [62][300/391]\tTime 0.070 (0.062)\tData 0.004 (0.003)\tLoss 0.5809 (0.7280)\tPrec 82.031% (74.489%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.344 (0.344)\tLoss 0.8933 (0.8933)\tPrec 67.969% (67.969%)\n",
      " * Prec 70.870% \n",
      "best acc: 72.270000\n",
      "Epoch: [63][0/391]\tTime 0.362 (0.362)\tData 0.289 (0.289)\tLoss 0.7669 (0.7669)\tPrec 74.219% (74.219%)\n",
      "Epoch: [63][100/391]\tTime 0.051 (0.062)\tData 0.002 (0.005)\tLoss 0.7828 (0.7233)\tPrec 71.875% (74.613%)\n",
      "Epoch: [63][200/391]\tTime 0.068 (0.062)\tData 0.002 (0.004)\tLoss 0.4829 (0.7161)\tPrec 83.594% (74.720%)\n",
      "Epoch: [63][300/391]\tTime 0.053 (0.063)\tData 0.002 (0.003)\tLoss 0.7534 (0.7141)\tPrec 70.312% (74.855%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.247 (0.247)\tLoss 0.8108 (0.8108)\tPrec 72.656% (72.656%)\n",
      " * Prec 71.320% \n",
      "best acc: 72.270000\n",
      "Epoch: [64][0/391]\tTime 0.360 (0.360)\tData 0.301 (0.301)\tLoss 0.7278 (0.7278)\tPrec 71.875% (71.875%)\n",
      "Epoch: [64][100/391]\tTime 0.043 (0.056)\tData 0.001 (0.005)\tLoss 0.6133 (0.7108)\tPrec 78.906% (74.876%)\n",
      "Epoch: [64][200/391]\tTime 0.065 (0.058)\tData 0.003 (0.004)\tLoss 0.7420 (0.7109)\tPrec 74.219% (75.027%)\n",
      "Epoch: [64][300/391]\tTime 0.048 (0.058)\tData 0.002 (0.003)\tLoss 0.6505 (0.7075)\tPrec 81.250% (75.130%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.260 (0.260)\tLoss 0.8626 (0.8626)\tPrec 72.656% (72.656%)\n",
      " * Prec 70.150% \n",
      "best acc: 72.270000\n",
      "Epoch: [65][0/391]\tTime 0.387 (0.387)\tData 0.315 (0.315)\tLoss 0.8241 (0.8241)\tPrec 72.656% (72.656%)\n",
      "Epoch: [65][100/391]\tTime 0.046 (0.054)\tData 0.002 (0.005)\tLoss 0.6379 (0.6964)\tPrec 78.906% (75.619%)\n",
      "Epoch: [65][200/391]\tTime 0.064 (0.053)\tData 0.003 (0.004)\tLoss 0.5570 (0.6942)\tPrec 82.031% (75.719%)\n",
      "Epoch: [65][300/391]\tTime 0.045 (0.053)\tData 0.002 (0.003)\tLoss 0.7657 (0.6969)\tPrec 71.094% (75.636%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.235 (0.235)\tLoss 1.2092 (1.2092)\tPrec 63.281% (63.281%)\n",
      " * Prec 64.210% \n",
      "best acc: 72.270000\n",
      "Epoch: [66][0/391]\tTime 0.328 (0.328)\tData 0.273 (0.273)\tLoss 0.6631 (0.6631)\tPrec 73.438% (73.438%)\n",
      "Epoch: [66][100/391]\tTime 0.058 (0.060)\tData 0.002 (0.005)\tLoss 0.7767 (0.6738)\tPrec 69.531% (75.882%)\n",
      "Epoch: [66][200/391]\tTime 0.063 (0.065)\tData 0.002 (0.004)\tLoss 0.5506 (0.6846)\tPrec 80.469% (75.773%)\n",
      "Epoch: [66][300/391]\tTime 0.062 (0.062)\tData 0.002 (0.003)\tLoss 0.6449 (0.6894)\tPrec 72.656% (75.644%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.281 (0.281)\tLoss 0.6505 (0.6505)\tPrec 76.562% (76.562%)\n",
      " * Prec 72.970% \n",
      "best acc: 72.970000\n",
      "Epoch: [67][0/391]\tTime 0.390 (0.390)\tData 0.312 (0.312)\tLoss 0.8174 (0.8174)\tPrec 71.094% (71.094%)\n",
      "Epoch: [67][100/391]\tTime 0.062 (0.066)\tData 0.002 (0.005)\tLoss 0.6485 (0.6787)\tPrec 78.125% (76.369%)\n",
      "Epoch: [67][200/391]\tTime 0.078 (0.065)\tData 0.003 (0.004)\tLoss 0.6815 (0.6838)\tPrec 80.469% (76.143%)\n",
      "Epoch: [67][300/391]\tTime 0.043 (0.064)\tData 0.002 (0.003)\tLoss 0.6784 (0.6820)\tPrec 76.562% (76.197%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.321 (0.321)\tLoss 0.6764 (0.6764)\tPrec 75.781% (75.781%)\n",
      " * Prec 72.200% \n",
      "best acc: 72.970000\n",
      "Epoch: [68][0/391]\tTime 0.374 (0.374)\tData 0.304 (0.304)\tLoss 0.8001 (0.8001)\tPrec 69.531% (69.531%)\n",
      "Epoch: [68][100/391]\tTime 0.041 (0.056)\tData 0.002 (0.005)\tLoss 0.5604 (0.6766)\tPrec 80.469% (76.276%)\n",
      "Epoch: [68][200/391]\tTime 0.041 (0.056)\tData 0.001 (0.004)\tLoss 0.6517 (0.6738)\tPrec 77.344% (76.395%)\n",
      "Epoch: [68][300/391]\tTime 0.046 (0.054)\tData 0.002 (0.003)\tLoss 0.6550 (0.6751)\tPrec 75.781% (76.300%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.283 (0.283)\tLoss 0.8037 (0.8037)\tPrec 71.875% (71.875%)\n",
      " * Prec 71.880% \n",
      "best acc: 72.970000\n",
      "Epoch: [69][0/391]\tTime 0.380 (0.380)\tData 0.306 (0.306)\tLoss 0.7934 (0.7934)\tPrec 68.750% (68.750%)\n",
      "Epoch: [69][100/391]\tTime 0.053 (0.060)\tData 0.002 (0.005)\tLoss 0.7332 (0.6539)\tPrec 72.656% (77.197%)\n",
      "Epoch: [69][200/391]\tTime 0.045 (0.059)\tData 0.002 (0.004)\tLoss 0.7208 (0.6703)\tPrec 72.656% (76.528%)\n",
      "Epoch: [69][300/391]\tTime 0.063 (0.060)\tData 0.002 (0.003)\tLoss 0.6604 (0.6700)\tPrec 75.000% (76.513%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.270 (0.270)\tLoss 1.0058 (1.0058)\tPrec 67.969% (67.969%)\n",
      " * Prec 65.810% \n",
      "best acc: 72.970000\n",
      "Epoch: [70][0/391]\tTime 0.388 (0.388)\tData 0.320 (0.320)\tLoss 0.6032 (0.6032)\tPrec 82.031% (82.031%)\n",
      "Epoch: [70][100/391]\tTime 0.062 (0.058)\tData 0.002 (0.005)\tLoss 0.6712 (0.6321)\tPrec 82.031% (78.063%)\n",
      "Epoch: [70][200/391]\tTime 0.066 (0.061)\tData 0.002 (0.004)\tLoss 0.5381 (0.6128)\tPrec 83.594% (78.739%)\n",
      "Epoch: [70][300/391]\tTime 0.073 (0.062)\tData 0.002 (0.003)\tLoss 0.6021 (0.6044)\tPrec 78.125% (79.059%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.351 (0.351)\tLoss 0.5775 (0.5775)\tPrec 81.250% (81.250%)\n",
      " * Prec 78.700% \n",
      "best acc: 78.700000\n",
      "Epoch: [71][0/391]\tTime 0.378 (0.378)\tData 0.301 (0.301)\tLoss 0.5741 (0.5741)\tPrec 82.812% (82.812%)\n",
      "Epoch: [71][100/391]\tTime 0.076 (0.071)\tData 0.006 (0.006)\tLoss 0.5377 (0.5767)\tPrec 81.250% (80.507%)\n",
      "Epoch: [71][200/391]\tTime 0.065 (0.067)\tData 0.002 (0.004)\tLoss 0.6013 (0.5807)\tPrec 78.906% (80.197%)\n",
      "Epoch: [71][300/391]\tTime 0.066 (0.066)\tData 0.002 (0.003)\tLoss 0.6559 (0.5840)\tPrec 75.000% (79.898%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.233 (0.233)\tLoss 0.5335 (0.5335)\tPrec 80.469% (80.469%)\n",
      " * Prec 79.350% \n",
      "best acc: 79.350000\n",
      "Epoch: [72][0/391]\tTime 0.408 (0.408)\tData 0.337 (0.337)\tLoss 0.6139 (0.6139)\tPrec 75.781% (75.781%)\n",
      "Epoch: [72][100/391]\tTime 0.063 (0.067)\tData 0.003 (0.008)\tLoss 0.5276 (0.5851)\tPrec 81.250% (79.602%)\n",
      "Epoch: [72][200/391]\tTime 0.063 (0.064)\tData 0.002 (0.005)\tLoss 0.4246 (0.5782)\tPrec 85.156% (79.800%)\n",
      "Epoch: [72][300/391]\tTime 0.064 (0.062)\tData 0.002 (0.004)\tLoss 0.5549 (0.5767)\tPrec 83.594% (79.838%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.266 (0.266)\tLoss 0.5607 (0.5607)\tPrec 79.688% (79.688%)\n",
      " * Prec 76.900% \n",
      "best acc: 79.350000\n",
      "Epoch: [73][0/391]\tTime 0.350 (0.350)\tData 0.290 (0.290)\tLoss 0.5956 (0.5956)\tPrec 79.688% (79.688%)\n",
      "Epoch: [73][100/391]\tTime 0.064 (0.062)\tData 0.002 (0.005)\tLoss 0.7294 (0.5724)\tPrec 75.781% (80.484%)\n",
      "Epoch: [73][200/391]\tTime 0.068 (0.064)\tData 0.003 (0.004)\tLoss 0.5634 (0.5715)\tPrec 79.688% (80.014%)\n",
      "Epoch: [73][300/391]\tTime 0.068 (0.063)\tData 0.002 (0.003)\tLoss 0.6062 (0.5719)\tPrec 82.031% (80.126%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.294 (0.294)\tLoss 0.6511 (0.6511)\tPrec 78.906% (78.906%)\n",
      " * Prec 76.500% \n",
      "best acc: 79.350000\n",
      "Epoch: [74][0/391]\tTime 0.351 (0.351)\tData 0.281 (0.281)\tLoss 0.5666 (0.5666)\tPrec 79.688% (79.688%)\n",
      "Epoch: [74][100/391]\tTime 0.057 (0.058)\tData 0.002 (0.005)\tLoss 0.5825 (0.5882)\tPrec 78.906% (79.401%)\n",
      "Epoch: [74][200/391]\tTime 0.057 (0.056)\tData 0.002 (0.003)\tLoss 0.6068 (0.5778)\tPrec 80.469% (79.812%)\n",
      "Epoch: [74][300/391]\tTime 0.047 (0.055)\tData 0.002 (0.003)\tLoss 0.5064 (0.5759)\tPrec 85.938% (79.950%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.306 (0.306)\tLoss 0.6870 (0.6870)\tPrec 78.906% (78.906%)\n",
      " * Prec 74.980% \n",
      "best acc: 79.350000\n",
      "Epoch: [75][0/391]\tTime 0.374 (0.374)\tData 0.303 (0.303)\tLoss 0.6236 (0.6236)\tPrec 75.781% (75.781%)\n",
      "Epoch: [75][100/391]\tTime 0.043 (0.056)\tData 0.001 (0.005)\tLoss 0.5871 (0.5710)\tPrec 81.250% (80.237%)\n",
      "Epoch: [75][200/391]\tTime 0.047 (0.058)\tData 0.002 (0.004)\tLoss 0.5400 (0.5702)\tPrec 77.344% (80.123%)\n",
      "Epoch: [75][300/391]\tTime 0.061 (0.058)\tData 0.003 (0.003)\tLoss 0.5683 (0.5688)\tPrec 80.469% (80.173%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.266 (0.266)\tLoss 0.5725 (0.5725)\tPrec 84.375% (84.375%)\n",
      " * Prec 77.990% \n",
      "best acc: 79.350000\n",
      "Epoch: [76][0/391]\tTime 0.314 (0.314)\tData 0.249 (0.249)\tLoss 0.4797 (0.4797)\tPrec 85.156% (85.156%)\n",
      "Epoch: [76][100/391]\tTime 0.052 (0.063)\tData 0.002 (0.005)\tLoss 0.4974 (0.5649)\tPrec 81.250% (80.121%)\n",
      "Epoch: [76][200/391]\tTime 0.061 (0.059)\tData 0.002 (0.003)\tLoss 0.5654 (0.5652)\tPrec 81.250% (80.181%)\n",
      "Epoch: [76][300/391]\tTime 0.046 (0.057)\tData 0.002 (0.003)\tLoss 0.5926 (0.5622)\tPrec 79.688% (80.305%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.262 (0.262)\tLoss 0.4950 (0.4950)\tPrec 81.250% (81.250%)\n",
      " * Prec 76.880% \n",
      "best acc: 79.350000\n",
      "Epoch: [77][0/391]\tTime 0.360 (0.360)\tData 0.311 (0.311)\tLoss 0.5270 (0.5270)\tPrec 85.938% (85.938%)\n",
      "Epoch: [77][100/391]\tTime 0.056 (0.061)\tData 0.002 (0.005)\tLoss 0.4704 (0.5593)\tPrec 87.500% (80.260%)\n",
      "Epoch: [77][200/391]\tTime 0.047 (0.055)\tData 0.002 (0.004)\tLoss 0.6961 (0.5666)\tPrec 78.906% (80.243%)\n",
      "Epoch: [77][300/391]\tTime 0.047 (0.056)\tData 0.002 (0.003)\tLoss 0.5995 (0.5657)\tPrec 77.344% (80.352%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.233 (0.233)\tLoss 0.5388 (0.5388)\tPrec 84.375% (84.375%)\n",
      " * Prec 76.840% \n",
      "best acc: 79.350000\n",
      "Epoch: [78][0/391]\tTime 0.360 (0.360)\tData 0.290 (0.290)\tLoss 0.6603 (0.6603)\tPrec 77.344% (77.344%)\n",
      "Epoch: [78][100/391]\tTime 0.064 (0.061)\tData 0.002 (0.005)\tLoss 0.5838 (0.5538)\tPrec 79.688% (80.593%)\n",
      "Epoch: [78][200/391]\tTime 0.064 (0.062)\tData 0.002 (0.004)\tLoss 0.7034 (0.5602)\tPrec 74.219% (80.492%)\n",
      "Epoch: [78][300/391]\tTime 0.066 (0.063)\tData 0.002 (0.003)\tLoss 0.5561 (0.5660)\tPrec 78.906% (80.173%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.339 (0.339)\tLoss 0.6331 (0.6331)\tPrec 78.125% (78.125%)\n",
      " * Prec 74.750% \n",
      "best acc: 79.350000\n",
      "Epoch: [79][0/391]\tTime 0.386 (0.386)\tData 0.310 (0.310)\tLoss 0.6583 (0.6583)\tPrec 75.781% (75.781%)\n",
      "Epoch: [79][100/391]\tTime 0.064 (0.069)\tData 0.002 (0.005)\tLoss 0.4863 (0.5635)\tPrec 85.938% (80.291%)\n",
      "Epoch: [79][200/391]\tTime 0.066 (0.067)\tData 0.002 (0.004)\tLoss 0.5027 (0.5668)\tPrec 79.688% (80.340%)\n",
      "Epoch: [79][300/391]\tTime 0.073 (0.067)\tData 0.002 (0.003)\tLoss 0.6406 (0.5652)\tPrec 82.812% (80.220%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.292 (0.292)\tLoss 0.4993 (0.4993)\tPrec 82.812% (82.812%)\n",
      " * Prec 77.140% \n",
      "best acc: 79.350000\n",
      "Epoch: [80][0/391]\tTime 0.359 (0.359)\tData 0.278 (0.278)\tLoss 0.4538 (0.4538)\tPrec 88.281% (88.281%)\n",
      "Epoch: [80][100/391]\tTime 0.075 (0.078)\tData 0.002 (0.005)\tLoss 0.6216 (0.5591)\tPrec 75.000% (80.507%)\n",
      "Epoch: [80][200/391]\tTime 0.065 (0.073)\tData 0.002 (0.004)\tLoss 0.4545 (0.5658)\tPrec 85.156% (80.302%)\n",
      "Epoch: [80][300/391]\tTime 0.060 (0.071)\tData 0.003 (0.003)\tLoss 0.5965 (0.5669)\tPrec 81.250% (80.126%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.314 (0.314)\tLoss 0.5201 (0.5201)\tPrec 82.031% (82.031%)\n",
      " * Prec 78.440% \n",
      "best acc: 79.350000\n",
      "Epoch: [81][0/391]\tTime 0.415 (0.415)\tData 0.339 (0.339)\tLoss 0.5825 (0.5825)\tPrec 79.688% (79.688%)\n",
      "Epoch: [81][100/391]\tTime 0.075 (0.078)\tData 0.002 (0.006)\tLoss 0.5248 (0.5509)\tPrec 81.250% (80.709%)\n",
      "Epoch: [81][200/391]\tTime 0.073 (0.074)\tData 0.002 (0.004)\tLoss 0.4363 (0.5623)\tPrec 85.156% (80.348%)\n",
      "Epoch: [81][300/391]\tTime 0.067 (0.072)\tData 0.002 (0.004)\tLoss 0.6453 (0.5645)\tPrec 75.781% (80.261%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.313 (0.313)\tLoss 0.5440 (0.5440)\tPrec 80.469% (80.469%)\n",
      " * Prec 77.640% \n",
      "best acc: 79.350000\n",
      "Epoch: [82][0/391]\tTime 0.330 (0.330)\tData 0.271 (0.271)\tLoss 0.5017 (0.5017)\tPrec 84.375% (84.375%)\n",
      "Epoch: [82][100/391]\tTime 0.064 (0.063)\tData 0.003 (0.005)\tLoss 0.6171 (0.5565)\tPrec 82.031% (80.739%)\n",
      "Epoch: [82][200/391]\tTime 0.073 (0.067)\tData 0.002 (0.004)\tLoss 0.7160 (0.5669)\tPrec 75.000% (80.387%)\n",
      "Epoch: [82][300/391]\tTime 0.074 (0.069)\tData 0.002 (0.003)\tLoss 0.4881 (0.5675)\tPrec 85.938% (80.160%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.321 (0.321)\tLoss 0.6203 (0.6203)\tPrec 76.562% (76.562%)\n",
      " * Prec 75.630% \n",
      "best acc: 79.350000\n",
      "Epoch: [83][0/391]\tTime 0.438 (0.438)\tData 0.358 (0.358)\tLoss 0.5959 (0.5959)\tPrec 83.594% (83.594%)\n",
      "Epoch: [83][100/391]\tTime 0.066 (0.069)\tData 0.002 (0.006)\tLoss 0.6593 (0.5783)\tPrec 79.688% (79.556%)\n",
      "Epoch: [83][200/391]\tTime 0.065 (0.067)\tData 0.002 (0.004)\tLoss 0.5513 (0.5734)\tPrec 82.031% (79.878%)\n",
      "Epoch: [83][300/391]\tTime 0.051 (0.065)\tData 0.001 (0.004)\tLoss 0.5214 (0.5660)\tPrec 78.906% (80.077%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.303 (0.303)\tLoss 0.5350 (0.5350)\tPrec 83.594% (83.594%)\n",
      " * Prec 77.530% \n",
      "best acc: 79.350000\n",
      "Epoch: [84][0/391]\tTime 0.439 (0.439)\tData 0.368 (0.368)\tLoss 0.6135 (0.6135)\tPrec 75.781% (75.781%)\n",
      "Epoch: [84][100/391]\tTime 0.066 (0.065)\tData 0.002 (0.006)\tLoss 0.4657 (0.5573)\tPrec 85.156% (80.345%)\n",
      "Epoch: [84][200/391]\tTime 0.071 (0.064)\tData 0.002 (0.004)\tLoss 0.6225 (0.5675)\tPrec 77.344% (80.053%)\n",
      "Epoch: [84][300/391]\tTime 0.064 (0.064)\tData 0.002 (0.004)\tLoss 0.6568 (0.5688)\tPrec 76.562% (80.025%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.369 (0.369)\tLoss 0.6616 (0.6616)\tPrec 78.906% (78.906%)\n",
      " * Prec 76.490% \n",
      "best acc: 79.350000\n",
      "Epoch: [85][0/391]\tTime 0.407 (0.407)\tData 0.335 (0.335)\tLoss 0.5738 (0.5738)\tPrec 82.031% (82.031%)\n",
      "Epoch: [85][100/391]\tTime 0.066 (0.069)\tData 0.003 (0.006)\tLoss 0.4573 (0.5613)\tPrec 83.594% (79.927%)\n",
      "Epoch: [85][200/391]\tTime 0.066 (0.067)\tData 0.002 (0.004)\tLoss 0.5191 (0.5553)\tPrec 84.375% (80.344%)\n",
      "Epoch: [85][300/391]\tTime 0.051 (0.066)\tData 0.001 (0.004)\tLoss 0.5206 (0.5573)\tPrec 80.469% (80.336%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.253 (0.253)\tLoss 0.6021 (0.6021)\tPrec 82.812% (82.812%)\n",
      " * Prec 77.380% \n",
      "best acc: 79.350000\n",
      "Epoch: [86][0/391]\tTime 0.379 (0.379)\tData 0.307 (0.307)\tLoss 0.5665 (0.5665)\tPrec 76.562% (76.562%)\n",
      "Epoch: [86][100/391]\tTime 0.061 (0.068)\tData 0.002 (0.005)\tLoss 0.4502 (0.5479)\tPrec 84.375% (80.832%)\n",
      "Epoch: [86][200/391]\tTime 0.064 (0.066)\tData 0.002 (0.004)\tLoss 0.4766 (0.5583)\tPrec 84.375% (80.375%)\n",
      "Epoch: [86][300/391]\tTime 0.043 (0.065)\tData 0.002 (0.003)\tLoss 0.6162 (0.5629)\tPrec 77.344% (80.401%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.303 (0.303)\tLoss 0.5544 (0.5544)\tPrec 82.031% (82.031%)\n",
      " * Prec 78.770% \n",
      "best acc: 79.350000\n",
      "Epoch: [87][0/391]\tTime 0.323 (0.323)\tData 0.252 (0.252)\tLoss 0.4986 (0.4986)\tPrec 78.125% (78.125%)\n",
      "Epoch: [87][100/391]\tTime 0.065 (0.066)\tData 0.002 (0.005)\tLoss 0.5984 (0.5520)\tPrec 78.125% (80.770%)\n",
      "Epoch: [87][200/391]\tTime 0.076 (0.066)\tData 0.002 (0.004)\tLoss 0.5202 (0.5583)\tPrec 82.812% (80.395%)\n",
      "Epoch: [87][300/391]\tTime 0.059 (0.068)\tData 0.002 (0.003)\tLoss 0.4753 (0.5589)\tPrec 82.031% (80.435%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.329 (0.329)\tLoss 0.5947 (0.5947)\tPrec 81.250% (81.250%)\n",
      " * Prec 77.280% \n",
      "best acc: 79.350000\n",
      "Epoch: [88][0/391]\tTime 0.371 (0.371)\tData 0.297 (0.297)\tLoss 0.6857 (0.6857)\tPrec 74.219% (74.219%)\n",
      "Epoch: [88][100/391]\tTime 0.081 (0.075)\tData 0.002 (0.005)\tLoss 0.5397 (0.5584)\tPrec 82.031% (80.391%)\n",
      "Epoch: [88][200/391]\tTime 0.066 (0.065)\tData 0.002 (0.004)\tLoss 0.4518 (0.5607)\tPrec 84.375% (80.469%)\n",
      "Epoch: [88][300/391]\tTime 0.064 (0.066)\tData 0.002 (0.003)\tLoss 0.4395 (0.5585)\tPrec 84.375% (80.578%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.287 (0.287)\tLoss 0.5918 (0.5918)\tPrec 75.000% (75.000%)\n",
      " * Prec 77.530% \n",
      "best acc: 79.350000\n",
      "Epoch: [89][0/391]\tTime 0.413 (0.413)\tData 0.338 (0.338)\tLoss 0.6109 (0.6109)\tPrec 82.812% (82.812%)\n",
      "Epoch: [89][100/391]\tTime 0.061 (0.065)\tData 0.002 (0.006)\tLoss 0.6245 (0.5725)\tPrec 74.219% (79.989%)\n",
      "Epoch: [89][200/391]\tTime 0.066 (0.065)\tData 0.003 (0.004)\tLoss 0.5765 (0.5571)\tPrec 81.250% (80.453%)\n",
      "Epoch: [89][300/391]\tTime 0.066 (0.065)\tData 0.002 (0.003)\tLoss 0.5971 (0.5628)\tPrec 75.781% (80.378%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.307 (0.307)\tLoss 0.6148 (0.6148)\tPrec 78.906% (78.906%)\n",
      " * Prec 77.260% \n",
      "best acc: 79.350000\n",
      "Epoch: [90][0/391]\tTime 0.346 (0.346)\tData 0.274 (0.274)\tLoss 0.5408 (0.5408)\tPrec 82.031% (82.031%)\n",
      "Epoch: [90][100/391]\tTime 0.060 (0.063)\tData 0.002 (0.005)\tLoss 0.5587 (0.5364)\tPrec 78.906% (81.366%)\n",
      "Epoch: [90][200/391]\tTime 0.064 (0.064)\tData 0.002 (0.004)\tLoss 0.6351 (0.5391)\tPrec 78.125% (81.176%)\n",
      "Epoch: [90][300/391]\tTime 0.060 (0.063)\tData 0.002 (0.003)\tLoss 0.5846 (0.5369)\tPrec 79.688% (81.315%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.338 (0.338)\tLoss 0.5036 (0.5036)\tPrec 82.812% (82.812%)\n",
      " * Prec 79.190% \n",
      "best acc: 79.350000\n",
      "Epoch: [91][0/391]\tTime 0.401 (0.401)\tData 0.335 (0.335)\tLoss 0.6937 (0.6937)\tPrec 82.031% (82.031%)\n",
      "Epoch: [91][100/391]\tTime 0.060 (0.067)\tData 0.002 (0.006)\tLoss 0.6024 (0.5287)\tPrec 79.688% (81.799%)\n",
      "Epoch: [91][200/391]\tTime 0.066 (0.066)\tData 0.003 (0.004)\tLoss 0.5935 (0.5310)\tPrec 78.125% (81.545%)\n",
      "Epoch: [91][300/391]\tTime 0.074 (0.067)\tData 0.003 (0.004)\tLoss 0.5887 (0.5323)\tPrec 77.344% (81.333%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.257 (0.257)\tLoss 0.5335 (0.5335)\tPrec 82.031% (82.031%)\n",
      " * Prec 79.620% \n",
      "best acc: 79.620000\n",
      "Epoch: [92][0/391]\tTime 0.331 (0.331)\tData 0.266 (0.266)\tLoss 0.5268 (0.5268)\tPrec 85.156% (85.156%)\n",
      "Epoch: [92][100/391]\tTime 0.059 (0.064)\tData 0.002 (0.005)\tLoss 0.5595 (0.5379)\tPrec 77.344% (81.374%)\n",
      "Epoch: [92][200/391]\tTime 0.066 (0.064)\tData 0.003 (0.004)\tLoss 0.5280 (0.5308)\tPrec 83.594% (81.499%)\n",
      "Epoch: [92][300/391]\tTime 0.066 (0.064)\tData 0.003 (0.003)\tLoss 0.5177 (0.5300)\tPrec 83.594% (81.530%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.298 (0.298)\tLoss 0.5789 (0.5789)\tPrec 78.125% (78.125%)\n",
      " * Prec 77.850% \n",
      "best acc: 79.620000\n",
      "Epoch: [93][0/391]\tTime 0.381 (0.381)\tData 0.304 (0.304)\tLoss 0.5836 (0.5836)\tPrec 78.906% (78.906%)\n",
      "Epoch: [93][100/391]\tTime 0.066 (0.068)\tData 0.003 (0.005)\tLoss 0.4639 (0.5403)\tPrec 82.812% (81.420%)\n",
      "Epoch: [93][200/391]\tTime 0.056 (0.066)\tData 0.002 (0.004)\tLoss 0.5604 (0.5347)\tPrec 82.031% (81.592%)\n",
      "Epoch: [93][300/391]\tTime 0.067 (0.065)\tData 0.003 (0.003)\tLoss 0.5291 (0.5344)\tPrec 82.812% (81.536%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.309 (0.309)\tLoss 0.5369 (0.5369)\tPrec 82.031% (82.031%)\n",
      " * Prec 77.740% \n",
      "best acc: 79.620000\n",
      "Epoch: [94][0/391]\tTime 0.374 (0.374)\tData 0.302 (0.302)\tLoss 0.5728 (0.5728)\tPrec 81.250% (81.250%)\n",
      "Epoch: [94][100/391]\tTime 0.067 (0.068)\tData 0.002 (0.005)\tLoss 0.3785 (0.5235)\tPrec 86.719% (81.312%)\n",
      "Epoch: [94][200/391]\tTime 0.066 (0.067)\tData 0.002 (0.004)\tLoss 0.5204 (0.5264)\tPrec 82.812% (81.433%)\n",
      "Epoch: [94][300/391]\tTime 0.074 (0.067)\tData 0.002 (0.003)\tLoss 0.5227 (0.5294)\tPrec 79.688% (81.510%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.329 (0.329)\tLoss 0.5165 (0.5165)\tPrec 83.594% (83.594%)\n",
      " * Prec 78.420% \n",
      "best acc: 79.620000\n",
      "Epoch: [95][0/391]\tTime 0.405 (0.405)\tData 0.322 (0.322)\tLoss 0.3708 (0.3708)\tPrec 89.062% (89.062%)\n",
      "Epoch: [95][100/391]\tTime 0.073 (0.075)\tData 0.002 (0.006)\tLoss 0.4723 (0.5263)\tPrec 84.375% (81.745%)\n",
      "Epoch: [95][200/391]\tTime 0.053 (0.067)\tData 0.002 (0.004)\tLoss 0.5456 (0.5272)\tPrec 79.688% (81.670%)\n",
      "Epoch: [95][300/391]\tTime 0.066 (0.065)\tData 0.002 (0.003)\tLoss 0.5353 (0.5338)\tPrec 78.906% (81.359%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.329 (0.329)\tLoss 0.4979 (0.4979)\tPrec 80.469% (80.469%)\n",
      " * Prec 78.370% \n",
      "best acc: 79.620000\n",
      "Epoch: [96][0/391]\tTime 0.383 (0.383)\tData 0.312 (0.312)\tLoss 0.4946 (0.4946)\tPrec 83.594% (83.594%)\n",
      "Epoch: [96][100/391]\tTime 0.066 (0.068)\tData 0.002 (0.005)\tLoss 0.5995 (0.5283)\tPrec 75.781% (81.776%)\n",
      "Epoch: [96][200/391]\tTime 0.065 (0.060)\tData 0.002 (0.004)\tLoss 0.5364 (0.5303)\tPrec 82.812% (81.518%)\n",
      "Epoch: [96][300/391]\tTime 0.066 (0.061)\tData 0.002 (0.003)\tLoss 0.4604 (0.5306)\tPrec 83.594% (81.471%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.358 (0.358)\tLoss 0.5110 (0.5110)\tPrec 82.812% (82.812%)\n",
      " * Prec 79.900% \n",
      "best acc: 79.900000\n",
      "Epoch: [97][0/391]\tTime 0.418 (0.418)\tData 0.345 (0.345)\tLoss 0.5055 (0.5055)\tPrec 83.594% (83.594%)\n",
      "Epoch: [97][100/391]\tTime 0.052 (0.064)\tData 0.002 (0.006)\tLoss 0.4042 (0.5174)\tPrec 87.500% (82.016%)\n",
      "Epoch: [97][200/391]\tTime 0.066 (0.061)\tData 0.002 (0.004)\tLoss 0.5160 (0.5217)\tPrec 82.031% (81.814%)\n",
      "Epoch: [97][300/391]\tTime 0.063 (0.061)\tData 0.002 (0.003)\tLoss 0.4554 (0.5247)\tPrec 87.500% (81.733%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.356 (0.356)\tLoss 0.4794 (0.4794)\tPrec 85.156% (85.156%)\n",
      " * Prec 79.050% \n",
      "best acc: 79.900000\n",
      "Epoch: [98][0/391]\tTime 0.385 (0.385)\tData 0.322 (0.322)\tLoss 0.4993 (0.4993)\tPrec 80.469% (80.469%)\n",
      "Epoch: [98][100/391]\tTime 0.066 (0.066)\tData 0.002 (0.006)\tLoss 0.5005 (0.5199)\tPrec 79.688% (81.289%)\n",
      "Epoch: [98][200/391]\tTime 0.066 (0.065)\tData 0.003 (0.004)\tLoss 0.5659 (0.5310)\tPrec 81.250% (81.320%)\n",
      "Epoch: [98][300/391]\tTime 0.067 (0.065)\tData 0.003 (0.003)\tLoss 0.4289 (0.5310)\tPrec 85.938% (81.289%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.332 (0.332)\tLoss 0.4818 (0.4818)\tPrec 82.812% (82.812%)\n",
      " * Prec 78.740% \n",
      "best acc: 79.900000\n",
      "Epoch: [99][0/391]\tTime 0.369 (0.369)\tData 0.299 (0.299)\tLoss 0.6171 (0.6171)\tPrec 77.344% (77.344%)\n",
      "Epoch: [99][100/391]\tTime 0.060 (0.062)\tData 0.002 (0.005)\tLoss 0.5577 (0.5361)\tPrec 78.125% (80.863%)\n",
      "Epoch: [99][200/391]\tTime 0.066 (0.063)\tData 0.002 (0.004)\tLoss 0.7659 (0.5322)\tPrec 71.875% (81.141%)\n",
      "Epoch: [99][300/391]\tTime 0.074 (0.065)\tData 0.002 (0.003)\tLoss 0.7053 (0.5328)\tPrec 75.781% (81.221%)\n",
      "Validation starts\n",
      "Test: [0/79]\tTime 0.289 (0.289)\tLoss 0.6004 (0.6004)\tPrec 80.469% (80.469%)\n",
      " * Prec 76.260% \n",
      "best acc: 79.900000\n"
     ]
    }
   ],
   "source": [
    "# This cell won't be given, but students will complete the training\n",
    "\n",
    "lr = 4e-2\n",
    "weight_decay = 1e-4\n",
    "epochs = 100\n",
    "best_prec = 0\n",
    "\n",
    "#model = nn.DataParallel(model).cuda()\n",
    "model.cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.makedirs('result')\n",
    "fdir = 'result/'+str(model_name)\n",
    "if not os.path.exists(fdir):\n",
    "    os.makedirs(fdir)\n",
    "        \n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "    train(trainloader, model, criterion, optimizer, epoch)\n",
    "    \n",
    "    # evaluate on test set\n",
    "    print(\"Validation starts\")\n",
    "    prec = validate(testloader, model, criterion)\n",
    "\n",
    "    # remember best precision and save checkpoint\n",
    "    is_best = prec > best_prec\n",
    "    best_prec = max(prec,best_prec)\n",
    "    print('best acc: {:1f}'.format(best_prec))\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_prec': best_prec,\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, is_best, fdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "entertaining-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Accuracy: 7990/10000 (80%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = \"result/resnet20_quant4bit/model_best.pth.tar\"\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "device = torch.device(\"cuda\") \n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in testloader:\n",
    "        data, target = data.to(device), target.to(device) # loading to GPU\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  \n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(testloader.dataset)\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(testloader.dataset),\n",
    "        100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b081a0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prehooked\n",
      "Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) 1\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 2\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 3\n",
      "prehooked\n",
      "Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False) 4\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 5\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 6\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 7\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 8\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 9\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 10\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 11\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 12\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 13\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 14\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 15\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 16\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 17\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 18\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 19\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 20\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 21\n",
      "prehooked\n",
      "QuantConv2d(\n",
      "  64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "  (weight_quant): weight_quantize_fn()\n",
      ") 22\n"
     ]
    }
   ],
   "source": [
    "class SaveOutput:\n",
    "    def __init__(self):\n",
    "        self.outputs = []\n",
    "    def __call__(self, module, module_in):\n",
    "        self.outputs.append(module_in)\n",
    "    def clear(self):\n",
    "        self.outputs = []  \n",
    "        \n",
    "######### Save inputs from selected layer ##########\n",
    "save_output = SaveOutput()\n",
    "device = torch.device(\"cuda\" if use_gpu else \"cpu\") \n",
    "counter =0\n",
    "for layer in model.modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        print(\"prehooked\")\n",
    "        counter += 1\n",
    "        print(layer, counter)\n",
    "        layer.register_forward_pre_hook(save_output)       ## Input for the module will be grapped       \n",
    "####################################################\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "images = images.to(device)\n",
    "out = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "spoken-worst",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]]],\n",
      "\n",
      "\n",
      "        [[[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.0000, -7.0000,  7.0000],\n",
      "          [-0.0000, -0.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-0.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]]],\n",
      "\n",
      "\n",
      "        [[[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.0000, -0.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-0.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [ 0.0000, -7.0000,  7.0000],\n",
      "          [-7.0000, -0.0000,  0.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]]],\n",
      "\n",
      "\n",
      "        [[[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000,  0.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000, -7.0000],\n",
      "          [ 0.0000,  0.0000, -7.0000],\n",
      "          [-0.0000, -0.0000, -7.0000]],\n",
      "\n",
      "         [[-0.0000, -0.0000, -7.0000],\n",
      "          [ 0.0000,  0.0000, -0.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]]],\n",
      "\n",
      "\n",
      "        [[[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000,  7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000, -7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.0000,  7.0000, -7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [-7.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[-7.0000, -7.0000, -7.0000],\n",
      "          [-7.0000, -7.0000, -7.0000],\n",
      "          [ 0.0000,  7.0000,  7.0000]],\n",
      "\n",
      "         [[ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000],\n",
      "          [ 7.0000,  7.0000,  7.0000]]]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "w_bit = 4\n",
    "weight_q = model.layer1[0].conv2.weight_q # quantized value is stored during the training\n",
    "w_alpha = model.layer1[0].conv2.weight_quant.wgt_alpha\n",
    "w_delta = w_alpha/(2**(w_bit-1)-1)\n",
    "weight_int = weight_q/w_delta\n",
    "print(weight_int) # you should see clean integer numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "interior-oxygen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          ...,\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 15.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000, 15.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000, 12.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          ...,\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          ...,\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          ...,\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          ...,\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 12.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 15.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 15.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 15.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 15.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 12.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          ...,\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          ...,\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 15.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 15.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 15.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          ...,\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          ...,\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          ...,\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          ...,\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 15.0000,  0.0000],\n",
      "          [ 0.0000, 12.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000, 15.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          ...,\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "         [[15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          ...,\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000],\n",
      "          [15.0000, 15.0000, 15.0000,  ..., 15.0000, 15.0000, 15.0000]]]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_bit = 4\n",
    "x = save_output.outputs[2][0]  # input of the 2nd conv layer\n",
    "x_alpha  = model.layer1[0].conv2.act_alpha\n",
    "x_delta = x_alpha/(2**x_bit-1)\n",
    "\n",
    "act_quant_fn = act_quantization(x_bit) # define the quantization function\n",
    "x_q = act_quant_fn(x, x_alpha)         # create the quantized value for x\n",
    "\n",
    "x_int = x_q/x_delta\n",
    "print(x_int) # you should see clean integer numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ranging-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_int = torch.nn.Conv2d(in_channels = 8, out_channels=8, kernel_size = 3, padding=1, bias = False)\n",
    "conv_int.weight = torch.nn.parameter.Parameter(weight_int)\n",
    "relu = nn.ReLU()\n",
    "bn = nn.BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True).to(device)\n",
    "output_int = conv_int(x_int)\n",
    "output_int = output_int*w_delta*x_delta\n",
    "output_recovered = relu(output_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d5639a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.0165, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#print(save_output.outputs[3][0].size())\n",
    "output_recovered = output_recovered[:, :8, :, :] \n",
    "#print(output_recovered.size())\n",
    "difference = abs(save_output.outputs[3][0] - output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight quantized version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 16, out_channels=16, kernel_size = 3, bias = False)\n",
    "conv_ref.weight = model.layer1[0].conv1.weight_q \n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "print(output_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sorted-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### input floating number / weight floating number version\n",
    "\n",
    "conv_ref = torch.nn.Conv2d(in_channels = 16, out_channels=16, kernel_size = 3, bias = False)\n",
    "weight = model.layer1[0].conv1.weight\n",
    "mean = weight.data.mean()\n",
    "std = weight.data.std()\n",
    "conv_ref.weight = torch.nn.parameter.Parameter(weight.add(-mean).div(std))\n",
    "\n",
    "output_ref = conv_ref(x)\n",
    "print(output_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = abs( output_ref - output_recovered )\n",
    "print(difference.mean())  ## It should be small, e.g.,2.3 in my trainned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-significance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-witch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-barbados",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-serbia",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
